{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import math\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "np.random.seed(123)\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to read the features\n",
    "def describe_column(meta):\n",
    "    \"\"\"\n",
    "    Utility function for describing a dataset column (see below for usage)\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        d = pd.Series(name=x.name, dtype=object)\n",
    "        m = next(m for m in meta if m['name'] == x.name)\n",
    "        d['Type'] = m['type']\n",
    "        d['#NaN'] = x.isna().sum()\n",
    "        d['Description'] = m['desc']\n",
    "        if m['type'] == 'categorical':\n",
    "            counts = x.dropna().map(dict(enumerate(m['cats']))).value_counts().sort_index()\n",
    "            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n",
    "        elif m['type'] == 'real' or m['type'] == 'integer':\n",
    "            stats = x.dropna().agg(['mean', 'std', 'min', 'max'])\n",
    "            d['Statistics'] = ', '.join(f'{s}={v :.1f}' for s, v in stats.items())\n",
    "        elif m['type'] == 'boolean':\n",
    "            counts = x.dropna().astype(bool).value_counts().sort_index()\n",
    "            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n",
    "        else:\n",
    "            d['Statistics'] = f'#unique={x.nunique()}'\n",
    "        return d\n",
    "    return f\n",
    "\n",
    "#define a function to read the statistical values di un df\n",
    "def describe_data(data, meta):\n",
    "    desc = data.apply(describe_column(meta)).T\n",
    "    desc = desc.style.set_properties(**{'text-align': 'left'})\n",
    "    desc = desc.set_table_styles([ dict(selector='th', props=[('text-align', 'left')])])\n",
    "    return desc \n",
    "\n",
    "#define a function to compute the root mean square log error \n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all() \n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 23285 apartments\n"
     ]
    }
   ],
   "source": [
    "#description of the apartments\n",
    "apartments = pd.read_csv('apartments_train.csv')\n",
    "print(f'Loaded {len(apartments)} apartments')\n",
    "with open('apartments_meta.json') as f: \n",
    "    apartments_meta = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6791 buildings\n"
     ]
    }
   ],
   "source": [
    "#description of the buildings\n",
    "buildings = pd.read_csv('buildings_train.csv')\n",
    "print(f'Loaded {len(buildings)} buildings')\n",
    "with open('buildings_meta.json') as f: \n",
    "    buildings_meta = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All apartments have an associated building: True\n"
     ]
    }
   ],
   "source": [
    "#merge the due dataset using the building_id \n",
    "print(f'All apartments have an associated building: {apartments.building_id.isin(buildings.id).all()}')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test apartments have an associated building: True\n",
      "Number of train samples: 23285\n",
      "Number of test samples:  9937\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seller</th>\n",
       "      <th>area_total</th>\n",
       "      <th>area_kitchen</th>\n",
       "      <th>area_living</th>\n",
       "      <th>floor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>layout</th>\n",
       "      <th>ceiling</th>\n",
       "      <th>bathrooms_shared</th>\n",
       "      <th>...</th>\n",
       "      <th>address</th>\n",
       "      <th>constructed</th>\n",
       "      <th>material</th>\n",
       "      <th>stories</th>\n",
       "      <th>elevator_without</th>\n",
       "      <th>elevator_passenger</th>\n",
       "      <th>elevator_service</th>\n",
       "      <th>parking</th>\n",
       "      <th>garbage_chute</th>\n",
       "      <th>heating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>33217</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>56.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10А</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>33218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>к1</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>33219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>70к5</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9935</th>\n",
       "      <td>33220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.8</td>\n",
       "      <td>10.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9936</th>\n",
       "      <td>33221</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  seller  area_total  area_kitchen  area_living  floor  rooms  \\\n",
       "9932  33217     3.0       106.0          19.9         56.7   16.0    3.0   \n",
       "9933  33218     NaN        82.0           NaN          NaN    3.0    3.0   \n",
       "9934  33219     NaN        49.3           NaN          NaN   15.0    1.0   \n",
       "9935  33220     NaN        38.8          10.5         15.1   14.0    1.0   \n",
       "9936  33221     1.0        71.5           7.0         49.0    2.0    3.0   \n",
       "\n",
       "      layout  ceiling  bathrooms_shared  ...  address  constructed  material  \\\n",
       "9932     NaN      3.3               NaN  ...      10А       2020.0       2.0   \n",
       "9933     NaN      NaN               2.0  ...       к1       2021.0       2.0   \n",
       "9934     NaN      0.0               NaN  ...     70к5       2016.0       2.0   \n",
       "9935     NaN      3.3               1.0  ...        1       2019.0       NaN   \n",
       "9936     NaN      NaN               NaN  ...        2          NaN       NaN   \n",
       "\n",
       "      stories  elevator_without  elevator_passenger  elevator_service  \\\n",
       "9932     20.0               0.0                 1.0               1.0   \n",
       "9933     20.0               0.0                 1.0               1.0   \n",
       "9934     24.0               0.0                 1.0               1.0   \n",
       "9935     14.0               1.0                 1.0               1.0   \n",
       "9936      2.0               NaN                 NaN               NaN   \n",
       "\n",
       "      parking  garbage_chute  heating  \n",
       "9932      0.0            NaN      0.0  \n",
       "9933      1.0            NaN      NaN  \n",
       "9934      0.0            1.0      1.0  \n",
       "9935      0.0            1.0      NaN  \n",
       "9936      NaN            NaN      NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do the same process with the data_test\n",
    "apartments_test = pd.read_csv('apartments_test.csv')\n",
    "buildings_test = pd.read_csv('buildings_test.csv')\n",
    "print(f'All test apartments have an associated building: {apartments_test.building_id.isin(buildings_test.id).all()}')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "print(f'Number of train samples: {len(data)}')\n",
    "print(f'Number of test samples:  {len(data_test)}')\n",
    "data_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test=data_test['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 2\n",
    "# id feature is pointless for the analysis of the price since it changes for each sample\n",
    "del data['id']\n",
    "del data_test['id']\n",
    "# street and address features are pointless becuase we have latitude and longitude\n",
    "del data['street']\n",
    "del data_test['street']\n",
    "del data['address']\n",
    "del data_test['address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 3\n",
    "# layout is pointless because for the largest number of the samples this features is NaN\n",
    "del data['layout']\n",
    "del data_test['layout']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.a \n",
    "# firstly samples with NaN values for the features 'bathroom_shared' and 'bathroom_private' have been deleted and then the heatmap is displayed only for these two attributes\n",
    "#bathroom_share is strongly correlated with bathroom_private: the coefficient is -0.61 (the correlation is inverse).\n",
    "#so the feature 'bathrooms_shared' has been deleted. \n",
    "df = data[data['bathrooms_shared'].notna()]\n",
    "df = df[df['bathrooms_private'].notna()]\n",
    "prova=df[['bathrooms_shared','bathrooms_private']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"bathrooms_shared\"]\n",
    "del data_test[\"bathrooms_shared\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.b \n",
    "# firstly samples with NaN values for the features 'balconies' and 'loggias' have been deleted and then the heatmap is displayed only for these two attributes\n",
    "#balconies is strongly correlated with loggias: the coefficient is -0.7(the correlation is inverse).\n",
    "#so the feature 'loggias' has been deleted. \n",
    "df = data[data['balconies'].notna()]\n",
    "df = df[df['loggias'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"loggias\"]\n",
    "del data_test[\"loggias\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.c \n",
    "\n",
    "df = data[data['area_living'].notna()]\n",
    "df = df[df['area_total'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"area_living\"]\n",
    "del data_test[\"area_living\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.d\n",
    "\n",
    "df = data[data['rooms'].notna()]\n",
    "df = df[df['area_total'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"rooms\"]\n",
    "del data_test[\"rooms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.e\n",
    "\n",
    "df = data[data['seller'].notna()]\n",
    "df = df[df['new'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"seller\"]\n",
    "del data_test[\"seller\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.f\n",
    "\n",
    "df = data[data['elevator_service'].notna()]\n",
    "df = df[df['constructed'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"elevator_service\"]\n",
    "del data_test[\"elevator_service\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.g\n",
    "\n",
    "df = data[data['new'].notna()]\n",
    "df = df[df['constructed'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"constructed\"]\n",
    "del data_test[\"constructed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.h \n",
    "\n",
    "df = data[data['stories'].notna()]\n",
    "df = df[df['floor'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"stories\"]\n",
    "del data_test[\"stories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 5.i \n",
    "\n",
    "df = data[data['windows_court'].notna()]\n",
    "df = df[df['windows_street'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data[\"windows_court\"]\n",
    "del data_test[\"windows_court\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 6\n",
    "# new is useless because its distribution is almost unique (boolean value = false) and it is not correlated with the price feature\n",
    "del data['new']\n",
    "del data_test['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 7\n",
    "\n",
    "data['heating']=data['heating'].fillna(0.0)\n",
    "data_test['heating']=data_test['heating'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 8,9\n",
    "data = data[data['phones'].notna()]\n",
    "data = data[data['elevator_passenger'].notna()]\n",
    "data_test['phones'].fillna((data_test['phones'].mean()), inplace=True)\n",
    "data_test['elevator_passenger']=data_test['elevator_passenger'].fillna(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 10\n",
    "\n",
    "data = data[data['district'].notna()]\n",
    "data_test['district']=data_test['district'].fillna(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point 11\n",
    "#ceiling\n",
    "elencobuilding = data['building_id'].unique().tolist()\n",
    "for column in elencobuilding:\n",
    "    dd = data.loc[data['building_id'] == column]['ceiling']\n",
    "    if dd.sum(axis = 0) - dd.isnull().sum(axis = 0)>0:\n",
    "        data.loc[data['building_id'] == column]['ceiling'].fillna(data.loc[data['building_id'] == column]['ceiling'].mode()[0])\n",
    "\n",
    "data['ceiling'].fillna((data['ceiling'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 12\n",
    "\n",
    "del data[\"material\"]\n",
    "del data_test[\"material\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 13\n",
    "\n",
    "data['parking']=data['parking'].fillna(3.0)\n",
    "data_test['parking']=data_test['parking'].fillna(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 14\n",
    "\n",
    "data['bathrooms_private'].fillna((data['bathrooms_private'].mean()), inplace=True)\n",
    "data_test['bathrooms_private'].fillna((data_test['bathrooms_private'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 15\n",
    "\n",
    "data['elevator_passenger']=data['elevator_passenger'].fillna(-1.0)\n",
    "data_test['elevator_passenger']=data_test['elevator_passenger'].fillna(-1.0)\n",
    "data['elevator_without']=data['elevator_without'].fillna(-1.0)\n",
    "data_test['elevator_without']=data_test['elevator_without'].fillna(-1.0)\n",
    "data['garbage_chute']=data['garbage_chute'].fillna(-1.0)\n",
    "data_test['garbage_chute']=data_test['garbage_chute'].fillna(-1.0)\n",
    "data['windows_street']=data['windows_street'].fillna(-1.0)\n",
    "data_test['windows_street']=data_test['windows_street'].fillna(-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 16\n",
    "\n",
    "data_test['longitude'].fillna((data_test['longitude'].mean()), inplace=True)\n",
    "data_test['latitude'].fillna((data_test['latitude'].mean()), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#point 17\n",
    "\n",
    "#I created a new df with just the feature where I want to fill in the NaN (area_kitchen) and the feature highly correlated with \n",
    "#it because I  want to use just that to apply the k-means. Then I have to replace the original column 'area_kitchen' with the new one\n",
    "\n",
    "data1=data[['area_kitchen','area_total']]\n",
    "data_test1=data_test[['area_kitchen','area_total']]\n",
    "\n",
    "imputer = Pipeline([(\"imputer\", KNNImputer(n_neighbors=10)),\n",
    "                    (\"pandarizer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = ['area_kitchen','area_total'])))])\n",
    "\n",
    "\n",
    "data1=imputer.fit_transform(data1)\n",
    "data_test1=imputer.fit_transform(data_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"x1=data1['area_kitchen']\\nx1 = x1.reset_index()\\ndata = data.reset_index()\\ndata['area_kitchen'] = x1\\n\\nx2=data_test1['area_kitchen']\\nx2 = x2.reset_index()\\ndata_test = data_test.reset_index()\\ndata_test['area_kitchen'] = x2\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''x1=data1['area_kitchen']\n",
    "x1 = x1.reset_index()\n",
    "data = data.reset_index()\n",
    "data['area_kitchen'] = x1\n",
    "\n",
    "x2=data_test1['area_kitchen']\n",
    "x2 = x2.reset_index()\n",
    "data_test = data_test.reset_index()\n",
    "data_test['area_kitchen'] = x2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data['index']\n",
    "# del data_test['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have to use a pipeline because otherwise I obtain an array and not a dataframe (I tried to transform the array in a df again,\n",
    "#but in that case I obtain again the NaN, they are not transformed)\n",
    "\n",
    "imputer = Pipeline([(\"imputer\", KNNImputer(n_neighbors=10)),\n",
    "                    (\"pandarizer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = ['price','area_total','area_kitchen','floor','ceiling','bathrooms_private','windows_street','balconies','condition','phones','building_id','latitude','longitude','district','elevator_without','elevator_passenger','parking','garbage_chute','heating'])))])\n",
    "\n",
    "imputer2 = Pipeline([(\"imputer\", KNNImputer(n_neighbors=10)),\n",
    "                    (\"pandarizer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = ['area_total','area_kitchen','floor','ceiling','bathrooms_private','windows_street','balconies','condition','phones','building_id','latitude','longitude','district','elevator_without','elevator_passenger','parking','garbage_chute','heating'])))])\n",
    "\n",
    "data=imputer.fit_transform(data)\n",
    "data_test=imputer2.fit_transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #before merge the deleted columns into our dataset, it's necessary to apply a reset of the index because they didn't match each\n",
    "# #other (the last of the data is 22736, while the last of the new vectors is 23283)\n",
    "\n",
    "# data=data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I can apply a log transformation to the feature 'area_total'. I'd like to do it also for the feature 'ceiling', but it contain\n",
    "#the value 0 (you can't apply the log with it) and if you try to replace 0=0.001 the resulting log transformation isn't good\n",
    "\n",
    "data['area_total']=data['area_total'].apply(math.log)\n",
    "data_test['area_total']=data_test['area_total'].apply(math.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before standardize I have to save the feature 'price' in another array and delete it from the df\n",
    "\n",
    "y=data['price']\n",
    "del data['price']\n",
    "del data['building_id']\n",
    "\n",
    "del data_test['building_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization of all the numerical attributes remained\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_df = pd.DataFrame(scaler.transform(data))\n",
    "scaled_df.columns = data.columns\n",
    "\n",
    "scaler = StandardScaler().fit(data_test)\n",
    "scaled_df2 = pd.DataFrame(scaler.transform(data_test))\n",
    "scaled_df2.columns = data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numerical=scaled_df.iloc[:,:]\n",
    "X_numerical2=scaled_df2.iloc[:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMMIES for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #transform the categorical attributes (street and address) into dummies \n",
    "\n",
    "# dummies = pd.get_dummies(df_categorical.astype(str),drop_first=True) \n",
    "# dummies2 = pd.get_dummies(df_categorical2.astype(str),drop_first=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=pd.concat([dummies,X_numerical], axis = 1)\n",
    "# X2=pd.concat([dummies2,X_numerical2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I'm not sure PCA is the best option here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PCA fit\n",
    "from sklearn.decomposition import PCA\n",
    "# we can choose the number of components e.g. 10, the percentage of the total variance or set it to None (that means it automatically chooses the number of components)\n",
    "pca2 = PCA(n_components=1)\n",
    "\n",
    "pca2.fit(data) #The fit learns some quantities from the data, most importantly the \"components\" and \"explained variance\"\n",
    "X = pd.DataFrame(pca2.transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explained_var=pd.DataFrame(pca2.explained_variance_ratio_).transpose()\n",
    "%matplotlib inline\n",
    "ax = sns.barplot( data=explained_var)#i primi due mi danno gia più del 75% di informazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cum_explained_var=np.cumsum(pca2.explained_variance_ratio_)\n",
    "pd.DataFrame(cum_explained_var).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca2.fit(X2)\n",
    "# X2 = pd.DataFrame(pca2.transform(X2))\n",
    "\n",
    "# cum_explained_var=np.cumsum(pca2.explained_variance_ratio_)\n",
    "# pd.DataFrame(cum_explained_var).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate Train/Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_numerical\n",
    "X2=X_numerical2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15915, 17) (6822, 17)\n"
     ]
    }
   ],
   "source": [
    "#after importing the necessary library, we can do the splitting between the training and test set, with a percentege of the \n",
    "#training equal to 70% of the starting dataset. Moreover, it has been fixed the random seed for replicability by putting the \n",
    "#parameter random_state= 123.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size =0.30, #by default is 75%-25%\n",
    "                                                    random_state= 123) #fix random seed for replicability\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Neural Network Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LeakyReLU, BatchNormalization, Dropout, ReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 15:05:07.764906: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-11 15:05:07.764928: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-11 15:05:07.764945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (loris-VivoBook-ASUSLaptop-X409JA-X409JA): /proc/driver/nvidia/version does not exist\n",
      "2021-10-11 15:05:07.765147: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model=Sequential([\n",
    "    \n",
    "    Dense(1024, input_dim = X_train.shape[1]), \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(512),  \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(512),  \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(units=256), \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(units=256), \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.01),\n",
    "\n",
    "    Dense(units=128), \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.05),\n",
    "\n",
    "    Dense(units=128),\n",
    "    LeakyReLU(), \n",
    "    Dropout(0.05),\n",
    "\n",
    "    Dense(units=1, activation=\"linear\"),\n",
    "],name=\"Moscow_Housing\",)\n",
    "model.compile(optimizer=Adam(learning_rate=0.005, decay=5e-4),loss='mse')\n",
    "checkpoint_name = 'Weights\\Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 15:05:08.008785: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 4s 125ms/step - loss: 3184173862879232.0000 - val_loss: 3313075495108608.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3313075495108608.00000, saving model to Weights\\Weights-001--3313075495108608.00000.hdf5\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 3184152119607296.0000 - val_loss: 3313019928969216.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 3313075495108608.00000 to 3313019928969216.00000, saving model to Weights\\Weights-002--3313019928969216.00000.hdf5\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 3184098432516096.0000 - val_loss: 3312670157570048.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 3313019928969216.00000 to 3312670157570048.00000, saving model to Weights\\Weights-003--3312670157570048.00000.hdf5\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 3183988105543680.0000 - val_loss: 3312678747504640.0000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 3312670157570048.00000\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 3183824896786432.0000 - val_loss: 3312448698318848.0000\n",
      "\n",
      "Epoch 00005: val_loss improved from 3312670157570048.00000 to 3312448698318848.00000, saving model to Weights\\Weights-005--3312448698318848.00000.hdf5\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 3183537133977600.0000 - val_loss: 3311708084895744.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 3312448698318848.00000 to 3311708084895744.00000, saving model to Weights\\Weights-006--3311708084895744.00000.hdf5\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 3183120790585344.0000 - val_loss: 3311794521112576.0000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 3311708084895744.00000\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 3182541775306752.0000 - val_loss: 3311610105954304.0000\n",
      "\n",
      "Epoch 00008: val_loss improved from 3311708084895744.00000 to 3311610105954304.00000, saving model to Weights\\Weights-008--3311610105954304.00000.hdf5\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 3181799551270912.0000 - val_loss: 3310707089080320.0000\n",
      "\n",
      "Epoch 00009: val_loss improved from 3311610105954304.00000 to 3310707089080320.00000, saving model to Weights\\Weights-009--3310707089080320.00000.hdf5\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 3180797481713664.0000 - val_loss: 3308227013902336.0000\n",
      "\n",
      "Epoch 00010: val_loss improved from 3310707089080320.00000 to 3308227013902336.00000, saving model to Weights\\Weights-010--3308227013902336.00000.hdf5\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 3179584690323456.0000 - val_loss: 3303037082796032.0000\n",
      "\n",
      "Epoch 00011: val_loss improved from 3308227013902336.00000 to 3303037082796032.00000, saving model to Weights\\Weights-011--3303037082796032.00000.hdf5\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 3178245197398016.0000 - val_loss: 3304813588643840.0000\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3303037082796032.00000\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 3176403998605312.0000 - val_loss: 3303378532696064.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3303037082796032.00000\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 3174284969115648.0000 - val_loss: 3301802816569344.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 3303037082796032.00000 to 3301802816569344.00000, saving model to Weights\\Weights-014--3301802816569344.00000.hdf5\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 3172033064075264.0000 - val_loss: 3301711011643392.0000\n",
      "\n",
      "Epoch 00015: val_loss improved from 3301802816569344.00000 to 3301711011643392.00000, saving model to Weights\\Weights-015--3301711011643392.00000.hdf5\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 3169267910443008.0000 - val_loss: 3296938229235712.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 3301711011643392.00000 to 3296938229235712.00000, saving model to Weights\\Weights-016--3296938229235712.00000.hdf5\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 3166501414633472.0000 - val_loss: 3274064307159040.0000\n",
      "\n",
      "Epoch 00017: val_loss improved from 3296938229235712.00000 to 3274064307159040.00000, saving model to Weights\\Weights-017--3274064307159040.00000.hdf5\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 3162751639748608.0000 - val_loss: 3287897859948544.0000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3274064307159040.00000\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 3158741214035968.0000 - val_loss: 3280487967621120.0000\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3274064307159040.00000\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 3154561673986048.0000 - val_loss: 3280856529502208.0000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3274064307159040.00000\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 3149987265380352.0000 - val_loss: 3275655055671296.0000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 3274064307159040.00000\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 3145128583626752.0000 - val_loss: 3279249137991680.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 3274064307159040.00000\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 3139018187341824.0000 - val_loss: 3264030726684672.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 3274064307159040.00000 to 3264030726684672.00000, saving model to Weights\\Weights-023--3264030726684672.00000.hdf5\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 3132703511674880.0000 - val_loss: 3265990573948928.0000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3264030726684672.00000\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 3127578005078016.0000 - val_loss: 3248723463241728.0000\n",
      "\n",
      "Epoch 00025: val_loss improved from 3264030726684672.00000 to 3248723463241728.00000, saving model to Weights\\Weights-025--3248723463241728.00000.hdf5\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 3s 163ms/step - loss: 3120394403840000.0000 - val_loss: 3240679526367232.0000\n",
      "\n",
      "Epoch 00026: val_loss improved from 3248723463241728.00000 to 3240679526367232.00000, saving model to Weights\\Weights-026--3240679526367232.00000.hdf5\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 3112984243077120.0000 - val_loss: 3229334907125760.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 3240679526367232.00000 to 3229334907125760.00000, saving model to Weights\\Weights-027--3229334907125760.00000.hdf5\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 3106303153012736.0000 - val_loss: 3219823970484224.0000\n",
      "\n",
      "Epoch 00028: val_loss improved from 3229334907125760.00000 to 3219823970484224.00000, saving model to Weights\\Weights-028--3219823970484224.00000.hdf5\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 3097932261752832.0000 - val_loss: 3209606511722496.0000\n",
      "\n",
      "Epoch 00029: val_loss improved from 3219823970484224.00000 to 3209606511722496.00000, saving model to Weights\\Weights-029--3209606511722496.00000.hdf5\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 3089794640904192.0000 - val_loss: 3188672572686336.0000\n",
      "\n",
      "Epoch 00030: val_loss improved from 3209606511722496.00000 to 3188672572686336.00000, saving model to Weights\\Weights-030--3188672572686336.00000.hdf5\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 3079386592968704.0000 - val_loss: 3185703676542976.0000\n",
      "\n",
      "Epoch 00031: val_loss improved from 3188672572686336.00000 to 3185703676542976.00000, saving model to Weights\\Weights-031--3185703676542976.00000.hdf5\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 3069350596575232.0000 - val_loss: 3191572480917504.0000\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3185703676542976.00000\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 3060080983408640.0000 - val_loss: 3183996158607360.0000\n",
      "\n",
      "Epoch 00033: val_loss improved from 3185703676542976.00000 to 3183996158607360.00000, saving model to Weights\\Weights-033--3183996158607360.00000.hdf5\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 3048995941253120.0000 - val_loss: 3166919905509376.0000\n",
      "\n",
      "Epoch 00034: val_loss improved from 3183996158607360.00000 to 3166919905509376.00000, saving model to Weights\\Weights-034--3166919905509376.00000.hdf5\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 3040627197476864.0000 - val_loss: 3138751094063104.0000\n",
      "\n",
      "Epoch 00035: val_loss improved from 3166919905509376.00000 to 3138751094063104.00000, saving model to Weights\\Weights-035--3138751094063104.00000.hdf5\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 3029596110848000.0000 - val_loss: 3127624444411904.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 3138751094063104.00000 to 3127624444411904.00000, saving model to Weights\\Weights-036--3127624444411904.00000.hdf5\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 3017399745904640.0000 - val_loss: 3108088517230592.0000\n",
      "\n",
      "Epoch 00037: val_loss improved from 3127624444411904.00000 to 3108088517230592.00000, saving model to Weights\\Weights-037--3108088517230592.00000.hdf5\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 3004236476448768.0000 - val_loss: 3109088439304192.0000\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3108088517230592.00000\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 2993057481883648.0000 - val_loss: 3103645105127424.0000\n",
      "\n",
      "Epoch 00039: val_loss improved from 3108088517230592.00000 to 3103645105127424.00000, saving model to Weights\\Weights-039--3103645105127424.00000.hdf5\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 2978393121357824.0000 - val_loss: 3088734052417536.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 3103645105127424.00000 to 3088734052417536.00000, saving model to Weights\\Weights-040--3088734052417536.00000.hdf5\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 2965998382612480.0000 - val_loss: 3043503214952448.0000\n",
      "\n",
      "Epoch 00041: val_loss improved from 3088734052417536.00000 to 3043503214952448.00000, saving model to Weights\\Weights-041--3043503214952448.00000.hdf5\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 2956094724898816.0000 - val_loss: 3056671853117440.0000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 3043503214952448.00000\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 2935882340368384.0000 - val_loss: 3047030188408832.0000\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3043503214952448.00000\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 2923487870058496.0000 - val_loss: 3050881431896064.0000\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3043503214952448.00000\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 2908223556288512.0000 - val_loss: 3014326965239808.0000\n",
      "\n",
      "Epoch 00045: val_loss improved from 3043503214952448.00000 to 3014326965239808.00000, saving model to Weights\\Weights-045--3014326965239808.00000.hdf5\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 2896194695069696.0000 - val_loss: 2987828896071680.0000\n",
      "\n",
      "Epoch 00046: val_loss improved from 3014326965239808.00000 to 2987828896071680.00000, saving model to Weights\\Weights-046--2987828896071680.00000.hdf5\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 2880340091731968.0000 - val_loss: 2972409695043584.0000\n",
      "\n",
      "Epoch 00047: val_loss improved from 2987828896071680.00000 to 2972409695043584.00000, saving model to Weights\\Weights-047--2972409695043584.00000.hdf5\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 2861635240722432.0000 - val_loss: 2960060858761216.0000\n",
      "\n",
      "Epoch 00048: val_loss improved from 2972409695043584.00000 to 2960060858761216.00000, saving model to Weights\\Weights-048--2960060858761216.00000.hdf5\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 2852930952626176.0000 - val_loss: 2943754746986496.0000\n",
      "\n",
      "Epoch 00049: val_loss improved from 2960060858761216.00000 to 2943754746986496.00000, saving model to Weights\\Weights-049--2943754746986496.00000.hdf5\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 3s 180ms/step - loss: 2828516747902976.0000 - val_loss: 2896727271014400.0000\n",
      "\n",
      "Epoch 00050: val_loss improved from 2943754746986496.00000 to 2896727271014400.00000, saving model to Weights\\Weights-050--2896727271014400.00000.hdf5\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 2817080323735552.0000 - val_loss: 2884640159301632.0000\n",
      "\n",
      "Epoch 00051: val_loss improved from 2896727271014400.00000 to 2884640159301632.00000, saving model to Weights\\Weights-051--2884640159301632.00000.hdf5\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 2798574651834368.0000 - val_loss: 2924608319651840.0000\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2884640159301632.00000\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 2783585484406784.0000 - val_loss: 2821241878609920.0000\n",
      "\n",
      "Epoch 00053: val_loss improved from 2884640159301632.00000 to 2821241878609920.00000, saving model to Weights\\Weights-053--2821241878609920.00000.hdf5\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 2768894817206272.0000 - val_loss: 2785886781571072.0000\n",
      "\n",
      "Epoch 00054: val_loss improved from 2821241878609920.00000 to 2785886781571072.00000, saving model to Weights\\Weights-054--2785886781571072.00000.hdf5\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 2745349269618688.0000 - val_loss: 2801385439494144.0000\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2785886781571072.00000\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 2727885395722240.0000 - val_loss: 2746859755929600.0000\n",
      "\n",
      "Epoch 00056: val_loss improved from 2785886781571072.00000 to 2746859755929600.00000, saving model to Weights\\Weights-056--2746859755929600.00000.hdf5\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 2704817663246336.0000 - val_loss: 2763481010929664.0000\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2746859755929600.00000\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 2695814874923008.0000 - val_loss: 2815109470617600.0000\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2746859755929600.00000\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 2673778471469056.0000 - val_loss: 2786572902596608.0000\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2746859755929600.00000\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 2657174396338176.0000 - val_loss: 2723382927818752.0000\n",
      "\n",
      "Epoch 00060: val_loss improved from 2746859755929600.00000 to 2723382927818752.00000, saving model to Weights\\Weights-060--2723382927818752.00000.hdf5\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 2641339657224192.0000 - val_loss: 2719921452613632.0000\n",
      "\n",
      "Epoch 00061: val_loss improved from 2723382927818752.00000 to 2719921452613632.00000, saving model to Weights\\Weights-061--2719921452613632.00000.hdf5\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 2618902748069888.0000 - val_loss: 2670474299441152.0000\n",
      "\n",
      "Epoch 00062: val_loss improved from 2719921452613632.00000 to 2670474299441152.00000, saving model to Weights\\Weights-062--2670474299441152.00000.hdf5\n",
      "Epoch 63/500\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 2601917226156032.0000 - val_loss: 2559914056613888.0000\n",
      "\n",
      "Epoch 00063: val_loss improved from 2670474299441152.00000 to 2559914056613888.00000, saving model to Weights\\Weights-063--2559914056613888.00000.hdf5\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 2589041719508992.0000 - val_loss: 2625263594635264.0000\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2559914056613888.00000\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 2563523708190720.0000 - val_loss: 2604752441442304.0000\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2559914056613888.00000\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 2542581179219968.0000 - val_loss: 2596625826447360.0000\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2559914056613888.00000\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 2526641213407232.0000 - val_loss: 2564978628362240.0000\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2559914056613888.00000\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 2510847276482560.0000 - val_loss: 2503666896470016.0000\n",
      "\n",
      "Epoch 00068: val_loss improved from 2559914056613888.00000 to 2503666896470016.00000, saving model to Weights\\Weights-068--2503666896470016.00000.hdf5\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 2485761177812992.0000 - val_loss: 2510294031007744.0000\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2503666896470016.00000\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 2463691488362496.0000 - val_loss: 2545061791268864.0000\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2503666896470016.00000\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 2439279699558400.0000 - val_loss: 2546227874889728.0000\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2503666896470016.00000\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 2420863416664064.0000 - val_loss: 2530981546295296.0000\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2503666896470016.00000\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 2396244127252480.0000 - val_loss: 2545437332471808.0000\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2503666896470016.00000\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 2398413354172416.0000 - val_loss: 2466582001352704.0000\n",
      "\n",
      "Epoch 00074: val_loss improved from 2503666896470016.00000 to 2466582001352704.00000, saving model to Weights\\Weights-074--2466582001352704.00000.hdf5\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 2371289025085440.0000 - val_loss: 2345360039084032.0000\n",
      "\n",
      "Epoch 00075: val_loss improved from 2466582001352704.00000 to 2345360039084032.00000, saving model to Weights\\Weights-075--2345360039084032.00000.hdf5\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 2s 146ms/step - loss: 2346420090699776.0000 - val_loss: 2361832580841472.0000\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2345360039084032.00000\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 2311094253125632.0000 - val_loss: 2299568439951360.0000\n",
      "\n",
      "Epoch 00077: val_loss improved from 2345360039084032.00000 to 2299568439951360.00000, saving model to Weights\\Weights-077--2299568439951360.00000.hdf5\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 2293154174730240.0000 - val_loss: 2416014130151424.0000\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2299568439951360.00000\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 2293026667888640.0000 - val_loss: 2355104514572288.0000\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2299568439951360.00000\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 2268093845864448.0000 - val_loss: 2212388120035328.0000\n",
      "\n",
      "Epoch 00080: val_loss improved from 2299568439951360.00000 to 2212388120035328.00000, saving model to Weights\\Weights-080--2212388120035328.00000.hdf5\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 2s 141ms/step - loss: 2243955324354560.0000 - val_loss: 2115857387880448.0000\n",
      "\n",
      "Epoch 00081: val_loss improved from 2212388120035328.00000 to 2115857387880448.00000, saving model to Weights\\Weights-081--2115857387880448.00000.hdf5\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2236381283745792.0000 - val_loss: 2181240211898368.0000\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2115857387880448.00000\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 2203319833460736.0000 - val_loss: 2201576210956288.0000\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2115857387880448.00000\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 2180197340151808.0000 - val_loss: 2170962489376768.0000\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2115857387880448.00000\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 2160398077788160.0000 - val_loss: 2197240978341888.0000\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2115857387880448.00000\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 2138779359903744.0000 - val_loss: 2208465472716800.0000\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2115857387880448.00000\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 2118282836443136.0000 - val_loss: 2069169214324736.0000\n",
      "\n",
      "Epoch 00087: val_loss improved from 2115857387880448.00000 to 2069169214324736.00000, saving model to Weights\\Weights-087--2069169214324736.00000.hdf5\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 2129660607463424.0000 - val_loss: 2086124268814336.0000\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2069169214324736.00000\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 2086910382047232.0000 - val_loss: 2206486432317440.0000\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2069169214324736.00000\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 2082779428814848.0000 - val_loss: 2144310204039168.0000\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2069169214324736.00000\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 2034245224628224.0000 - val_loss: 2084695521099776.0000\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2069169214324736.00000\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 2012835617964032.0000 - val_loss: 2086254996881408.0000\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2069169214324736.00000\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 2011663494545408.0000 - val_loss: 2048794828996608.0000\n",
      "\n",
      "Epoch 00093: val_loss improved from 2069169214324736.00000 to 2048794828996608.00000, saving model to Weights\\Weights-093--2048794828996608.00000.hdf5\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 1990760962457600.0000 - val_loss: 1947066246889472.0000\n",
      "\n",
      "Epoch 00094: val_loss improved from 2048794828996608.00000 to 1947066246889472.00000, saving model to Weights\\Weights-094--1947066246889472.00000.hdf5\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1977703859224576.0000 - val_loss: 1988001580187648.0000\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1947066246889472.00000\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 1983178868785152.0000 - val_loss: 1984133156831232.0000\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1947066246889472.00000\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 1942992604626944.0000 - val_loss: 1935793434132480.0000\n",
      "\n",
      "Epoch 00097: val_loss improved from 1947066246889472.00000 to 1935793434132480.00000, saving model to Weights\\Weights-097--1935793434132480.00000.hdf5\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 1903047395508224.0000 - val_loss: 1948218908737536.0000\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1935793434132480.00000\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1872807470301184.0000 - val_loss: 1901560934170624.0000\n",
      "\n",
      "Epoch 00099: val_loss improved from 1935793434132480.00000 to 1901560934170624.00000, saving model to Weights\\Weights-099--1901560934170624.00000.hdf5\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 1860989062479872.0000 - val_loss: 1802464093274112.0000\n",
      "\n",
      "Epoch 00100: val_loss improved from 1901560934170624.00000 to 1802464093274112.00000, saving model to Weights\\Weights-100--1802464093274112.00000.hdf5\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1851285120745472.0000 - val_loss: 1941436081635328.0000\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1802464093274112.00000\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 1831159843520512.0000 - val_loss: 1867201195802624.0000\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1802464093274112.00000\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 1812339967918080.0000 - val_loss: 1905850264322048.0000\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1802464093274112.00000\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 1806943475728384.0000 - val_loss: 1805882618806272.0000\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1802464093274112.00000\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 1800018646269952.0000 - val_loss: 1706543380692992.0000\n",
      "\n",
      "Epoch 00105: val_loss improved from 1802464093274112.00000 to 1706543380692992.00000, saving model to Weights\\Weights-105--1706543380692992.00000.hdf5\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 1754459243806720.0000 - val_loss: 1813227415535616.0000\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1706543380692992.00000\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 3s 159ms/step - loss: 1729948536537088.0000 - val_loss: 1795573086683136.0000\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1706543380692992.00000\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1709659916337152.0000 - val_loss: 1666270546100224.0000\n",
      "\n",
      "Epoch 00108: val_loss improved from 1706543380692992.00000 to 1666270546100224.00000, saving model to Weights\\Weights-108--1666270546100224.00000.hdf5\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1706286487961600.0000 - val_loss: 1645533772906496.0000\n",
      "\n",
      "Epoch 00109: val_loss improved from 1666270546100224.00000 to 1645533772906496.00000, saving model to Weights\\Weights-109--1645533772906496.00000.hdf5\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 1702809846153216.0000 - val_loss: 1622635691638784.0000\n",
      "\n",
      "Epoch 00110: val_loss improved from 1645533772906496.00000 to 1622635691638784.00000, saving model to Weights\\Weights-110--1622635691638784.00000.hdf5\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1667857670733824.0000 - val_loss: 1637151473139712.0000\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1622635691638784.00000\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 1655910950764544.0000 - val_loss: 1670658392064000.0000\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1622635691638784.00000\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 1636065249067008.0000 - val_loss: 1686757405884416.0000\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1622635691638784.00000\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 1611010557345792.0000 - val_loss: 1745285193662464.0000\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1622635691638784.00000\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 1576459558715392.0000 - val_loss: 1476020808974336.0000\n",
      "\n",
      "Epoch 00115: val_loss improved from 1622635691638784.00000 to 1476020808974336.00000, saving model to Weights\\Weights-115--1476020808974336.00000.hdf5\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1575716395155456.0000 - val_loss: 1569698072231936.0000\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 1562030213431296.0000 - val_loss: 1496647792066560.0000\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 1541336926781440.0000 - val_loss: 1666252695142400.0000\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 1515679865896960.0000 - val_loss: 1521081861013504.0000\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 1525352132247552.0000 - val_loss: 1538279044284416.0000\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 1505485492584448.0000 - val_loss: 1521680874733568.0000\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 1493388717195264.0000 - val_loss: 1506835454492672.0000\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 1472242043060224.0000 - val_loss: 1552819119194112.0000\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1476020808974336.00000\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 1442981504614400.0000 - val_loss: 1451367294042112.0000\n",
      "\n",
      "Epoch 00124: val_loss improved from 1476020808974336.00000 to 1451367294042112.00000, saving model to Weights\\Weights-124--1451367294042112.00000.hdf5\n",
      "Epoch 125/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1434341641027584.0000 - val_loss: 1512071690715136.0000\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1451367294042112.00000\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 2s 137ms/step - loss: 1404689455251456.0000 - val_loss: 1530685542105088.0000\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1451367294042112.00000\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 1439201933393920.0000 - val_loss: 1457884772696064.0000\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1451367294042112.00000\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1391011192373248.0000 - val_loss: 1498333432512512.0000\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1451367294042112.00000\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1397519410003968.0000 - val_loss: 1443407645900800.0000\n",
      "\n",
      "Epoch 00129: val_loss improved from 1451367294042112.00000 to 1443407645900800.00000, saving model to Weights\\Weights-129--1443407645900800.00000.hdf5\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 1363198594777088.0000 - val_loss: 1353442710781952.0000\n",
      "\n",
      "Epoch 00130: val_loss improved from 1443407645900800.00000 to 1353442710781952.00000, saving model to Weights\\Weights-130--1353442710781952.00000.hdf5\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1355410879545344.0000 - val_loss: 1491226738032640.0000\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1353442710781952.00000\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 1324356252729344.0000 - val_loss: 1400349793452032.0000\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1353442710781952.00000\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 1320932224270336.0000 - val_loss: 1442390678175744.0000\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1353442710781952.00000\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 1279563904581632.0000 - val_loss: 1349893994053632.0000\n",
      "\n",
      "Epoch 00134: val_loss improved from 1353442710781952.00000 to 1349893994053632.00000, saving model to Weights\\Weights-134--1349893994053632.00000.hdf5\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 2s 144ms/step - loss: 1300376645009408.0000 - val_loss: 1273196481347584.0000\n",
      "\n",
      "Epoch 00135: val_loss improved from 1349893994053632.00000 to 1273196481347584.00000, saving model to Weights\\Weights-135--1273196481347584.00000.hdf5\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 1288039083016192.0000 - val_loss: 1187707707457536.0000\n",
      "\n",
      "Epoch 00136: val_loss improved from 1273196481347584.00000 to 1187707707457536.00000, saving model to Weights\\Weights-136--1187707707457536.00000.hdf5\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 1274633550561280.0000 - val_loss: 1298951655391232.0000\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1187707707457536.00000\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1260372245872640.0000 - val_loss: 1265393196859392.0000\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1187707707457536.00000\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 1228094761336832.0000 - val_loss: 1207765607383040.0000\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1187707707457536.00000\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 1247825035788288.0000 - val_loss: 1306800573906944.0000\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1187707707457536.00000\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 1231081508438016.0000 - val_loss: 1101994655744000.0000\n",
      "\n",
      "Epoch 00141: val_loss improved from 1187707707457536.00000 to 1101994655744000.00000, saving model to Weights\\Weights-141--1101994655744000.00000.hdf5\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1197126973390848.0000 - val_loss: 1167843349495808.0000\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1101994655744000.00000\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1210513446928384.0000 - val_loss: 1119829775876096.0000\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1101994655744000.00000\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 1179569952391168.0000 - val_loss: 1014796686196736.0000\n",
      "\n",
      "Epoch 00144: val_loss improved from 1101994655744000.00000 to 1014796686196736.00000, saving model to Weights\\Weights-144--1014796686196736.00000.hdf5\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 1161821134258176.0000 - val_loss: 1032935004176384.0000\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1141644451643392.0000 - val_loss: 1143637584904192.0000\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 1132085062402048.0000 - val_loss: 1177804720832512.0000\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 1125824140935168.0000 - val_loss: 1082738908069888.0000\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 1139860563820544.0000 - val_loss: 1184197108563968.0000\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 2s 133ms/step - loss: 1106970174029824.0000 - val_loss: 1075092188561408.0000\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 1083540322123776.0000 - val_loss: 1032780922224640.0000\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 1079203613114368.0000 - val_loss: 1089807350497280.0000\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 1086412111740928.0000 - val_loss: 1020192305971200.0000\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 1072659693568000.0000 - val_loss: 1053296773038080.0000\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 1082628446879744.0000 - val_loss: 1073901006225408.0000\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1014796686196736.00000\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 1038095541600256.0000 - val_loss: 998856250621952.0000\n",
      "\n",
      "Epoch 00156: val_loss improved from 1014796686196736.00000 to 998856250621952.00000, saving model to Weights\\Weights-156--998856250621952.00000.hdf5\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 1063574629777408.0000 - val_loss: 959343893676032.0000\n",
      "\n",
      "Epoch 00157: val_loss improved from 998856250621952.00000 to 959343893676032.00000, saving model to Weights\\Weights-157--959343893676032.00000.hdf5\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 1032328272936960.0000 - val_loss: 971428253925376.0000\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 959343893676032.00000\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 1015785535307776.0000 - val_loss: 1033950763941888.0000\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 959343893676032.00000\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 1025887499714560.0000 - val_loss: 967038998675456.0000\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 959343893676032.00000\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 1050706370887680.0000 - val_loss: 945447661207552.0000\n",
      "\n",
      "Epoch 00161: val_loss improved from 959343893676032.00000 to 945447661207552.00000, saving model to Weights\\Weights-161--945447661207552.00000.hdf5\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 967550905090048.0000 - val_loss: 889434778107904.0000\n",
      "\n",
      "Epoch 00162: val_loss improved from 945447661207552.00000 to 889434778107904.00000, saving model to Weights\\Weights-162--889434778107904.00000.hdf5\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 957342304698368.0000 - val_loss: 957100847005696.0000\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 889434778107904.00000\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 989110365847552.0000 - val_loss: 939505104191488.0000\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 889434778107904.00000\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 965110289924096.0000 - val_loss: 958416918937600.0000\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 889434778107904.00000\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 939768170938368.0000 - val_loss: 919315872219136.0000\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 889434778107904.00000\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 920315324530688.0000 - val_loss: 872898617147392.0000\n",
      "\n",
      "Epoch 00167: val_loss improved from 889434778107904.00000 to 872898617147392.00000, saving model to Weights\\Weights-167--872898617147392.00000.hdf5\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 956004959256576.0000 - val_loss: 904722378653696.0000\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 872898617147392.00000\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 972366368735232.0000 - val_loss: 907159235723264.0000\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 872898617147392.00000\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 931290308149248.0000 - val_loss: 813702156648448.0000\n",
      "\n",
      "Epoch 00170: val_loss improved from 872898617147392.00000 to 813702156648448.00000, saving model to Weights\\Weights-170--813702156648448.00000.hdf5\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 947093036335104.0000 - val_loss: 801049988300800.0000\n",
      "\n",
      "Epoch 00171: val_loss improved from 813702156648448.00000 to 801049988300800.00000, saving model to Weights\\Weights-171--801049988300800.00000.hdf5\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 888297819734016.0000 - val_loss: 875436271730688.0000\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 801049988300800.00000\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 912644714266624.0000 - val_loss: 884443556347904.0000\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 801049988300800.00000\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 907064008245248.0000 - val_loss: 941076458242048.0000\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 801049988300800.00000\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 908586976804864.0000 - val_loss: 930955367809024.0000\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 801049988300800.00000\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 921191497859072.0000 - val_loss: 795151454699520.0000\n",
      "\n",
      "Epoch 00176: val_loss improved from 801049988300800.00000 to 795151454699520.00000, saving model to Weights\\Weights-176--795151454699520.00000.hdf5\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 846376992768000.0000 - val_loss: 874135299293184.0000\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 795151454699520.00000\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 853341416456192.0000 - val_loss: 832935154417664.0000\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 795151454699520.00000\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 878978814443520.0000 - val_loss: 737275763752960.0000\n",
      "\n",
      "Epoch 00179: val_loss improved from 795151454699520.00000 to 737275763752960.00000, saving model to Weights\\Weights-179--737275763752960.00000.hdf5\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 942512587931648.0000 - val_loss: 860257052000256.0000\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 876618193043456.0000 - val_loss: 816327925170176.0000\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 813321112518656.0000 - val_loss: 914515910721536.0000\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 815909635620864.0000 - val_loss: 878065261477888.0000\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 852447459278848.0000 - val_loss: 859500735102976.0000\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 822161698717696.0000 - val_loss: 780923436007424.0000\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 186/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 803447922229248.0000 - val_loss: 748450798895104.0000\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 878731988041728.0000 - val_loss: 786338181808128.0000\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 737275763752960.00000\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 2s 129ms/step - loss: 772644953653248.0000 - val_loss: 731061415837696.0000\n",
      "\n",
      "Epoch 00188: val_loss improved from 737275763752960.00000 to 731061415837696.00000, saving model to Weights\\Weights-188--731061415837696.00000.hdf5\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 821857158692864.0000 - val_loss: 757356883345408.0000\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 731061415837696.00000\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 768473332449280.0000 - val_loss: 743797201829888.0000\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 731061415837696.00000\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 781510437240832.0000 - val_loss: 672322168029184.0000\n",
      "\n",
      "Epoch 00191: val_loss improved from 731061415837696.00000 to 672322168029184.00000, saving model to Weights\\Weights-191--672322168029184.00000.hdf5\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 752752342859776.0000 - val_loss: 744939596021760.0000\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 672322168029184.00000\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 789106355339264.0000 - val_loss: 644972218941440.0000\n",
      "\n",
      "Epoch 00193: val_loss improved from 672322168029184.00000 to 644972218941440.00000, saving model to Weights\\Weights-193--644972218941440.00000.hdf5\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 812935572094976.0000 - val_loss: 643196048637952.0000\n",
      "\n",
      "Epoch 00194: val_loss improved from 644972218941440.00000 to 643196048637952.00000, saving model to Weights\\Weights-194--643196048637952.00000.hdf5\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 2s 134ms/step - loss: 748677761073152.0000 - val_loss: 748711181287424.0000\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 643196048637952.00000\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 740255531532288.0000 - val_loss: 655078545424384.0000\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 643196048637952.00000\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 745620885209088.0000 - val_loss: 684156212215808.0000\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 643196048637952.00000\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 713407690964992.0000 - val_loss: 642185120710656.0000\n",
      "\n",
      "Epoch 00198: val_loss improved from 643196048637952.00000 to 642185120710656.00000, saving model to Weights\\Weights-198--642185120710656.00000.hdf5\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 737288111783936.0000 - val_loss: 663678747672576.0000\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 642185120710656.00000\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 747835007959040.0000 - val_loss: 679798028369920.0000\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 642185120710656.00000\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 727400224653312.0000 - val_loss: 668186047414272.0000\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 642185120710656.00000\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 700573120724992.0000 - val_loss: 683005362307072.0000\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 642185120710656.00000\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 744537144164352.0000 - val_loss: 594929139056640.0000\n",
      "\n",
      "Epoch 00203: val_loss improved from 642185120710656.00000 to 594929139056640.00000, saving model to Weights\\Weights-203--594929139056640.00000.hdf5\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 683509416984576.0000 - val_loss: 628186345897984.0000\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 719724379897856.0000 - val_loss: 632768740458496.0000\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 714647527227392.0000 - val_loss: 722772397391872.0000\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 739377076502528.0000 - val_loss: 632808267579392.0000\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 709806293778432.0000 - val_loss: 615525923160064.0000\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 695197432283136.0000 - val_loss: 656420320051200.0000\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 737675866800128.0000 - val_loss: 620227167518720.0000\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 646386873794560.0000 - val_loss: 662910552506368.0000\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 594929139056640.00000\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 693825257340928.0000 - val_loss: 590885628674048.0000\n",
      "\n",
      "Epoch 00212: val_loss improved from 594929139056640.00000 to 590885628674048.00000, saving model to Weights\\Weights-212--590885628674048.00000.hdf5\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 682693104762880.0000 - val_loss: 580352489816064.0000\n",
      "\n",
      "Epoch 00213: val_loss improved from 590885628674048.00000 to 580352489816064.00000, saving model to Weights\\Weights-213--580352489816064.00000.hdf5\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 636756214939648.0000 - val_loss: 646799391981568.0000\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 661261083738112.0000 - val_loss: 667577839779840.0000\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 634355865092096.0000 - val_loss: 664237026312192.0000\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 662869213446144.0000 - val_loss: 629663747538944.0000\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 652466198675456.0000 - val_loss: 631340932268032.0000\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 662383546597376.0000 - val_loss: 621098106355712.0000\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 630723597828096.0000 - val_loss: 660999828930560.0000\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 635510942859264.0000 - val_loss: 587964312715264.0000\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 580352489816064.00000\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 684489206398976.0000 - val_loss: 574716016328704.0000\n",
      "\n",
      "Epoch 00222: val_loss improved from 580352489816064.00000 to 574716016328704.00000, saving model to Weights\\Weights-222--574716016328704.00000.hdf5\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 2s 138ms/step - loss: 644175838052352.0000 - val_loss: 636573678829568.0000\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 574716016328704.00000\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 648253909499904.0000 - val_loss: 609574373556224.0000\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 574716016328704.00000\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 639353999065088.0000 - val_loss: 567432288665600.0000\n",
      "\n",
      "Epoch 00225: val_loss improved from 574716016328704.00000 to 567432288665600.00000, saving model to Weights\\Weights-225--567432288665600.00000.hdf5\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 2s 136ms/step - loss: 683249839898624.0000 - val_loss: 530440003780608.0000\n",
      "\n",
      "Epoch 00226: val_loss improved from 567432288665600.00000 to 530440003780608.00000, saving model to Weights\\Weights-226--530440003780608.00000.hdf5\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 598674014994432.0000 - val_loss: 545319582433280.0000\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 530440003780608.00000\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 582184226258944.0000 - val_loss: 540177030184960.0000\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 530440003780608.00000\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 607034571489280.0000 - val_loss: 606021563187200.0000\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 530440003780608.00000\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 664907443863552.0000 - val_loss: 489867695882240.0000\n",
      "\n",
      "Epoch 00230: val_loss improved from 530440003780608.00000 to 489867695882240.00000, saving model to Weights\\Weights-230--489867695882240.00000.hdf5\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 614654850105344.0000 - val_loss: 498619329282048.0000\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 3s 221ms/step - loss: 656076387123200.0000 - val_loss: 538382337835008.0000\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 611134251991040.0000 - val_loss: 557939068764160.0000\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 572829049290752.0000 - val_loss: 532735730909184.0000\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 600785058529280.0000 - val_loss: 559482237091840.0000\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 236/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 593283562602496.0000 - val_loss: 562113743421440.0000\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 237/500\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 592804807966720.0000 - val_loss: 547248928718848.0000\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 238/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 561800814788608.0000 - val_loss: 524883020742656.0000\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 239/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 571141664014336.0000 - val_loss: 537229072007168.0000\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 240/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 569629902635008.0000 - val_loss: 507567625207808.0000\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 241/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 574301551984640.0000 - val_loss: 574892646858752.0000\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 242/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 516976724148224.0000 - val_loss: 563573998747648.0000\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 243/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 566450821529600.0000 - val_loss: 583332458921984.0000\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 244/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 581483609718784.0000 - val_loss: 523676470476800.0000\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 245/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 575164571975680.0000 - val_loss: 547798281879552.0000\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 489867695882240.00000\n",
      "Epoch 246/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 615758455373824.0000 - val_loss: 484537943457792.0000\n",
      "\n",
      "Epoch 00246: val_loss improved from 489867695882240.00000 to 484537943457792.00000, saving model to Weights\\Weights-246--484537943457792.00000.hdf5\n",
      "Epoch 247/500\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 560716234883072.0000 - val_loss: 462917715623936.0000\n",
      "\n",
      "Epoch 00247: val_loss improved from 484537943457792.00000 to 462917715623936.00000, saving model to Weights\\Weights-247--462917715623936.00000.hdf5\n",
      "Epoch 248/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 541746035425280.0000 - val_loss: 510469479596032.0000\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 249/500\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 535219664846848.0000 - val_loss: 588530308874240.0000\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 250/500\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 553297383522304.0000 - val_loss: 536554325934080.0000\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 251/500\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 546234343358464.0000 - val_loss: 555323400126464.0000\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 252/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 564762765164544.0000 - val_loss: 501101249953792.0000\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 253/500\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 542352699555840.0000 - val_loss: 537115993571328.0000\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 254/500\n",
      "16/16 [==============================] - 2s 131ms/step - loss: 536680087945216.0000 - val_loss: 577396545683456.0000\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 255/500\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 518026206117888.0000 - val_loss: 526101516386304.0000\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 256/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 498194697945088.0000 - val_loss: 536856986910720.0000\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 462917715623936.00000\n",
      "Epoch 257/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 522224536649728.0000 - val_loss: 454975180242944.0000\n",
      "\n",
      "Epoch 00257: val_loss improved from 462917715623936.00000 to 454975180242944.00000, saving model to Weights\\Weights-257--454975180242944.00000.hdf5\n",
      "Epoch 258/500\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 573592681054208.0000 - val_loss: 474945503100928.0000\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 454975180242944.00000\n",
      "Epoch 259/500\n",
      "16/16 [==============================] - 2s 132ms/step - loss: 555581534371840.0000 - val_loss: 472033347502080.0000\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 454975180242944.00000\n",
      "Epoch 260/500\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 487023689334784.0000 - val_loss: 450564081057792.0000\n",
      "\n",
      "Epoch 00260: val_loss improved from 454975180242944.00000 to 450564081057792.00000, saving model to Weights\\Weights-260--450564081057792.00000.hdf5\n",
      "Epoch 261/500\n",
      "16/16 [==============================] - 3s 162ms/step - loss: 594580709834752.0000 - val_loss: 480545402257408.0000\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 262/500\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 655843318038528.0000 - val_loss: 528365366804480.0000\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 263/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 510028977012736.0000 - val_loss: 513806602076160.0000\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 264/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 533322161717248.0000 - val_loss: 507141752356864.0000\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 265/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 527797122498560.0000 - val_loss: 506326513876992.0000\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 266/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 481947205763072.0000 - val_loss: 506272055033856.0000\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 267/500\n",
      "16/16 [==============================] - 2s 159ms/step - loss: 539396822532096.0000 - val_loss: 483139428286464.0000\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 268/500\n",
      "16/16 [==============================] - 3s 161ms/step - loss: 528572431204352.0000 - val_loss: 480601673039872.0000\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 269/500\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 516755197788160.0000 - val_loss: 474851416473600.0000\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 270/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 514775217864704.0000 - val_loss: 463536258023424.0000\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 271/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 497295271395328.0000 - val_loss: 480542348804096.0000\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 272/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 512820336656384.0000 - val_loss: 480135031554048.0000\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 273/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 501452262866944.0000 - val_loss: 537060595204096.0000\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 450564081057792.00000\n",
      "Epoch 274/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 494433179009024.0000 - val_loss: 424403871465472.0000\n",
      "\n",
      "Epoch 00274: val_loss improved from 450564081057792.00000 to 424403871465472.00000, saving model to Weights\\Weights-274--424403871465472.00000.hdf5\n",
      "Epoch 275/500\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 452420010246144.0000 - val_loss: 433943765581824.0000\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 424403871465472.00000\n",
      "Epoch 276/500\n",
      "16/16 [==============================] - 3s 189ms/step - loss: 488864149929984.0000 - val_loss: 465678943387648.0000\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 424403871465472.00000\n",
      "Epoch 277/500\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 490850303868928.0000 - val_loss: 464858235535360.0000\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 424403871465472.00000\n",
      "Epoch 278/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 508671163367424.0000 - val_loss: 417240839094272.0000\n",
      "\n",
      "Epoch 00278: val_loss improved from 424403871465472.00000 to 417240839094272.00000, saving model to Weights\\Weights-278--417240839094272.00000.hdf5\n",
      "Epoch 279/500\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 500798991630336.0000 - val_loss: 422881540440064.0000\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 417240839094272.00000\n",
      "Epoch 280/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 513368918065152.0000 - val_loss: 438298593984512.0000\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 417240839094272.00000\n",
      "Epoch 281/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 484703199035392.0000 - val_loss: 405565977133056.0000\n",
      "\n",
      "Epoch 00281: val_loss improved from 417240839094272.00000 to 405565977133056.00000, saving model to Weights\\Weights-281--405565977133056.00000.hdf5\n",
      "Epoch 282/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 481114854522880.0000 - val_loss: 418791490060288.0000\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 283/500\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 471352830066688.0000 - val_loss: 436286334697472.0000\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 284/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 519992898486272.0000 - val_loss: 406403831300096.0000\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 285/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 436381159522304.0000 - val_loss: 412312498339840.0000\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 286/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 488037100290048.0000 - val_loss: 427137785921536.0000\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 287/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 457710503985152.0000 - val_loss: 439600875044864.0000\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 288/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 462413090521088.0000 - val_loss: 437143985979392.0000\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 405565977133056.00000\n",
      "Epoch 289/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 496288302891008.0000 - val_loss: 391718029492224.0000\n",
      "\n",
      "Epoch 00289: val_loss improved from 405565977133056.00000 to 391718029492224.00000, saving model to Weights\\Weights-289--391718029492224.00000.hdf5\n",
      "Epoch 290/500\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 467858672844800.0000 - val_loss: 421023497322496.0000\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 291/500\n",
      "16/16 [==============================] - 2s 130ms/step - loss: 510195373441024.0000 - val_loss: 396361694445568.0000\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 292/500\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 453691555446784.0000 - val_loss: 400567809605632.0000\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 293/500\n",
      "16/16 [==============================] - 2s 124ms/step - loss: 459948450381824.0000 - val_loss: 407649606696960.0000\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 294/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 444781847117824.0000 - val_loss: 428703234392064.0000\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 295/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 472887173578752.0000 - val_loss: 414285465387008.0000\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 296/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 475790068154368.0000 - val_loss: 402542420819968.0000\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 297/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 445512394211328.0000 - val_loss: 402827096621056.0000\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 298/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 432827409629184.0000 - val_loss: 398072232280064.0000\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 299/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 446092449677312.0000 - val_loss: 407199071338496.0000\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 300/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 495489204092928.0000 - val_loss: 398257385635840.0000\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 301/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 454895790456832.0000 - val_loss: 422830403485696.0000\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 302/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 507525615058944.0000 - val_loss: 400433759649792.0000\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 391718029492224.00000\n",
      "Epoch 303/500\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 447311851290624.0000 - val_loss: 382118878248960.0000\n",
      "\n",
      "Epoch 00303: val_loss improved from 391718029492224.00000 to 382118878248960.00000, saving model to Weights\\Weights-303--382118878248960.00000.hdf5\n",
      "Epoch 304/500\n",
      "16/16 [==============================] - 2s 158ms/step - loss: 428480802062336.0000 - val_loss: 376242725453824.0000\n",
      "\n",
      "Epoch 00304: val_loss improved from 382118878248960.00000 to 376242725453824.00000, saving model to Weights\\Weights-304--376242725453824.00000.hdf5\n",
      "Epoch 305/500\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 436822333194240.0000 - val_loss: 384960938639360.0000\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 306/500\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 436496083451904.0000 - val_loss: 424151609245696.0000\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 307/500\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 455053093634048.0000 - val_loss: 431234681405440.0000\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 308/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 444074251255808.0000 - val_loss: 414035719749632.0000\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 309/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 497549043564544.0000 - val_loss: 400002081882112.0000\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 310/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 465058119286784.0000 - val_loss: 399467358453760.0000\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 311/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 400380005449728.0000 - val_loss: 437966539325440.0000\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 312/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 540021438283776.0000 - val_loss: 381045941731328.0000\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 313/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 465483857920000.0000 - val_loss: 397006308638720.0000\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 314/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 435717016649728.0000 - val_loss: 416137670033408.0000\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 315/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 437526942711808.0000 - val_loss: 418844673835008.0000\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 316/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 458383941435392.0000 - val_loss: 396473095159808.0000\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 317/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 428100697456640.0000 - val_loss: 428710784139264.0000\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 318/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 387590532366336.0000 - val_loss: 412237671956480.0000\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 319/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 420289829666816.0000 - val_loss: 395500217958400.0000\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 320/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 467274758619136.0000 - val_loss: 397424262643712.0000\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 321/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 410811373715456.0000 - val_loss: 389731808444416.0000\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 322/500\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 485775934226432.0000 - val_loss: 377132421218304.0000\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 376242725453824.00000\n",
      "Epoch 323/500\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 440272232120320.0000 - val_loss: 359553556283392.0000\n",
      "\n",
      "Epoch 00323: val_loss improved from 376242725453824.00000 to 359553556283392.00000, saving model to Weights\\Weights-323--359553556283392.00000.hdf5\n",
      "Epoch 324/500\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 467100409790464.0000 - val_loss: 400226124824576.0000\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 325/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 381768200880128.0000 - val_loss: 414288384622592.0000\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 326/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 425386848550912.0000 - val_loss: 391957272592384.0000\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 327/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 411336433467392.0000 - val_loss: 381805882507264.0000\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 328/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 458402463481856.0000 - val_loss: 393649019944960.0000\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 329/500\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 471382492184576.0000 - val_loss: 403136300711936.0000\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 330/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 509765339840512.0000 - val_loss: 408902092980224.0000\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 331/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 386406899777536.0000 - val_loss: 392174302658560.0000\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 332/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 459346215436288.0000 - val_loss: 393722000834560.0000\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 333/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 467324016525312.0000 - val_loss: 372793027854336.0000\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 334/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 496607103549440.0000 - val_loss: 374909641424896.0000\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 335/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 496778029826048.0000 - val_loss: 386397907189760.0000\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 336/500\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 401505555316736.0000 - val_loss: 404443916926976.0000\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 337/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 404995585343488.0000 - val_loss: 466378486185984.0000\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 338/500\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 416157131603968.0000 - val_loss: 412757228781568.0000\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 339/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 392381098622976.0000 - val_loss: 372560629858304.0000\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 340/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 435577430212608.0000 - val_loss: 399951213363200.0000\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 341/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 380831562465280.0000 - val_loss: 406867721322496.0000\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 342/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 423413814394880.0000 - val_loss: 417815458742272.0000\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 343/500\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 442452297121792.0000 - val_loss: 384637306142720.0000\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 344/500\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 383149536182272.0000 - val_loss: 363704239521792.0000\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 359553556283392.00000\n",
      "Epoch 345/500\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 411118262550528.0000 - val_loss: 358251979866112.0000\n",
      "\n",
      "Epoch 00345: val_loss improved from 359553556283392.00000 to 358251979866112.00000, saving model to Weights\\Weights-345--358251979866112.00000.hdf5\n",
      "Epoch 346/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 428366213677056.0000 - val_loss: 368737202995200.0000\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 347/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 380834179710976.0000 - val_loss: 365496314626048.0000\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 348/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 436075545755648.0000 - val_loss: 376224505397248.0000\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 349/500\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 428154720092160.0000 - val_loss: 391568611606528.0000\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 350/500\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 404459016421376.0000 - val_loss: 398444686475264.0000\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 351/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 393093123670016.0000 - val_loss: 405684323614720.0000\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 352/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 397864228356096.0000 - val_loss: 366111937789952.0000\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 353/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 412710521012224.0000 - val_loss: 376280004427776.0000\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 354/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 384377863274496.0000 - val_loss: 378339978117120.0000\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 355/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 463434655203328.0000 - val_loss: 376950589751296.0000\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 356/500\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 381376687767552.0000 - val_loss: 362551678337024.0000\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 357/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 404646250151936.0000 - val_loss: 374554299990016.0000\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 358/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 445354050846720.0000 - val_loss: 384220895641600.0000\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 359/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 384071075102720.0000 - val_loss: 388977941020672.0000\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 360/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 419008989888512.0000 - val_loss: 383990410248192.0000\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 361/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 406179788357632.0000 - val_loss: 377190134841344.0000\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 362/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 407116024119296.0000 - val_loss: 391024492937216.0000\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 363/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 390182444466176.0000 - val_loss: 381974359310336.0000\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 364/500\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 384355046260736.0000 - val_loss: 398450155847680.0000\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 365/500\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 391934086479872.0000 - val_loss: 391091232702464.0000\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 366/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 360957373054976.0000 - val_loss: 385819898544128.0000\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 367/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 466329463160832.0000 - val_loss: 397152236863488.0000\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 368/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 429584105340928.0000 - val_loss: 376465426219008.0000\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 369/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 356837962547200.0000 - val_loss: 380095545999360.0000\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 370/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 360050463866880.0000 - val_loss: 376251952922624.0000\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 371/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 370952667922432.0000 - val_loss: 363247899246592.0000\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 372/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 485177289605120.0000 - val_loss: 375675420672000.0000\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 373/500\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 357128678146048.0000 - val_loss: 376804762189824.0000\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 374/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 375895705518080.0000 - val_loss: 371371225907200.0000\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 375/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 339098304774144.0000 - val_loss: 391143040745472.0000\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 358251979866112.00000\n",
      "Epoch 376/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 451230505631744.0000 - val_loss: 349335057334272.0000\n",
      "\n",
      "Epoch 00376: val_loss improved from 358251979866112.00000 to 349335057334272.00000, saving model to Weights\\Weights-376--349335057334272.00000.hdf5\n",
      "Epoch 377/500\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 476329388539904.0000 - val_loss: 348845967933440.0000\n",
      "\n",
      "Epoch 00377: val_loss improved from 349335057334272.00000 to 348845967933440.00000, saving model to Weights\\Weights-377--348845967933440.00000.hdf5\n",
      "Epoch 378/500\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 357960526069760.0000 - val_loss: 358739827752960.0000\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 379/500\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 410775067820032.0000 - val_loss: 363751719043072.0000\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 380/500\n",
      "16/16 [==============================] - 2s 140ms/step - loss: 373411134046208.0000 - val_loss: 376065356726272.0000\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 381/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 342779158855680.0000 - val_loss: 357640953659392.0000\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 382/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 357595353186304.0000 - val_loss: 360908316475392.0000\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 383/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 406835676839936.0000 - val_loss: 368946012225536.0000\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 384/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 352985242664960.0000 - val_loss: 369558850371584.0000\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 385/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 379019858018304.0000 - val_loss: 360266151755776.0000\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 348845967933440.00000\n",
      "Epoch 386/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 395553972158464.0000 - val_loss: 342540653953024.0000\n",
      "\n",
      "Epoch 00386: val_loss improved from 348845967933440.00000 to 342540653953024.00000, saving model to Weights\\Weights-386--342540653953024.00000.hdf5\n",
      "Epoch 387/500\n",
      "16/16 [==============================] - 2s 126ms/step - loss: 378928891953152.0000 - val_loss: 357204779597824.0000\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 388/500\n",
      "16/16 [==============================] - 2s 135ms/step - loss: 417345965129728.0000 - val_loss: 370396637429760.0000\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 389/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 431222501146624.0000 - val_loss: 371120708517888.0000\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 390/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 454287280832512.0000 - val_loss: 362674319785984.0000\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 391/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 428974253539328.0000 - val_loss: 364343988322304.0000\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 392/500\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 403069930045440.0000 - val_loss: 358877736468480.0000\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 393/500\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 380046757855232.0000 - val_loss: 352366666711040.0000\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 394/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 394278266208256.0000 - val_loss: 370118739623936.0000\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 395/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 438355468746752.0000 - val_loss: 346636106596352.0000\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 396/500\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 382149379227648.0000 - val_loss: 370647523917824.0000\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 397/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 383752979087360.0000 - val_loss: 364011598118912.0000\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 342540653953024.00000\n",
      "Epoch 398/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 435630311997440.0000 - val_loss: 337040579231744.0000\n",
      "\n",
      "Epoch 00398: val_loss improved from 342540653953024.00000 to 337040579231744.00000, saving model to Weights\\Weights-398--337040579231744.00000.hdf5\n",
      "Epoch 399/500\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 386850724249600.0000 - val_loss: 353301828730880.0000\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 400/500\n",
      "16/16 [==============================] - 2s 128ms/step - loss: 389072698736640.0000 - val_loss: 372859398520832.0000\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 401/500\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 324583026589696.0000 - val_loss: 376150954082304.0000\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 402/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 447184679993344.0000 - val_loss: 354929151574016.0000\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 403/500\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 453269071593472.0000 - val_loss: 366581464956928.0000\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 404/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 358200406704128.0000 - val_loss: 370585817317376.0000\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 405/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 350044364472320.0000 - val_loss: 382466602827776.0000\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 406/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 400718133460992.0000 - val_loss: 381828733075456.0000\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 407/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 379831237738496.0000 - val_loss: 355323583922176.0000\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 408/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 395496896069632.0000 - val_loss: 343052325486592.0000\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 409/500\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 350179286843392.0000 - val_loss: 359505372119040.0000\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 410/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 344173479723008.0000 - val_loss: 351496734507008.0000\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 411/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 412413362962432.0000 - val_loss: 357583911124992.0000\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 412/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 366711656153088.0000 - val_loss: 360073012445184.0000\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 413/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 458020513382400.0000 - val_loss: 361929814048768.0000\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 414/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 393209691766784.0000 - val_loss: 376689670488064.0000\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 415/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 356089933594624.0000 - val_loss: 365439473418240.0000\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 416/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 442450418073600.0000 - val_loss: 358628225712128.0000\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 417/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 415724715638784.0000 - val_loss: 360295713210368.0000\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 418/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 373499046658048.0000 - val_loss: 382325405777920.0000\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 419/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 424609895677952.0000 - val_loss: 378142141186048.0000\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 420/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 390423499505664.0000 - val_loss: 361443140567040.0000\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 421/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 412784877633536.0000 - val_loss: 370481395924992.0000\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 422/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 359397427511296.0000 - val_loss: 358058739892224.0000\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 423/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 464218218299392.0000 - val_loss: 349914307493888.0000\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 424/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 358118735216640.0000 - val_loss: 353048727650304.0000\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 425/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 403099491500032.0000 - val_loss: 351367784824832.0000\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 426/500\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 405532087156736.0000 - val_loss: 357327051948032.0000\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 427/500\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 415130869301248.0000 - val_loss: 349470885675008.0000\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 428/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 334612681195520.0000 - val_loss: 363724875497472.0000\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 429/500\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 388452713496576.0000 - val_loss: 377864042053632.0000\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 430/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 396444976545792.0000 - val_loss: 360904021508096.0000\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 431/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 343848874147840.0000 - val_loss: 372013659062272.0000\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 432/500\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 416913180065792.0000 - val_loss: 388003620978688.0000\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 433/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 427860380614656.0000 - val_loss: 377313615151104.0000\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 434/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 407715373383680.0000 - val_loss: 351364563599360.0000\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 435/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 314742048555008.0000 - val_loss: 351377381392384.0000\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 436/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 361621750808576.0000 - val_loss: 353715621986304.0000\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 437/500\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 371216204431360.0000 - val_loss: 367145951166464.0000\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 438/500\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 396706197798912.0000 - val_loss: 380921421234176.0000\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 439/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 357722222493696.0000 - val_loss: 362361961578496.0000\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 440/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 362027054792704.0000 - val_loss: 353757464363008.0000\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 441/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 401907906510848.0000 - val_loss: 363581128310784.0000\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 442/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 365213148774400.0000 - val_loss: 387987313524736.0000\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 443/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 400564756152320.0000 - val_loss: 374513262919680.0000\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 444/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 344540565209088.0000 - val_loss: 358603261214720.0000\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 445/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 355178561667072.0000 - val_loss: 362096009150464.0000\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 446/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 476019580469248.0000 - val_loss: 382321849008128.0000\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 447/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 417099071619072.0000 - val_loss: 357095694139392.0000\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 448/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 395862404497408.0000 - val_loss: 347879801618432.0000\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 449/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 383392168280064.0000 - val_loss: 344504695521280.0000\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 450/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 348036265934848.0000 - val_loss: 338856645754880.0000\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 451/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 384383265538048.0000 - val_loss: 354449088315392.0000\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 452/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 333716643643392.0000 - val_loss: 355653491097600.0000\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 453/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 331807966887936.0000 - val_loss: 360897511948288.0000\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 454/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 318237413736448.0000 - val_loss: 379216117891072.0000\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 455/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 341870504837120.0000 - val_loss: 369946605387776.0000\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 456/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 348118742728704.0000 - val_loss: 371406860713984.0000\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 457/500\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 387863229235200.0000 - val_loss: 354999682990080.0000\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 458/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 349294691352576.0000 - val_loss: 359076647141376.0000\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 459/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 371043331997696.0000 - val_loss: 365847495311360.0000\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 460/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 337503563284480.0000 - val_loss: 363776180224000.0000\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 461/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 412837323210752.0000 - val_loss: 361588800356352.0000\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 462/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 341203274956800.0000 - val_loss: 359431753695232.0000\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 463/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 433783912267776.0000 - val_loss: 373607830126592.0000\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 464/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 399618084962304.0000 - val_loss: 381105769283584.0000\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 465/500\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 332438622437376.0000 - val_loss: 366940598042624.0000\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 466/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 350554089848832.0000 - val_loss: 380163829268480.0000\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 467/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 348503477846016.0000 - val_loss: 371747035545600.0000\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 468/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 354360940822528.0000 - val_loss: 380776566751232.0000\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 469/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 345256012808192.0000 - val_loss: 399508596850688.0000\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 470/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 312853638676480.0000 - val_loss: 367347210649600.0000\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 471/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 342707687915520.0000 - val_loss: 362941211738112.0000\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 472/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 332936503099392.0000 - val_loss: 365622680616960.0000\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 473/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 341687767400448.0000 - val_loss: 351111730954240.0000\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 474/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 309235162284032.0000 - val_loss: 355669060354048.0000\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 475/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 298440265302016.0000 - val_loss: 368473297387520.0000\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 476/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 358458473840640.0000 - val_loss: 350545936121856.0000\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 477/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 335631762849792.0000 - val_loss: 339310670774272.0000\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 478/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 319615058051072.0000 - val_loss: 370831603531776.0000\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 479/500\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 354021269307392.0000 - val_loss: 353462252470272.0000\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 480/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 402143156633600.0000 - val_loss: 364430525202432.0000\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 481/500\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 331985805377536.0000 - val_loss: 362715289747456.0000\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 482/500\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 339858581094400.0000 - val_loss: 362367498059776.0000\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 483/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 351371207376896.0000 - val_loss: 395412070465536.0000\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 484/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 371885279805440.0000 - val_loss: 351492540203008.0000\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 485/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 362338070822912.0000 - val_loss: 360732323479552.0000\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 486/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 352633961316352.0000 - val_loss: 359367362740224.0000\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 487/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 306848636862464.0000 - val_loss: 377533832888320.0000\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 488/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 354839024369664.0000 - val_loss: 365358741454848.0000\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 489/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 289127299809280.0000 - val_loss: 367980047237120.0000\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 490/500\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 367312616030208.0000 - val_loss: 369635690020864.0000\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 491/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 300283276034048.0000 - val_loss: 367041462665216.0000\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 492/500\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 303403636883456.0000 - val_loss: 355839449759744.0000\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 493/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 356500270743552.0000 - val_loss: 352370726797312.0000\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 494/500\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 311045390336000.0000 - val_loss: 350699481202688.0000\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 495/500\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 337256267120640.0000 - val_loss: 355708520366080.0000\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 496/500\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 322886850052096.0000 - val_loss: 373363688079360.0000\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 497/500\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 322950167265280.0000 - val_loss: 369116535848960.0000\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 498/500\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 355385626066944.0000 - val_loss: 371832163139584.0000\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 499/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 312177751752704.0000 - val_loss: 365354815586304.0000\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 337040579231744.00000\n",
      "Epoch 500/500\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 316314107248640.0000 - val_loss: 357047476420608.0000\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 337040579231744.00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82840d13a0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size=1024,\n",
    "          epochs=500,\n",
    "          callbacks=callbacks_list, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5320015.801885216\n",
      "MSE: 357047483340206.5\n",
      "RMSE: 18895700.12834154\n",
      "VarScore: 0.8712079007279457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8275e24f70>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('VarScore:',metrics.explained_variance_score(y_test,y_pred))\n",
    "# Visualizing Our predictions\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(y_test,y_pred)\n",
    "# Perfect predictions\n",
    "plt.plot(y_test,y_test,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "-48472500.0     1\n",
      "-45001668.0     1\n",
      "-42532156.0     2\n",
      "-37703300.0     1\n",
      "-31596762.0     1\n",
      "               ..\n",
      " 528687232.0    1\n",
      " 529740640.0    1\n",
      " 558864512.0    1\n",
      " 604630336.0    1\n",
      " 748359296.0    1\n",
      "Length: 9618, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['area_total', 'area_kitchen', 'floor', 'ceiling', 'bathrooms_private',\n",
       "       'windows_street', 'balconies', 'condition', 'phones', 'latitude',\n",
       "       'longitude', 'district', 'elevator_without', 'elevator_passenger',\n",
       "       'parking', 'garbage_chute', 'heating', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(X2) - set(X))\n",
    "X2=X2[X.columns]\n",
    "y_pred2= model.predict(X2)\n",
    "X2['target']=y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pd.DataFrame()\n",
    "\n",
    "predictions['id']=id_test\n",
    "predictions['price_prediction']=y_pred2\n",
    "\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3184177084104704.0000 - val_loss: 3313279237619712.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3313279237619712.00000, saving model to Weights\\Weights-001--3313279237619712.00000.hdf5\n",
      "Epoch 2/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3184145677156352.0000 - val_loss: 3313149851729920.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 3313279237619712.00000 to 3313149851729920.00000, saving model to Weights\\Weights-002--3313149851729920.00000.hdf5\n",
      "Epoch 3/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 3183402916249600.0000 - val_loss: 3311340059885568.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 3313149851729920.00000 to 3311340059885568.00000, saving model to Weights\\Weights-003--3311340059885568.00000.hdf5\n",
      "Epoch 4/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 3178580204847104.0000 - val_loss: 3302782069112832.0000\n",
      "\n",
      "Epoch 00004: val_loss improved from 3311340059885568.00000 to 3302782069112832.00000, saving model to Weights\\Weights-004--3302782069112832.00000.hdf5\n",
      "Epoch 5/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 3162824385757184.0000 - val_loss: 3279078144606208.0000\n",
      "\n",
      "Epoch 00005: val_loss improved from 3302782069112832.00000 to 3279078144606208.00000, saving model to Weights\\Weights-005--3279078144606208.00000.hdf5\n",
      "Epoch 6/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 3123798433857536.0000 - val_loss: 3224871899234304.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 3279078144606208.00000 to 3224871899234304.00000, saving model to Weights\\Weights-006--3224871899234304.00000.hdf5\n",
      "Epoch 7/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 3045461720039424.0000 - val_loss: 3126703173926912.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 3224871899234304.00000 to 3126703173926912.00000, saving model to Weights\\Weights-007--3126703173926912.00000.hdf5\n",
      "Epoch 8/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2910717321674752.0000 - val_loss: 2968987411415040.0000\n",
      "\n",
      "Epoch 00008: val_loss improved from 3126703173926912.00000 to 2968987411415040.00000, saving model to Weights\\Weights-008--2968987411415040.00000.hdf5\n",
      "Epoch 9/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2721390868299776.0000 - val_loss: 2767915296227328.0000\n",
      "\n",
      "Epoch 00009: val_loss improved from 2968987411415040.00000 to 2767915296227328.00000, saving model to Weights\\Weights-009--2767915296227328.00000.hdf5\n",
      "Epoch 10/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2489714426773504.0000 - val_loss: 2538139646164992.0000\n",
      "\n",
      "Epoch 00010: val_loss improved from 2767915296227328.00000 to 2538139646164992.00000, saving model to Weights\\Weights-010--2538139646164992.00000.hdf5\n",
      "Epoch 11/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2253286946111488.0000 - val_loss: 2326084494295040.0000\n",
      "\n",
      "Epoch 00011: val_loss improved from 2538139646164992.00000 to 2326084494295040.00000, saving model to Weights\\Weights-011--2326084494295040.00000.hdf5\n",
      "Epoch 12/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2059623917944832.0000 - val_loss: 2168708571070464.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 2326084494295040.00000 to 2168708571070464.00000, saving model to Weights\\Weights-012--2168708571070464.00000.hdf5\n",
      "Epoch 13/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1924994946826240.0000 - val_loss: 2066729404465152.0000\n",
      "\n",
      "Epoch 00013: val_loss improved from 2168708571070464.00000 to 2066729404465152.00000, saving model to Weights\\Weights-013--2066729404465152.00000.hdf5\n",
      "Epoch 14/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1842752362905600.0000 - val_loss: 2008094510940160.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 2066729404465152.00000 to 2008094510940160.00000, saving model to Weights\\Weights-014--2008094510940160.00000.hdf5\n",
      "Epoch 15/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1794758385074176.0000 - val_loss: 1971049478488064.0000\n",
      "\n",
      "Epoch 00015: val_loss improved from 2008094510940160.00000 to 1971049478488064.00000, saving model to Weights\\Weights-015--1971049478488064.00000.hdf5\n",
      "Epoch 16/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1764780318654464.0000 - val_loss: 1945530930298880.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 1971049478488064.00000 to 1945530930298880.00000, saving model to Weights\\Weights-016--1945530930298880.00000.hdf5\n",
      "Epoch 17/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1743173277712384.0000 - val_loss: 1926094861107200.0000\n",
      "\n",
      "Epoch 00017: val_loss improved from 1945530930298880.00000 to 1926094861107200.00000, saving model to Weights\\Weights-017--1926094861107200.00000.hdf5\n",
      "Epoch 18/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1725600687456256.0000 - val_loss: 1909611179278336.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 1926094861107200.00000 to 1909611179278336.00000, saving model to Weights\\Weights-018--1909611179278336.00000.hdf5\n",
      "Epoch 19/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1709394433671168.0000 - val_loss: 1894573559250944.0000\n",
      "\n",
      "Epoch 00019: val_loss improved from 1909611179278336.00000 to 1894573559250944.00000, saving model to Weights\\Weights-019--1894573559250944.00000.hdf5\n",
      "Epoch 20/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1696042990960640.0000 - val_loss: 1879893897904128.0000\n",
      "\n",
      "Epoch 00020: val_loss improved from 1894573559250944.00000 to 1879893897904128.00000, saving model to Weights\\Weights-020--1879893897904128.00000.hdf5\n",
      "Epoch 21/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1682786842836992.0000 - val_loss: 1867001211387904.0000\n",
      "\n",
      "Epoch 00021: val_loss improved from 1879893897904128.00000 to 1867001211387904.00000, saving model to Weights\\Weights-021--1867001211387904.00000.hdf5\n",
      "Epoch 22/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1669943280009216.0000 - val_loss: 1854737469145088.0000\n",
      "\n",
      "Epoch 00022: val_loss improved from 1867001211387904.00000 to 1854737469145088.00000, saving model to Weights\\Weights-022--1854737469145088.00000.hdf5\n",
      "Epoch 23/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1658116282253312.0000 - val_loss: 1843315003621376.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 1854737469145088.00000 to 1843315003621376.00000, saving model to Weights\\Weights-023--1843315003621376.00000.hdf5\n",
      "Epoch 24/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1647697496899584.0000 - val_loss: 1831834690256896.0000\n",
      "\n",
      "Epoch 00024: val_loss improved from 1843315003621376.00000 to 1831834690256896.00000, saving model to Weights\\Weights-024--1831834690256896.00000.hdf5\n",
      "Epoch 25/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1636953501990912.0000 - val_loss: 1821737356361728.0000\n",
      "\n",
      "Epoch 00025: val_loss improved from 1831834690256896.00000 to 1821737356361728.00000, saving model to Weights\\Weights-025--1821737356361728.00000.hdf5\n",
      "Epoch 26/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1627019913723904.0000 - val_loss: 1811142343131136.0000\n",
      "\n",
      "Epoch 00026: val_loss improved from 1821737356361728.00000 to 1811142343131136.00000, saving model to Weights\\Weights-026--1811142343131136.00000.hdf5\n",
      "Epoch 27/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1617410192834560.0000 - val_loss: 1801098427891712.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 1811142343131136.00000 to 1801098427891712.00000, saving model to Weights\\Weights-027--1801098427891712.00000.hdf5\n",
      "Epoch 28/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1607908114563072.0000 - val_loss: 1792300053168128.0000\n",
      "\n",
      "Epoch 00028: val_loss improved from 1801098427891712.00000 to 1792300053168128.00000, saving model to Weights\\Weights-028--1792300053168128.00000.hdf5\n",
      "Epoch 29/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1599525143707648.0000 - val_loss: 1782044208136192.0000\n",
      "\n",
      "Epoch 00029: val_loss improved from 1792300053168128.00000 to 1782044208136192.00000, saving model to Weights\\Weights-029--1782044208136192.00000.hdf5\n",
      "Epoch 30/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1590672008151040.0000 - val_loss: 1773525140504576.0000\n",
      "\n",
      "Epoch 00030: val_loss improved from 1782044208136192.00000 to 1773525140504576.00000, saving model to Weights\\Weights-030--1773525140504576.00000.hdf5\n",
      "Epoch 31/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1582902680748032.0000 - val_loss: 1765039090434048.0000\n",
      "\n",
      "Epoch 00031: val_loss improved from 1773525140504576.00000 to 1765039090434048.00000, saving model to Weights\\Weights-031--1765039090434048.00000.hdf5\n",
      "Epoch 32/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1573926064881664.0000 - val_loss: 1756137368059904.0000\n",
      "\n",
      "Epoch 00032: val_loss improved from 1765039090434048.00000 to 1756137368059904.00000, saving model to Weights\\Weights-032--1756137368059904.00000.hdf5\n",
      "Epoch 33/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1567051432853504.0000 - val_loss: 1747916666437632.0000\n",
      "\n",
      "Epoch 00033: val_loss improved from 1756137368059904.00000 to 1747916666437632.00000, saving model to Weights\\Weights-033--1747916666437632.00000.hdf5\n",
      "Epoch 34/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1558493710516224.0000 - val_loss: 1740212166197248.0000\n",
      "\n",
      "Epoch 00034: val_loss improved from 1747916666437632.00000 to 1740212166197248.00000, saving model to Weights\\Weights-034--1740212166197248.00000.hdf5\n",
      "Epoch 35/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1551048116273152.0000 - val_loss: 1732368884826112.0000\n",
      "\n",
      "Epoch 00035: val_loss improved from 1740212166197248.00000 to 1732368884826112.00000, saving model to Weights\\Weights-035--1732368884826112.00000.hdf5\n",
      "Epoch 36/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1543845724553216.0000 - val_loss: 1725073748656128.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 1732368884826112.00000 to 1725073748656128.00000, saving model to Weights\\Weights-036--1725073748656128.00000.hdf5\n",
      "Epoch 37/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1536844927860736.0000 - val_loss: 1717624798969856.0000\n",
      "\n",
      "Epoch 00037: val_loss improved from 1725073748656128.00000 to 1717624798969856.00000, saving model to Weights\\Weights-037--1717624798969856.00000.hdf5\n",
      "Epoch 38/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1530190412906496.0000 - val_loss: 1710460793520128.0000\n",
      "\n",
      "Epoch 00038: val_loss improved from 1717624798969856.00000 to 1710460793520128.00000, saving model to Weights\\Weights-038--1710460793520128.00000.hdf5\n",
      "Epoch 39/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1523585424293888.0000 - val_loss: 1703473687035904.0000\n",
      "\n",
      "Epoch 00039: val_loss improved from 1710460793520128.00000 to 1703473687035904.00000, saving model to Weights\\Weights-039--1703473687035904.00000.hdf5\n",
      "Epoch 40/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1517066066591744.0000 - val_loss: 1696965469405184.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 1703473687035904.00000 to 1696965469405184.00000, saving model to Weights\\Weights-040--1696965469405184.00000.hdf5\n",
      "Epoch 41/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1510806688628736.0000 - val_loss: 1690218344218624.0000\n",
      "\n",
      "Epoch 00041: val_loss improved from 1696965469405184.00000 to 1690218344218624.00000, saving model to Weights\\Weights-041--1690218344218624.00000.hdf5\n",
      "Epoch 42/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1504835073474560.0000 - val_loss: 1683909439913984.0000\n",
      "\n",
      "Epoch 00042: val_loss improved from 1690218344218624.00000 to 1683909439913984.00000, saving model to Weights\\Weights-042--1683909439913984.00000.hdf5\n",
      "Epoch 43/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1498554489110528.0000 - val_loss: 1677305659260928.0000\n",
      "\n",
      "Epoch 00043: val_loss improved from 1683909439913984.00000 to 1677305659260928.00000, saving model to Weights\\Weights-043--1677305659260928.00000.hdf5\n",
      "Epoch 44/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1492790777217024.0000 - val_loss: 1671440344547328.0000\n",
      "\n",
      "Epoch 00044: val_loss improved from 1677305659260928.00000 to 1671440344547328.00000, saving model to Weights\\Weights-044--1671440344547328.00000.hdf5\n",
      "Epoch 45/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1486826275602432.0000 - val_loss: 1664861528391680.0000\n",
      "\n",
      "Epoch 00045: val_loss improved from 1671440344547328.00000 to 1664861528391680.00000, saving model to Weights\\Weights-045--1664861528391680.00000.hdf5\n",
      "Epoch 46/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1481009145053184.0000 - val_loss: 1659129894535168.0000\n",
      "\n",
      "Epoch 00046: val_loss improved from 1664861528391680.00000 to 1659129894535168.00000, saving model to Weights\\Weights-046--1659129894535168.00000.hdf5\n",
      "Epoch 47/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1475679895945216.0000 - val_loss: 1653324575145984.0000\n",
      "\n",
      "Epoch 00047: val_loss improved from 1659129894535168.00000 to 1653324575145984.00000, saving model to Weights\\Weights-047--1653324575145984.00000.hdf5\n",
      "Epoch 48/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1470088519614464.0000 - val_loss: 1647311486713856.0000\n",
      "\n",
      "Epoch 00048: val_loss improved from 1653324575145984.00000 to 1647311486713856.00000, saving model to Weights\\Weights-048--1647311486713856.00000.hdf5\n",
      "Epoch 49/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1464461978238976.0000 - val_loss: 1641726821269504.0000\n",
      "\n",
      "Epoch 00049: val_loss improved from 1647311486713856.00000 to 1641726821269504.00000, saving model to Weights\\Weights-049--1641726821269504.00000.hdf5\n",
      "Epoch 50/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1458933147369472.0000 - val_loss: 1636115983368192.0000\n",
      "\n",
      "Epoch 00050: val_loss improved from 1641726821269504.00000 to 1636115983368192.00000, saving model to Weights\\Weights-050--1636115983368192.00000.hdf5\n",
      "Epoch 51/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1453933537001472.0000 - val_loss: 1630753179828224.0000\n",
      "\n",
      "Epoch 00051: val_loss improved from 1636115983368192.00000 to 1630753179828224.00000, saving model to Weights\\Weights-051--1630753179828224.00000.hdf5\n",
      "Epoch 52/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1448594087346176.0000 - val_loss: 1624730830372864.0000\n",
      "\n",
      "Epoch 00052: val_loss improved from 1630753179828224.00000 to 1624730830372864.00000, saving model to Weights\\Weights-052--1624730830372864.00000.hdf5\n",
      "Epoch 53/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1443354629898240.0000 - val_loss: 1619552710426624.0000\n",
      "\n",
      "Epoch 00053: val_loss improved from 1624730830372864.00000 to 1619552710426624.00000, saving model to Weights\\Weights-053--1619552710426624.00000.hdf5\n",
      "Epoch 54/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1438369515044864.0000 - val_loss: 1614449349754880.0000\n",
      "\n",
      "Epoch 00054: val_loss improved from 1619552710426624.00000 to 1614449349754880.00000, saving model to Weights\\Weights-054--1614449349754880.00000.hdf5\n",
      "Epoch 55/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1433503048663040.0000 - val_loss: 1609360887250944.0000\n",
      "\n",
      "Epoch 00055: val_loss improved from 1614449349754880.00000 to 1609360887250944.00000, saving model to Weights\\Weights-055--1609360887250944.00000.hdf5\n",
      "Epoch 56/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1428424920924160.0000 - val_loss: 1604281417334784.0000\n",
      "\n",
      "Epoch 00056: val_loss improved from 1609360887250944.00000 to 1604281417334784.00000, saving model to Weights\\Weights-056--1604281417334784.00000.hdf5\n",
      "Epoch 57/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1423561138896896.0000 - val_loss: 1599425688371200.0000\n",
      "\n",
      "Epoch 00057: val_loss improved from 1604281417334784.00000 to 1599425688371200.00000, saving model to Weights\\Weights-057--1599425688371200.00000.hdf5\n",
      "Epoch 58/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1419192754503680.0000 - val_loss: 1594421917253632.0000\n",
      "\n",
      "Epoch 00058: val_loss improved from 1599425688371200.00000 to 1594421917253632.00000, saving model to Weights\\Weights-058--1594421917253632.00000.hdf5\n",
      "Epoch 59/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1414182004064256.0000 - val_loss: 1589931260510208.0000\n",
      "\n",
      "Epoch 00059: val_loss improved from 1594421917253632.00000 to 1589931260510208.00000, saving model to Weights\\Weights-059--1589931260510208.00000.hdf5\n",
      "Epoch 60/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1409203465879552.0000 - val_loss: 1584128357040128.0000\n",
      "\n",
      "Epoch 00060: val_loss improved from 1589931260510208.00000 to 1584128357040128.00000, saving model to Weights\\Weights-060--1584128357040128.00000.hdf5\n",
      "Epoch 61/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1404388807540736.0000 - val_loss: 1579264843448320.0000\n",
      "\n",
      "Epoch 00061: val_loss improved from 1584128357040128.00000 to 1579264843448320.00000, saving model to Weights\\Weights-061--1579264843448320.00000.hdf5\n",
      "Epoch 62/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1399540594769920.0000 - val_loss: 1574117190926336.0000\n",
      "\n",
      "Epoch 00062: val_loss improved from 1579264843448320.00000 to 1574117190926336.00000, saving model to Weights\\Weights-062--1574117190926336.00000.hdf5\n",
      "Epoch 63/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1394985647734784.0000 - val_loss: 1569915504951296.0000\n",
      "\n",
      "Epoch 00063: val_loss improved from 1574117190926336.00000 to 1569915504951296.00000, saving model to Weights\\Weights-063--1569915504951296.00000.hdf5\n",
      "Epoch 64/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1390704236429312.0000 - val_loss: 1564329899982848.0000\n",
      "\n",
      "Epoch 00064: val_loss improved from 1569915504951296.00000 to 1564329899982848.00000, saving model to Weights\\Weights-064--1564329899982848.00000.hdf5\n",
      "Epoch 65/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1385793343979520.0000 - val_loss: 1560037482823680.0000\n",
      "\n",
      "Epoch 00065: val_loss improved from 1564329899982848.00000 to 1560037482823680.00000, saving model to Weights\\Weights-065--1560037482823680.00000.hdf5\n",
      "Epoch 66/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1381733391925248.0000 - val_loss: 1555235038298112.0000\n",
      "\n",
      "Epoch 00066: val_loss improved from 1560037482823680.00000 to 1555235038298112.00000, saving model to Weights\\Weights-066--1555235038298112.00000.hdf5\n",
      "Epoch 67/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1376874844389376.0000 - val_loss: 1550641570775040.0000\n",
      "\n",
      "Epoch 00067: val_loss improved from 1555235038298112.00000 to 1550641570775040.00000, saving model to Weights\\Weights-067--1550641570775040.00000.hdf5\n",
      "Epoch 68/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1372310636331008.0000 - val_loss: 1545905564024832.0000\n",
      "\n",
      "Epoch 00068: val_loss improved from 1550641570775040.00000 to 1545905564024832.00000, saving model to Weights\\Weights-068--1545905564024832.00000.hdf5\n",
      "Epoch 69/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1367857694769152.0000 - val_loss: 1541794475016192.0000\n",
      "\n",
      "Epoch 00069: val_loss improved from 1545905564024832.00000 to 1541794475016192.00000, saving model to Weights\\Weights-069--1541794475016192.00000.hdf5\n",
      "Epoch 70/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1363581249519616.0000 - val_loss: 1536876737462272.0000\n",
      "\n",
      "Epoch 00070: val_loss improved from 1541794475016192.00000 to 1536876737462272.00000, saving model to Weights\\Weights-070--1536876737462272.00000.hdf5\n",
      "Epoch 71/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1359453920165888.0000 - val_loss: 1532994489679872.0000\n",
      "\n",
      "Epoch 00071: val_loss improved from 1536876737462272.00000 to 1532994489679872.00000, saving model to Weights\\Weights-071--1532994489679872.00000.hdf5\n",
      "Epoch 72/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1355486578343936.0000 - val_loss: 1527900658466816.0000\n",
      "\n",
      "Epoch 00072: val_loss improved from 1532994489679872.00000 to 1527900658466816.00000, saving model to Weights\\Weights-072--1527900658466816.00000.hdf5\n",
      "Epoch 73/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1350640110403584.0000 - val_loss: 1523647164448768.0000\n",
      "\n",
      "Epoch 00073: val_loss improved from 1527900658466816.00000 to 1523647164448768.00000, saving model to Weights\\Weights-073--1523647164448768.00000.hdf5\n",
      "Epoch 74/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1346350646034432.0000 - val_loss: 1519214758199296.0000\n",
      "\n",
      "Epoch 00074: val_loss improved from 1523647164448768.00000 to 1519214758199296.00000, saving model to Weights\\Weights-074--1519214758199296.00000.hdf5\n",
      "Epoch 75/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1342438870351872.0000 - val_loss: 1515272515092480.0000\n",
      "\n",
      "Epoch 00075: val_loss improved from 1519214758199296.00000 to 1515272515092480.00000, saving model to Weights\\Weights-075--1515272515092480.00000.hdf5\n",
      "Epoch 76/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1338078002151424.0000 - val_loss: 1510348335087616.0000\n",
      "\n",
      "Epoch 00076: val_loss improved from 1515272515092480.00000 to 1510348335087616.00000, saving model to Weights\\Weights-076--1510348335087616.00000.hdf5\n",
      "Epoch 77/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1334092272500736.0000 - val_loss: 1506074171539456.0000\n",
      "\n",
      "Epoch 00077: val_loss improved from 1510348335087616.00000 to 1506074171539456.00000, saving model to Weights\\Weights-077--1506074171539456.00000.hdf5\n",
      "Epoch 78/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1329815156162560.0000 - val_loss: 1502359830134784.0000\n",
      "\n",
      "Epoch 00078: val_loss improved from 1506074171539456.00000 to 1502359830134784.00000, saving model to Weights\\Weights-078--1502359830134784.00000.hdf5\n",
      "Epoch 79/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1325810367594496.0000 - val_loss: 1497792132415488.0000\n",
      "\n",
      "Epoch 00079: val_loss improved from 1502359830134784.00000 to 1497792132415488.00000, saving model to Weights\\Weights-079--1497792132415488.00000.hdf5\n",
      "Epoch 80/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1321462250078208.0000 - val_loss: 1493161083928576.0000\n",
      "\n",
      "Epoch 00080: val_loss improved from 1497792132415488.00000 to 1493161083928576.00000, saving model to Weights\\Weights-080--1493161083928576.00000.hdf5\n",
      "Epoch 81/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1317373843865600.0000 - val_loss: 1489704172126208.0000\n",
      "\n",
      "Epoch 00081: val_loss improved from 1493161083928576.00000 to 1489704172126208.00000, saving model to Weights\\Weights-081--1489704172126208.00000.hdf5\n",
      "Epoch 82/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1313254701793280.0000 - val_loss: 1484945079926784.0000\n",
      "\n",
      "Epoch 00082: val_loss improved from 1489704172126208.00000 to 1484945079926784.00000, saving model to Weights\\Weights-082--1484945079926784.00000.hdf5\n",
      "Epoch 83/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1309056639696896.0000 - val_loss: 1481031559413760.0000\n",
      "\n",
      "Epoch 00083: val_loss improved from 1484945079926784.00000 to 1481031559413760.00000, saving model to Weights\\Weights-083--1481031559413760.00000.hdf5\n",
      "Epoch 84/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1305344177340416.0000 - val_loss: 1476631231201280.0000\n",
      "\n",
      "Epoch 00084: val_loss improved from 1481031559413760.00000 to 1476631231201280.00000, saving model to Weights\\Weights-084--1476631231201280.00000.hdf5\n",
      "Epoch 85/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1301281406713856.0000 - val_loss: 1472819716161536.0000\n",
      "\n",
      "Epoch 00085: val_loss improved from 1476631231201280.00000 to 1472819716161536.00000, saving model to Weights\\Weights-085--1472819716161536.00000.hdf5\n",
      "Epoch 86/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1297526934208512.0000 - val_loss: 1468843247534080.0000\n",
      "\n",
      "Epoch 00086: val_loss improved from 1472819716161536.00000 to 1468843247534080.00000, saving model to Weights\\Weights-086--1468843247534080.00000.hdf5\n",
      "Epoch 87/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1293461747662848.0000 - val_loss: 1464763565473792.0000\n",
      "\n",
      "Epoch 00087: val_loss improved from 1468843247534080.00000 to 1464763565473792.00000, saving model to Weights\\Weights-087--1464763565473792.00000.hdf5\n",
      "Epoch 88/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1289483131551744.0000 - val_loss: 1460410481901568.0000\n",
      "\n",
      "Epoch 00088: val_loss improved from 1464763565473792.00000 to 1460410481901568.00000, saving model to Weights\\Weights-088--1460410481901568.00000.hdf5\n",
      "Epoch 89/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1285445728075776.0000 - val_loss: 1456728352751616.0000\n",
      "\n",
      "Epoch 00089: val_loss improved from 1460410481901568.00000 to 1456728352751616.00000, saving model to Weights\\Weights-089--1456728352751616.00000.hdf5\n",
      "Epoch 90/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1282032034381824.0000 - val_loss: 1452718195474432.0000\n",
      "\n",
      "Epoch 00090: val_loss improved from 1456728352751616.00000 to 1452718195474432.00000, saving model to Weights\\Weights-090--1452718195474432.00000.hdf5\n",
      "Epoch 91/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 1278197702328320.0000 - val_loss: 1448547379576832.0000\n",
      "\n",
      "Epoch 00091: val_loss improved from 1452718195474432.00000 to 1448547379576832.00000, saving model to Weights\\Weights-091--1448547379576832.00000.hdf5\n",
      "Epoch 92/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1274040308203520.0000 - val_loss: 1445458358566912.0000\n",
      "\n",
      "Epoch 00092: val_loss improved from 1448547379576832.00000 to 1445458358566912.00000, saving model to Weights\\Weights-092--1445458358566912.00000.hdf5\n",
      "Epoch 93/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1269830032293888.0000 - val_loss: 1440442776289280.0000\n",
      "\n",
      "Epoch 00093: val_loss improved from 1445458358566912.00000 to 1440442776289280.00000, saving model to Weights\\Weights-093--1440442776289280.00000.hdf5\n",
      "Epoch 94/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1266287892234240.0000 - val_loss: 1436622805532672.0000\n",
      "\n",
      "Epoch 00094: val_loss improved from 1440442776289280.00000 to 1436622805532672.00000, saving model to Weights\\Weights-094--1436622805532672.00000.hdf5\n",
      "Epoch 95/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1262866145476608.0000 - val_loss: 1432715056381952.0000\n",
      "\n",
      "Epoch 00095: val_loss improved from 1436622805532672.00000 to 1432715056381952.00000, saving model to Weights\\Weights-095--1432715056381952.00000.hdf5\n",
      "Epoch 96/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1258604195741696.0000 - val_loss: 1430447179431936.0000\n",
      "\n",
      "Epoch 00096: val_loss improved from 1432715056381952.00000 to 1430447179431936.00000, saving model to Weights\\Weights-096--1430447179431936.00000.hdf5\n",
      "Epoch 97/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1255307539906560.0000 - val_loss: 1424913382506496.0000\n",
      "\n",
      "Epoch 00097: val_loss improved from 1430447179431936.00000 to 1424913382506496.00000, saving model to Weights\\Weights-097--1424913382506496.00000.hdf5\n",
      "Epoch 98/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1251105988149248.0000 - val_loss: 1420900406657024.0000\n",
      "\n",
      "Epoch 00098: val_loss improved from 1424913382506496.00000 to 1420900406657024.00000, saving model to Weights\\Weights-098--1420900406657024.00000.hdf5\n",
      "Epoch 99/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1247496605007872.0000 - val_loss: 1416847836577792.0000\n",
      "\n",
      "Epoch 00099: val_loss improved from 1420900406657024.00000 to 1416847836577792.00000, saving model to Weights\\Weights-099--1416847836577792.00000.hdf5\n",
      "Epoch 100/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1243642006077440.0000 - val_loss: 1413144635244544.0000\n",
      "\n",
      "Epoch 00100: val_loss improved from 1416847836577792.00000 to 1413144635244544.00000, saving model to Weights\\Weights-100--1413144635244544.00000.hdf5\n",
      "Epoch 101/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1239520313868288.0000 - val_loss: 1409064818966528.0000\n",
      "\n",
      "Epoch 00101: val_loss improved from 1413144635244544.00000 to 1409064818966528.00000, saving model to Weights\\Weights-101--1409064818966528.00000.hdf5\n",
      "Epoch 102/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1235924218281984.0000 - val_loss: 1405609249341440.0000\n",
      "\n",
      "Epoch 00102: val_loss improved from 1409064818966528.00000 to 1405609249341440.00000, saving model to Weights\\Weights-102--1405609249341440.00000.hdf5\n",
      "Epoch 103/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1232201421160448.0000 - val_loss: 1401682575491072.0000\n",
      "\n",
      "Epoch 00103: val_loss improved from 1405609249341440.00000 to 1401682575491072.00000, saving model to Weights\\Weights-103--1401682575491072.00000.hdf5\n",
      "Epoch 104/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1228395677483008.0000 - val_loss: 1397970650005504.0000\n",
      "\n",
      "Epoch 00104: val_loss improved from 1401682575491072.00000 to 1397970650005504.00000, saving model to Weights\\Weights-104--1397970650005504.00000.hdf5\n",
      "Epoch 105/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1225325849608192.0000 - val_loss: 1393689104482304.0000\n",
      "\n",
      "Epoch 00105: val_loss improved from 1397970650005504.00000 to 1393689104482304.00000, saving model to Weights\\Weights-105--1393689104482304.00000.hdf5\n",
      "Epoch 106/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1221070610759680.0000 - val_loss: 1389738808311808.0000\n",
      "\n",
      "Epoch 00106: val_loss improved from 1393689104482304.00000 to 1389738808311808.00000, saving model to Weights\\Weights-106--1389738808311808.00000.hdf5\n",
      "Epoch 107/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1217361235410944.0000 - val_loss: 1386127948775424.0000\n",
      "\n",
      "Epoch 00107: val_loss improved from 1389738808311808.00000 to 1386127948775424.00000, saving model to Weights\\Weights-107--1386127948775424.00000.hdf5\n",
      "Epoch 108/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1214258255757312.0000 - val_loss: 1382243687727104.0000\n",
      "\n",
      "Epoch 00108: val_loss improved from 1386127948775424.00000 to 1382243687727104.00000, saving model to Weights\\Weights-108--1382243687727104.00000.hdf5\n",
      "Epoch 109/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1210307154280448.0000 - val_loss: 1378422509010944.0000\n",
      "\n",
      "Epoch 00109: val_loss improved from 1382243687727104.00000 to 1378422509010944.00000, saving model to Weights\\Weights-109--1378422509010944.00000.hdf5\n",
      "Epoch 110/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1206953321693184.0000 - val_loss: 1374581063417856.0000\n",
      "\n",
      "Epoch 00110: val_loss improved from 1378422509010944.00000 to 1374581063417856.00000, saving model to Weights\\Weights-110--1374581063417856.00000.hdf5\n",
      "Epoch 111/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1203421247963136.0000 - val_loss: 1370559229198336.0000\n",
      "\n",
      "Epoch 00111: val_loss improved from 1374581063417856.00000 to 1370559229198336.00000, saving model to Weights\\Weights-111--1370559229198336.00000.hdf5\n",
      "Epoch 112/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1199347203047424.0000 - val_loss: 1367173452791808.0000\n",
      "\n",
      "Epoch 00112: val_loss improved from 1370559229198336.00000 to 1367173452791808.00000, saving model to Weights\\Weights-112--1367173452791808.00000.hdf5\n",
      "Epoch 113/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1195731243237376.0000 - val_loss: 1363088402022400.0000\n",
      "\n",
      "Epoch 00113: val_loss improved from 1367173452791808.00000 to 1363088402022400.00000, saving model to Weights\\Weights-113--1363088402022400.00000.hdf5\n",
      "Epoch 114/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1192378484391936.0000 - val_loss: 1359541027471360.0000\n",
      "\n",
      "Epoch 00114: val_loss improved from 1363088402022400.00000 to 1359541027471360.00000, saving model to Weights\\Weights-114--1359541027471360.00000.hdf5\n",
      "Epoch 115/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1188331417239552.0000 - val_loss: 1356061701308416.0000\n",
      "\n",
      "Epoch 00115: val_loss improved from 1359541027471360.00000 to 1356061701308416.00000, saving model to Weights\\Weights-115--1356061701308416.00000.hdf5\n",
      "Epoch 116/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1185289103998976.0000 - val_loss: 1352812692766720.0000\n",
      "\n",
      "Epoch 00116: val_loss improved from 1356061701308416.00000 to 1352812692766720.00000, saving model to Weights\\Weights-116--1352812692766720.00000.hdf5\n",
      "Epoch 117/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1182062912471040.0000 - val_loss: 1348444039938048.0000\n",
      "\n",
      "Epoch 00117: val_loss improved from 1352812692766720.00000 to 1348444039938048.00000, saving model to Weights\\Weights-117--1348444039938048.00000.hdf5\n",
      "Epoch 118/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1177978667008000.0000 - val_loss: 1344418716057600.0000\n",
      "\n",
      "Epoch 00118: val_loss improved from 1348444039938048.00000 to 1344418716057600.00000, saving model to Weights\\Weights-118--1344418716057600.00000.hdf5\n",
      "Epoch 119/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1174970176634880.0000 - val_loss: 1340723030917120.0000\n",
      "\n",
      "Epoch 00119: val_loss improved from 1344418716057600.00000 to 1340723030917120.00000, saving model to Weights\\Weights-119--1340723030917120.00000.hdf5\n",
      "Epoch 120/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1171563999133696.0000 - val_loss: 1337120761315328.0000\n",
      "\n",
      "Epoch 00120: val_loss improved from 1340723030917120.00000 to 1337120761315328.00000, saving model to Weights\\Weights-120--1337120761315328.00000.hdf5\n",
      "Epoch 121/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1168217816956928.0000 - val_loss: 1333370181124096.0000\n",
      "\n",
      "Epoch 00121: val_loss improved from 1337120761315328.00000 to 1333370181124096.00000, saving model to Weights\\Weights-121--1333370181124096.00000.hdf5\n",
      "Epoch 122/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1164184171577344.0000 - val_loss: 1329626043383808.0000\n",
      "\n",
      "Epoch 00122: val_loss improved from 1333370181124096.00000 to 1329626043383808.00000, saving model to Weights\\Weights-122--1329626043383808.00000.hdf5\n",
      "Epoch 123/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1160695852826624.0000 - val_loss: 1326108733603840.0000\n",
      "\n",
      "Epoch 00123: val_loss improved from 1329626043383808.00000 to 1326108733603840.00000, saving model to Weights\\Weights-123--1326108733603840.00000.hdf5\n",
      "Epoch 124/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1157690046808064.0000 - val_loss: 1322965488631808.0000\n",
      "\n",
      "Epoch 00124: val_loss improved from 1326108733603840.00000 to 1322965488631808.00000, saving model to Weights\\Weights-124--1322965488631808.00000.hdf5\n",
      "Epoch 125/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1154211928604672.0000 - val_loss: 1318979222110208.0000\n",
      "\n",
      "Epoch 00125: val_loss improved from 1322965488631808.00000 to 1318979222110208.00000, saving model to Weights\\Weights-125--1318979222110208.00000.hdf5\n",
      "Epoch 126/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1151152569712640.0000 - val_loss: 1314891621203968.0000\n",
      "\n",
      "Epoch 00126: val_loss improved from 1318979222110208.00000 to 1314891621203968.00000, saving model to Weights\\Weights-126--1314891621203968.00000.hdf5\n",
      "Epoch 127/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1147292736290816.0000 - val_loss: 1311873601372160.0000\n",
      "\n",
      "Epoch 00127: val_loss improved from 1314891621203968.00000 to 1311873601372160.00000, saving model to Weights\\Weights-127--1311873601372160.00000.hdf5\n",
      "Epoch 128/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1144252167880704.0000 - val_loss: 1308089466748928.0000\n",
      "\n",
      "Epoch 00128: val_loss improved from 1311873601372160.00000 to 1308089466748928.00000, saving model to Weights\\Weights-128--1308089466748928.00000.hdf5\n",
      "Epoch 129/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1140846124597248.0000 - val_loss: 1304310700834816.0000\n",
      "\n",
      "Epoch 00129: val_loss improved from 1308089466748928.00000 to 1304310700834816.00000, saving model to Weights\\Weights-129--1304310700834816.00000.hdf5\n",
      "Epoch 130/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1137811864420352.0000 - val_loss: 1300949352054784.0000\n",
      "\n",
      "Epoch 00130: val_loss improved from 1304310700834816.00000 to 1300949352054784.00000, saving model to Weights\\Weights-130--1300949352054784.00000.hdf5\n",
      "Epoch 131/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1134551984242688.0000 - val_loss: 1298465384562688.0000\n",
      "\n",
      "Epoch 00131: val_loss improved from 1300949352054784.00000 to 1298465384562688.00000, saving model to Weights\\Weights-131--1298465384562688.00000.hdf5\n",
      "Epoch 132/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1131247812214784.0000 - val_loss: 1293553015717888.0000\n",
      "\n",
      "Epoch 00132: val_loss improved from 1298465384562688.00000 to 1293553015717888.00000, saving model to Weights\\Weights-132--1293553015717888.00000.hdf5\n",
      "Epoch 133/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1127865122816000.0000 - val_loss: 1289754519797760.0000\n",
      "\n",
      "Epoch 00133: val_loss improved from 1293553015717888.00000 to 1289754519797760.00000, saving model to Weights\\Weights-133--1289754519797760.00000.hdf5\n",
      "Epoch 134/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1124334391263232.0000 - val_loss: 1286807501144064.0000\n",
      "\n",
      "Epoch 00134: val_loss improved from 1289754519797760.00000 to 1286807501144064.00000, saving model to Weights\\Weights-134--1286807501144064.00000.hdf5\n",
      "Epoch 135/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1121447971913728.0000 - val_loss: 1283166308401152.0000\n",
      "\n",
      "Epoch 00135: val_loss improved from 1286807501144064.00000 to 1283166308401152.00000, saving model to Weights\\Weights-135--1283166308401152.00000.hdf5\n",
      "Epoch 136/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1117855298879488.0000 - val_loss: 1279614638882816.0000\n",
      "\n",
      "Epoch 00136: val_loss improved from 1283166308401152.00000 to 1279614638882816.00000, saving model to Weights\\Weights-136--1279614638882816.00000.hdf5\n",
      "Epoch 137/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1114704604823552.0000 - val_loss: 1276076928008192.0000\n",
      "\n",
      "Epoch 00137: val_loss improved from 1279614638882816.00000 to 1276076928008192.00000, saving model to Weights\\Weights-137--1276076928008192.00000.hdf5\n",
      "Epoch 138/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1111727722725376.0000 - val_loss: 1272930864463872.0000\n",
      "\n",
      "Epoch 00138: val_loss improved from 1276076928008192.00000 to 1272930864463872.00000, saving model to Weights\\Weights-138--1272930864463872.00000.hdf5\n",
      "Epoch 139/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1108379124629504.0000 - val_loss: 1269546430234624.0000\n",
      "\n",
      "Epoch 00139: val_loss improved from 1272930864463872.00000 to 1269546430234624.00000, saving model to Weights\\Weights-139--1269546430234624.00000.hdf5\n",
      "Epoch 140/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1105370768474112.0000 - val_loss: 1266138105249792.0000\n",
      "\n",
      "Epoch 00140: val_loss improved from 1269546430234624.00000 to 1266138105249792.00000, saving model to Weights\\Weights-140--1266138105249792.00000.hdf5\n",
      "Epoch 141/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1102500522360832.0000 - val_loss: 1263377515020288.0000\n",
      "\n",
      "Epoch 00141: val_loss improved from 1266138105249792.00000 to 1263377515020288.00000, saving model to Weights\\Weights-141--1263377515020288.00000.hdf5\n",
      "Epoch 142/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1099326742855680.0000 - val_loss: 1258842969079808.0000\n",
      "\n",
      "Epoch 00142: val_loss improved from 1263377515020288.00000 to 1258842969079808.00000, saving model to Weights\\Weights-142--1258842969079808.00000.hdf5\n",
      "Epoch 143/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1096515619651584.0000 - val_loss: 1256312025382912.0000\n",
      "\n",
      "Epoch 00143: val_loss improved from 1258842969079808.00000 to 1256312025382912.00000, saving model to Weights\\Weights-143--1256312025382912.00000.hdf5\n",
      "Epoch 144/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1093562963853312.0000 - val_loss: 1252199996850176.0000\n",
      "\n",
      "Epoch 00144: val_loss improved from 1256312025382912.00000 to 1252199996850176.00000, saving model to Weights\\Weights-144--1252199996850176.00000.hdf5\n",
      "Epoch 145/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1090356502331392.0000 - val_loss: 1249406657495040.0000\n",
      "\n",
      "Epoch 00145: val_loss improved from 1252199996850176.00000 to 1249406657495040.00000, saving model to Weights\\Weights-145--1249406657495040.00000.hdf5\n",
      "Epoch 146/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1087253589786624.0000 - val_loss: 1245750163931136.0000\n",
      "\n",
      "Epoch 00146: val_loss improved from 1249406657495040.00000 to 1245750163931136.00000, saving model to Weights\\Weights-146--1245750163931136.00000.hdf5\n",
      "Epoch 147/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1084256977682432.0000 - val_loss: 1242123466702848.0000\n",
      "\n",
      "Epoch 00147: val_loss improved from 1245750163931136.00000 to 1242123466702848.00000, saving model to Weights\\Weights-147--1242123466702848.00000.hdf5\n",
      "Epoch 148/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1081381966839808.0000 - val_loss: 1239091890880512.0000\n",
      "\n",
      "Epoch 00148: val_loss improved from 1242123466702848.00000 to 1239091890880512.00000, saving model to Weights\\Weights-148--1239091890880512.00000.hdf5\n",
      "Epoch 149/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1078534336413696.0000 - val_loss: 1236226476605440.0000\n",
      "\n",
      "Epoch 00149: val_loss improved from 1239091890880512.00000 to 1236226476605440.00000, saving model to Weights\\Weights-149--1236226476605440.00000.hdf5\n",
      "Epoch 150/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1075874006827008.0000 - val_loss: 1232549850382336.0000\n",
      "\n",
      "Epoch 00150: val_loss improved from 1236226476605440.00000 to 1232549850382336.00000, saving model to Weights\\Weights-150--1232549850382336.00000.hdf5\n",
      "Epoch 151/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1072839813758976.0000 - val_loss: 1229085690822656.0000\n",
      "\n",
      "Epoch 00151: val_loss improved from 1232549850382336.00000 to 1229085690822656.00000, saving model to Weights\\Weights-151--1229085690822656.00000.hdf5\n",
      "Epoch 152/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1070258873958400.0000 - val_loss: 1226138537951232.0000\n",
      "\n",
      "Epoch 00152: val_loss improved from 1229085690822656.00000 to 1226138537951232.00000, saving model to Weights\\Weights-152--1226138537951232.00000.hdf5\n",
      "Epoch 153/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1067268502978560.0000 - val_loss: 1222537476308992.0000\n",
      "\n",
      "Epoch 00153: val_loss improved from 1226138537951232.00000 to 1222537476308992.00000, saving model to Weights\\Weights-153--1222537476308992.00000.hdf5\n",
      "Epoch 154/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1064250080493568.0000 - val_loss: 1221582919827456.0000\n",
      "\n",
      "Epoch 00154: val_loss improved from 1222537476308992.00000 to 1221582919827456.00000, saving model to Weights\\Weights-154--1221582919827456.00000.hdf5\n",
      "Epoch 155/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1060893429334016.0000 - val_loss: 1216470029697024.0000\n",
      "\n",
      "Epoch 00155: val_loss improved from 1221582919827456.00000 to 1216470029697024.00000, saving model to Weights\\Weights-155--1216470029697024.00000.hdf5\n",
      "Epoch 156/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1058199947968512.0000 - val_loss: 1213115391803392.0000\n",
      "\n",
      "Epoch 00156: val_loss improved from 1216470029697024.00000 to 1213115391803392.00000, saving model to Weights\\Weights-156--1213115391803392.00000.hdf5\n",
      "Epoch 157/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1056116284850176.0000 - val_loss: 1209419840880640.0000\n",
      "\n",
      "Epoch 00157: val_loss improved from 1213115391803392.00000 to 1209419840880640.00000, saving model to Weights\\Weights-157--1209419840880640.00000.hdf5\n",
      "Epoch 158/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1052773458116608.0000 - val_loss: 1206640997040128.0000\n",
      "\n",
      "Epoch 00158: val_loss improved from 1209419840880640.00000 to 1206640997040128.00000, saving model to Weights\\Weights-158--1206640997040128.00000.hdf5\n",
      "Epoch 159/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1050868975665152.0000 - val_loss: 1203788065013760.0000\n",
      "\n",
      "Epoch 00159: val_loss improved from 1206640997040128.00000 to 1203788065013760.00000, saving model to Weights\\Weights-159--1203788065013760.00000.hdf5\n",
      "Epoch 160/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1047178256580608.0000 - val_loss: 1201592531419136.0000\n",
      "\n",
      "Epoch 00160: val_loss improved from 1203788065013760.00000 to 1201592531419136.00000, saving model to Weights\\Weights-160--1201592531419136.00000.hdf5\n",
      "Epoch 161/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1045033390178304.0000 - val_loss: 1197475402612736.0000\n",
      "\n",
      "Epoch 00161: val_loss improved from 1201592531419136.00000 to 1197475402612736.00000, saving model to Weights\\Weights-161--1197475402612736.00000.hdf5\n",
      "Epoch 162/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1042353531912192.0000 - val_loss: 1195042437857280.0000\n",
      "\n",
      "Epoch 00162: val_loss improved from 1197475402612736.00000 to 1195042437857280.00000, saving model to Weights\\Weights-162--1195042437857280.00000.hdf5\n",
      "Epoch 163/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1039652333027328.0000 - val_loss: 1191602034835456.0000\n",
      "\n",
      "Epoch 00163: val_loss improved from 1195042437857280.00000 to 1191602034835456.00000, saving model to Weights\\Weights-163--1191602034835456.00000.hdf5\n",
      "Epoch 164/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1036819198115840.0000 - val_loss: 1188459058298880.0000\n",
      "\n",
      "Epoch 00164: val_loss improved from 1191602034835456.00000 to 1188459058298880.00000, saving model to Weights\\Weights-164--1188459058298880.00000.hdf5\n",
      "Epoch 165/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1035149328252928.0000 - val_loss: 1185560492244992.0000\n",
      "\n",
      "Epoch 00165: val_loss improved from 1188459058298880.00000 to 1185560492244992.00000, saving model to Weights\\Weights-165--1185560492244992.00000.hdf5\n",
      "Epoch 166/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1032203718885376.0000 - val_loss: 1182515226214400.0000\n",
      "\n",
      "Epoch 00166: val_loss improved from 1185560492244992.00000 to 1182515226214400.00000, saving model to Weights\\Weights-166--1182515226214400.00000.hdf5\n",
      "Epoch 167/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1029246097031168.0000 - val_loss: 1179584850558976.0000\n",
      "\n",
      "Epoch 00167: val_loss improved from 1182515226214400.00000 to 1179584850558976.00000, saving model to Weights\\Weights-167--1179584850558976.00000.hdf5\n",
      "Epoch 168/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1026694819348480.0000 - val_loss: 1176994716844032.0000\n",
      "\n",
      "Epoch 00168: val_loss improved from 1179584850558976.00000 to 1176994716844032.00000, saving model to Weights\\Weights-168--1176994716844032.00000.hdf5\n",
      "Epoch 169/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1024041737519104.0000 - val_loss: 1174347943247872.0000\n",
      "\n",
      "Epoch 00169: val_loss improved from 1176994716844032.00000 to 1174347943247872.00000, saving model to Weights\\Weights-169--1174347943247872.00000.hdf5\n",
      "Epoch 170/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1021851773960192.0000 - val_loss: 1171370591387648.0000\n",
      "\n",
      "Epoch 00170: val_loss improved from 1174347943247872.00000 to 1171370591387648.00000, saving model to Weights\\Weights-170--1171370591387648.00000.hdf5\n",
      "Epoch 171/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1019529136177152.0000 - val_loss: 1168550542704640.0000\n",
      "\n",
      "Epoch 00171: val_loss improved from 1171370591387648.00000 to 1168550542704640.00000, saving model to Weights\\Weights-171--1168550542704640.00000.hdf5\n",
      "Epoch 172/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1016917057863680.0000 - val_loss: 1166172338782208.0000\n",
      "\n",
      "Epoch 00172: val_loss improved from 1168550542704640.00000 to 1166172338782208.00000, saving model to Weights\\Weights-172--1166172338782208.00000.hdf5\n",
      "Epoch 173/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 1014615894917120.0000 - val_loss: 1162657310703616.0000\n",
      "\n",
      "Epoch 00173: val_loss improved from 1166172338782208.00000 to 1162657310703616.00000, saving model to Weights\\Weights-173--1162657310703616.00000.hdf5\n",
      "Epoch 174/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1012562095243264.0000 - val_loss: 1159870682234880.0000\n",
      "\n",
      "Epoch 00174: val_loss improved from 1162657310703616.00000 to 1159870682234880.00000, saving model to Weights\\Weights-174--1159870682234880.00000.hdf5\n",
      "Epoch 175/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1010007931879424.0000 - val_loss: 1157264173957120.0000\n",
      "\n",
      "Epoch 00175: val_loss improved from 1159870682234880.00000 to 1157264173957120.00000, saving model to Weights\\Weights-175--1157264173957120.00000.hdf5\n",
      "Epoch 176/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1007844744757248.0000 - val_loss: 1154578074566656.0000\n",
      "\n",
      "Epoch 00176: val_loss improved from 1157264173957120.00000 to 1154578074566656.00000, saving model to Weights\\Weights-176--1154578074566656.00000.hdf5\n",
      "Epoch 177/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1005265549787136.0000 - val_loss: 1151579784740864.0000\n",
      "\n",
      "Epoch 00177: val_loss improved from 1154578074566656.00000 to 1151579784740864.00000, saving model to Weights\\Weights-177--1151579784740864.00000.hdf5\n",
      "Epoch 178/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1002940630302720.0000 - val_loss: 1148885900722176.0000\n",
      "\n",
      "Epoch 00178: val_loss improved from 1151579784740864.00000 to 1148885900722176.00000, saving model to Weights\\Weights-178--1148885900722176.00000.hdf5\n",
      "Epoch 179/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1001085875519488.0000 - val_loss: 1146880419430400.0000\n",
      "\n",
      "Epoch 00179: val_loss improved from 1148885900722176.00000 to 1146880419430400.00000, saving model to Weights\\Weights-179--1146880419430400.00000.hdf5\n",
      "Epoch 180/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 998813233840128.0000 - val_loss: 1143764018003968.0000\n",
      "\n",
      "Epoch 00180: val_loss improved from 1146880419430400.00000 to 1143764018003968.00000, saving model to Weights\\Weights-180--1143764018003968.00000.hdf5\n",
      "Epoch 181/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 996327387299840.0000 - val_loss: 1140949740683264.0000\n",
      "\n",
      "Epoch 00181: val_loss improved from 1143764018003968.00000 to 1140949740683264.00000, saving model to Weights\\Weights-181--1140949740683264.00000.hdf5\n",
      "Epoch 182/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 994705701601280.0000 - val_loss: 1137784618221568.0000\n",
      "\n",
      "Epoch 00182: val_loss improved from 1140949740683264.00000 to 1137784618221568.00000, saving model to Weights\\Weights-182--1137784618221568.00000.hdf5\n",
      "Epoch 183/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 991895920574464.0000 - val_loss: 1135744374538240.0000\n",
      "\n",
      "Epoch 00183: val_loss improved from 1137784618221568.00000 to 1135744374538240.00000, saving model to Weights\\Weights-183--1135744374538240.00000.hdf5\n",
      "Epoch 184/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 989070436073472.0000 - val_loss: 1132523551719424.0000\n",
      "\n",
      "Epoch 00184: val_loss improved from 1135744374538240.00000 to 1132523551719424.00000, saving model to Weights\\Weights-184--1132523551719424.00000.hdf5\n",
      "Epoch 185/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 987640346181632.0000 - val_loss: 1129786046939136.0000\n",
      "\n",
      "Epoch 00185: val_loss improved from 1132523551719424.00000 to 1129786046939136.00000, saving model to Weights\\Weights-185--1129786046939136.00000.hdf5\n",
      "Epoch 186/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 985463804395520.0000 - val_loss: 1127175512129536.0000\n",
      "\n",
      "Epoch 00186: val_loss improved from 1129786046939136.00000 to 1127175512129536.00000, saving model to Weights\\Weights-186--1127175512129536.00000.hdf5\n",
      "Epoch 187/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 982635836866560.0000 - val_loss: 1125048899338240.0000\n",
      "\n",
      "Epoch 00187: val_loss improved from 1127175512129536.00000 to 1125048899338240.00000, saving model to Weights\\Weights-187--1125048899338240.00000.hdf5\n",
      "Epoch 188/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 981684434501632.0000 - val_loss: 1122101880684544.0000\n",
      "\n",
      "Epoch 00188: val_loss improved from 1125048899338240.00000 to 1122101880684544.00000, saving model to Weights\\Weights-188--1122101880684544.00000.hdf5\n",
      "Epoch 189/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 978623464996864.0000 - val_loss: 1119675559706624.0000\n",
      "\n",
      "Epoch 00189: val_loss improved from 1122101880684544.00000 to 1119675559706624.00000, saving model to Weights\\Weights-189--1119675559706624.00000.hdf5\n",
      "Epoch 190/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 976866957590528.0000 - val_loss: 1117274740097024.0000\n",
      "\n",
      "Epoch 00190: val_loss improved from 1119675559706624.00000 to 1117274740097024.00000, saving model to Weights\\Weights-190--1117274740097024.00000.hdf5\n",
      "Epoch 191/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 974455400562688.0000 - val_loss: 1114958410547200.0000\n",
      "\n",
      "Epoch 00191: val_loss improved from 1117274740097024.00000 to 1114958410547200.00000, saving model to Weights\\Weights-191--1114958410547200.00000.hdf5\n",
      "Epoch 192/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 972034314076160.0000 - val_loss: 1112150709895168.0000\n",
      "\n",
      "Epoch 00192: val_loss improved from 1114958410547200.00000 to 1112150709895168.00000, saving model to Weights\\Weights-192--1112150709895168.00000.hdf5\n",
      "Epoch 193/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 970274719662080.0000 - val_loss: 1109505882456064.0000\n",
      "\n",
      "Epoch 00193: val_loss improved from 1112150709895168.00000 to 1109505882456064.00000, saving model to Weights\\Weights-193--1109505882456064.00000.hdf5\n",
      "Epoch 194/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 968106633592832.0000 - val_loss: 1106605840007168.0000\n",
      "\n",
      "Epoch 00194: val_loss improved from 1109505882456064.00000 to 1106605840007168.00000, saving model to Weights\\Weights-194--1106605840007168.00000.hdf5\n",
      "Epoch 195/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 966803580780544.0000 - val_loss: 1104635121106944.0000\n",
      "\n",
      "Epoch 00195: val_loss improved from 1106605840007168.00000 to 1104635121106944.00000, saving model to Weights\\Weights-195--1104635121106944.00000.hdf5\n",
      "Epoch 196/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 964270422491136.0000 - val_loss: 1104332057477120.0000\n",
      "\n",
      "Epoch 00196: val_loss improved from 1104635121106944.00000 to 1104332057477120.00000, saving model to Weights\\Weights-196--1104332057477120.00000.hdf5\n",
      "Epoch 197/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 962477005209600.0000 - val_loss: 1099711746408448.0000\n",
      "\n",
      "Epoch 00197: val_loss improved from 1104332057477120.00000 to 1099711746408448.00000, saving model to Weights\\Weights-197--1099711746408448.00000.hdf5\n",
      "Epoch 198/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 960173828997120.0000 - val_loss: 1097305222545408.0000\n",
      "\n",
      "Epoch 00198: val_loss improved from 1099711746408448.00000 to 1097305222545408.00000, saving model to Weights\\Weights-198--1097305222545408.00000.hdf5\n",
      "Epoch 199/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 958365245112320.0000 - val_loss: 1095965863837696.0000\n",
      "\n",
      "Epoch 00199: val_loss improved from 1097305222545408.00000 to 1095965863837696.00000, saving model to Weights\\Weights-199--1095965863837696.00000.hdf5\n",
      "Epoch 200/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 956426470031360.0000 - val_loss: 1092574316068864.0000\n",
      "\n",
      "Epoch 00200: val_loss improved from 1095965863837696.00000 to 1092574316068864.00000, saving model to Weights\\Weights-200--1092574316068864.00000.hdf5\n",
      "Epoch 201/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 954540241190912.0000 - val_loss: 1090702582743040.0000\n",
      "\n",
      "Epoch 00201: val_loss improved from 1092574316068864.00000 to 1090702582743040.00000, saving model to Weights\\Weights-201--1090702582743040.00000.hdf5\n",
      "Epoch 202/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 952781452083200.0000 - val_loss: 1087908907843584.0000\n",
      "\n",
      "Epoch 00202: val_loss improved from 1090702582743040.00000 to 1087908907843584.00000, saving model to Weights\\Weights-202--1087908907843584.00000.hdf5\n",
      "Epoch 203/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 950927703932928.0000 - val_loss: 1085239115907072.0000\n",
      "\n",
      "Epoch 00203: val_loss improved from 1087908907843584.00000 to 1085239115907072.00000, saving model to Weights\\Weights-203--1085239115907072.00000.hdf5\n",
      "Epoch 204/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 949385542238208.0000 - val_loss: 1082936946327552.0000\n",
      "\n",
      "Epoch 00204: val_loss improved from 1085239115907072.00000 to 1082936946327552.00000, saving model to Weights\\Weights-204--1082936946327552.00000.hdf5\n",
      "Epoch 205/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 947221952462848.0000 - val_loss: 1080841941811200.0000\n",
      "\n",
      "Epoch 00205: val_loss improved from 1082936946327552.00000 to 1080841941811200.00000, saving model to Weights\\Weights-205--1080841941811200.00000.hdf5\n",
      "Epoch 206/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 945100037292032.0000 - val_loss: 1078595338371072.0000\n",
      "\n",
      "Epoch 00206: val_loss improved from 1080841941811200.00000 to 1078595338371072.00000, saving model to Weights\\Weights-206--1078595338371072.00000.hdf5\n",
      "Epoch 207/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 943427080421376.0000 - val_loss: 1076532344782848.0000\n",
      "\n",
      "Epoch 00207: val_loss improved from 1078595338371072.00000 to 1076532344782848.00000, saving model to Weights\\Weights-207--1076532344782848.00000.hdf5\n",
      "Epoch 208/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 942650362429440.0000 - val_loss: 1073473522761728.0000\n",
      "\n",
      "Epoch 00208: val_loss improved from 1076532344782848.00000 to 1073473522761728.00000, saving model to Weights\\Weights-208--1073473522761728.00000.hdf5\n",
      "Epoch 209/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 940127606013952.0000 - val_loss: 1071975518699520.0000\n",
      "\n",
      "Epoch 00209: val_loss improved from 1073473522761728.00000 to 1071975518699520.00000, saving model to Weights\\Weights-209--1071975518699520.00000.hdf5\n",
      "Epoch 210/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 938062934704128.0000 - val_loss: 1069226068541440.0000\n",
      "\n",
      "Epoch 00210: val_loss improved from 1071975518699520.00000 to 1069226068541440.00000, saving model to Weights\\Weights-210--1069226068541440.00000.hdf5\n",
      "Epoch 211/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 936430914240512.0000 - val_loss: 1067829197537280.0000\n",
      "\n",
      "Epoch 00211: val_loss improved from 1069226068541440.00000 to 1067829197537280.00000, saving model to Weights\\Weights-211--1067829197537280.00000.hdf5\n",
      "Epoch 212/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 934399998689280.0000 - val_loss: 1065050823458816.0000\n",
      "\n",
      "Epoch 00212: val_loss improved from 1067829197537280.00000 to 1065050823458816.00000, saving model to Weights\\Weights-212--1065050823458816.00000.hdf5\n",
      "Epoch 213/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 933091711385600.0000 - val_loss: 1062360898863104.0000\n",
      "\n",
      "Epoch 00213: val_loss improved from 1065050823458816.00000 to 1062360898863104.00000, saving model to Weights\\Weights-213--1062360898863104.00000.hdf5\n",
      "Epoch 214/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 930997780611072.0000 - val_loss: 1060249117130752.0000\n",
      "\n",
      "Epoch 00214: val_loss improved from 1062360898863104.00000 to 1060249117130752.00000, saving model to Weights\\Weights-214--1060249117130752.00000.hdf5\n",
      "Epoch 215/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 929471993479168.0000 - val_loss: 1058637631979520.0000\n",
      "\n",
      "Epoch 00215: val_loss improved from 1060249117130752.00000 to 1058637631979520.00000, saving model to Weights\\Weights-215--1058637631979520.00000.hdf5\n",
      "Epoch 216/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 928035259809792.0000 - val_loss: 1056348817063936.0000\n",
      "\n",
      "Epoch 00216: val_loss improved from 1058637631979520.00000 to 1056348817063936.00000, saving model to Weights\\Weights-216--1056348817063936.00000.hdf5\n",
      "Epoch 217/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 926504909275136.0000 - val_loss: 1053299725828096.0000\n",
      "\n",
      "Epoch 00217: val_loss improved from 1056348817063936.00000 to 1053299725828096.00000, saving model to Weights\\Weights-217--1053299725828096.00000.hdf5\n",
      "Epoch 218/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 924571368685568.0000 - val_loss: 1051259549253632.0000\n",
      "\n",
      "Epoch 00218: val_loss improved from 1053299725828096.00000 to 1051259549253632.00000, saving model to Weights\\Weights-218--1051259549253632.00000.hdf5\n",
      "Epoch 219/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 923058063802368.0000 - val_loss: 1049155552149504.0000\n",
      "\n",
      "Epoch 00219: val_loss improved from 1051259549253632.00000 to 1049155552149504.00000, saving model to Weights\\Weights-219--1049155552149504.00000.hdf5\n",
      "Epoch 220/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 920864342147072.0000 - val_loss: 1047987052609536.0000\n",
      "\n",
      "Epoch 00220: val_loss improved from 1049155552149504.00000 to 1047987052609536.00000, saving model to Weights\\Weights-220--1047987052609536.00000.hdf5\n",
      "Epoch 221/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 919395329114112.0000 - val_loss: 1044953463521280.0000\n",
      "\n",
      "Epoch 00221: val_loss improved from 1047987052609536.00000 to 1044953463521280.00000, saving model to Weights\\Weights-221--1044953463521280.00000.hdf5\n",
      "Epoch 222/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 918030334820352.0000 - val_loss: 1043046229606400.0000\n",
      "\n",
      "Epoch 00222: val_loss improved from 1044953463521280.00000 to 1043046229606400.00000, saving model to Weights\\Weights-222--1043046229606400.00000.hdf5\n",
      "Epoch 223/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 915609986531328.0000 - val_loss: 1041748612612096.0000\n",
      "\n",
      "Epoch 00223: val_loss improved from 1043046229606400.00000 to 1041748612612096.00000, saving model to Weights\\Weights-223--1041748612612096.00000.hdf5\n",
      "Epoch 224/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 914180836163584.0000 - val_loss: 1039661392723968.0000\n",
      "\n",
      "Epoch 00224: val_loss improved from 1041748612612096.00000 to 1039661392723968.00000, saving model to Weights\\Weights-224--1039661392723968.00000.hdf5\n",
      "Epoch 225/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 913145077956608.0000 - val_loss: 1036908721340416.0000\n",
      "\n",
      "Epoch 00225: val_loss improved from 1039661392723968.00000 to 1036908721340416.00000, saving model to Weights\\Weights-225--1036908721340416.00000.hdf5\n",
      "Epoch 226/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 911192881102848.0000 - val_loss: 1035047456997376.0000\n",
      "\n",
      "Epoch 00226: val_loss improved from 1036908721340416.00000 to 1035047456997376.00000, saving model to Weights\\Weights-226--1035047456997376.00000.hdf5\n",
      "Epoch 227/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 910624536133632.0000 - val_loss: 1033052176252928.0000\n",
      "\n",
      "Epoch 00227: val_loss improved from 1035047456997376.00000 to 1033052176252928.00000, saving model to Weights\\Weights-227--1033052176252928.00000.hdf5\n",
      "Epoch 228/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 908228816797696.0000 - val_loss: 1031558064504832.0000\n",
      "\n",
      "Epoch 00228: val_loss improved from 1033052176252928.00000 to 1031558064504832.00000, saving model to Weights\\Weights-228--1031558064504832.00000.hdf5\n",
      "Epoch 229/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 907261643849728.0000 - val_loss: 1029931815403520.0000\n",
      "\n",
      "Epoch 00229: val_loss improved from 1031558064504832.00000 to 1029931815403520.00000, saving model to Weights\\Weights-229--1029931815403520.00000.hdf5\n",
      "Epoch 230/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 905226970202112.0000 - val_loss: 1027075527933952.0000\n",
      "\n",
      "Epoch 00230: val_loss improved from 1029931815403520.00000 to 1027075527933952.00000, saving model to Weights\\Weights-230--1027075527933952.00000.hdf5\n",
      "Epoch 231/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 903867613052928.0000 - val_loss: 1025156617076736.0000\n",
      "\n",
      "Epoch 00231: val_loss improved from 1027075527933952.00000 to 1025156617076736.00000, saving model to Weights\\Weights-231--1025156617076736.00000.hdf5\n",
      "Epoch 232/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 902396452536320.0000 - val_loss: 1023091207569408.0000\n",
      "\n",
      "Epoch 00232: val_loss improved from 1025156617076736.00000 to 1023091207569408.00000, saving model to Weights\\Weights-232--1023091207569408.00000.hdf5\n",
      "Epoch 233/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 900975086796800.0000 - val_loss: 1022127591391232.0000\n",
      "\n",
      "Epoch 00233: val_loss improved from 1023091207569408.00000 to 1022127591391232.00000, saving model to Weights\\Weights-233--1022127591391232.00000.hdf5\n",
      "Epoch 234/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 899445273133056.0000 - val_loss: 1020281627869184.0000\n",
      "\n",
      "Epoch 00234: val_loss improved from 1022127591391232.00000 to 1020281627869184.00000, saving model to Weights\\Weights-234--1020281627869184.00000.hdf5\n",
      "Epoch 235/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 898257580457984.0000 - val_loss: 1018489015894016.0000\n",
      "\n",
      "Epoch 00235: val_loss improved from 1020281627869184.00000 to 1018489015894016.00000, saving model to Weights\\Weights-235--1018489015894016.00000.hdf5\n",
      "Epoch 236/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 896772528406528.0000 - val_loss: 1016310259515392.0000\n",
      "\n",
      "Epoch 00236: val_loss improved from 1018489015894016.00000 to 1016310259515392.00000, saving model to Weights\\Weights-236--1016310259515392.00000.hdf5\n",
      "Epoch 237/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 895325191536640.0000 - val_loss: 1014441076326400.0000\n",
      "\n",
      "Epoch 00237: val_loss improved from 1016310259515392.00000 to 1014441076326400.00000, saving model to Weights\\Weights-237--1014441076326400.00000.hdf5\n",
      "Epoch 238/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 894596456382464.0000 - val_loss: 1015438246936576.0000\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1014441076326400.00000\n",
      "Epoch 239/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 892197046059008.0000 - val_loss: 1010874038878208.0000\n",
      "\n",
      "Epoch 00239: val_loss improved from 1014441076326400.00000 to 1010874038878208.00000, saving model to Weights\\Weights-239--1010874038878208.00000.hdf5\n",
      "Epoch 240/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 891682455289856.0000 - val_loss: 1008214783033344.0000\n",
      "\n",
      "Epoch 00240: val_loss improved from 1010874038878208.00000 to 1008214783033344.00000, saving model to Weights\\Weights-240--1008214783033344.00000.hdf5\n",
      "Epoch 241/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 889748243611648.0000 - val_loss: 1006559878447104.0000\n",
      "\n",
      "Epoch 00241: val_loss improved from 1008214783033344.00000 to 1006559878447104.00000, saving model to Weights\\Weights-241--1006559878447104.00000.hdf5\n",
      "Epoch 242/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 888554980900864.0000 - val_loss: 1005228774129664.0000\n",
      "\n",
      "Epoch 00242: val_loss improved from 1006559878447104.00000 to 1005228774129664.00000, saving model to Weights\\Weights-242--1005228774129664.00000.hdf5\n",
      "Epoch 243/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 887535127494656.0000 - val_loss: 1004181406089216.0000\n",
      "\n",
      "Epoch 00243: val_loss improved from 1005228774129664.00000 to 1004181406089216.00000, saving model to Weights\\Weights-243--1004181406089216.00000.hdf5\n",
      "Epoch 244/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 885693056286720.0000 - val_loss: 1001678513897472.0000\n",
      "\n",
      "Epoch 00244: val_loss improved from 1004181406089216.00000 to 1001678513897472.00000, saving model to Weights\\Weights-244--1001678513897472.00000.hdf5\n",
      "Epoch 245/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 884861510352896.0000 - val_loss: 999714371665920.0000\n",
      "\n",
      "Epoch 00245: val_loss improved from 1001678513897472.00000 to 999714371665920.00000, saving model to Weights\\Weights-245--999714371665920.00000.hdf5\n",
      "Epoch 246/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 883724082216960.0000 - val_loss: 999224678285312.0000\n",
      "\n",
      "Epoch 00246: val_loss improved from 999714371665920.00000 to 999224678285312.00000, saving model to Weights\\Weights-246--999224678285312.00000.hdf5\n",
      "Epoch 247/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 883042054832128.0000 - val_loss: 996453820399616.0000\n",
      "\n",
      "Epoch 00247: val_loss improved from 999224678285312.00000 to 996453820399616.00000, saving model to Weights\\Weights-247--996453820399616.00000.hdf5\n",
      "Epoch 248/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 880706196602880.0000 - val_loss: 995176403173376.0000\n",
      "\n",
      "Epoch 00248: val_loss improved from 996453820399616.00000 to 995176403173376.00000, saving model to Weights\\Weights-248--995176403173376.00000.hdf5\n",
      "Epoch 249/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 881050733510656.0000 - val_loss: 996615284326400.0000\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 995176403173376.00000\n",
      "Epoch 250/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 878501804638208.0000 - val_loss: 991362203779072.0000\n",
      "\n",
      "Epoch 00250: val_loss improved from 995176403173376.00000 to 991362203779072.00000, saving model to Weights\\Weights-250--991362203779072.00000.hdf5\n",
      "Epoch 251/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 878073046106112.0000 - val_loss: 991437768359936.0000\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 991362203779072.00000\n",
      "Epoch 252/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 877329547001856.0000 - val_loss: 988762070843392.0000\n",
      "\n",
      "Epoch 00252: val_loss improved from 991362203779072.00000 to 988762070843392.00000, saving model to Weights\\Weights-252--988762070843392.00000.hdf5\n",
      "Epoch 253/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 875235347791872.0000 - val_loss: 987496934539264.0000\n",
      "\n",
      "Epoch 00253: val_loss improved from 988762070843392.00000 to 987496934539264.00000, saving model to Weights\\Weights-253--987496934539264.00000.hdf5\n",
      "Epoch 254/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 873792775651328.0000 - val_loss: 986206095540224.0000\n",
      "\n",
      "Epoch 00254: val_loss improved from 987496934539264.00000 to 986206095540224.00000, saving model to Weights\\Weights-254--986206095540224.00000.hdf5\n",
      "Epoch 255/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 872361008037888.0000 - val_loss: 984725070020608.0000\n",
      "\n",
      "Epoch 00255: val_loss improved from 986206095540224.00000 to 984725070020608.00000, saving model to Weights\\Weights-255--984725070020608.00000.hdf5\n",
      "Epoch 256/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 871360280657920.0000 - val_loss: 982989366362112.0000\n",
      "\n",
      "Epoch 00256: val_loss improved from 984725070020608.00000 to 982989366362112.00000, saving model to Weights\\Weights-256--982989366362112.00000.hdf5\n",
      "Epoch 257/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 871272099610624.0000 - val_loss: 982109904699392.0000\n",
      "\n",
      "Epoch 00257: val_loss improved from 982989366362112.00000 to 982109904699392.00000, saving model to Weights\\Weights-257--982109904699392.00000.hdf5\n",
      "Epoch 258/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 869382179782656.0000 - val_loss: 980960866729984.0000\n",
      "\n",
      "Epoch 00258: val_loss improved from 982109904699392.00000 to 980960866729984.00000, saving model to Weights\\Weights-258--980960866729984.00000.hdf5\n",
      "Epoch 259/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 868629218328576.0000 - val_loss: 978833113088000.0000\n",
      "\n",
      "Epoch 00259: val_loss improved from 980960866729984.00000 to 978833113088000.00000, saving model to Weights\\Weights-259--978833113088000.00000.hdf5\n",
      "Epoch 260/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 866704737435648.0000 - val_loss: 977045265842176.0000\n",
      "\n",
      "Epoch 00260: val_loss improved from 978833113088000.00000 to 977045265842176.00000, saving model to Weights\\Weights-260--977045265842176.00000.hdf5\n",
      "Epoch 261/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 866435765108736.0000 - val_loss: 976001655898112.0000\n",
      "\n",
      "Epoch 00261: val_loss improved from 977045265842176.00000 to 976001655898112.00000, saving model to Weights\\Weights-261--976001655898112.00000.hdf5\n",
      "Epoch 262/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 864734488297472.0000 - val_loss: 974661693210624.0000\n",
      "\n",
      "Epoch 00262: val_loss improved from 976001655898112.00000 to 974661693210624.00000, saving model to Weights\\Weights-262--974661693210624.00000.hdf5\n",
      "Epoch 263/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 864242244780032.0000 - val_loss: 972879885762560.0000\n",
      "\n",
      "Epoch 00263: val_loss improved from 974661693210624.00000 to 972879885762560.00000, saving model to Weights\\Weights-263--972879885762560.00000.hdf5\n",
      "Epoch 264/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 862722296119296.0000 - val_loss: 971774401445888.0000\n",
      "\n",
      "Epoch 00264: val_loss improved from 972879885762560.00000 to 971774401445888.00000, saving model to Weights\\Weights-264--971774401445888.00000.hdf5\n",
      "Epoch 265/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 861674324099072.0000 - val_loss: 970125201113088.0000\n",
      "\n",
      "Epoch 00265: val_loss improved from 971774401445888.00000 to 970125201113088.00000, saving model to Weights\\Weights-265--970125201113088.00000.hdf5\n",
      "Epoch 266/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 860905793388544.0000 - val_loss: 968455532576768.0000\n",
      "\n",
      "Epoch 00266: val_loss improved from 970125201113088.00000 to 968455532576768.00000, saving model to Weights\\Weights-266--968455532576768.00000.hdf5\n",
      "Epoch 267/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 859449732366336.0000 - val_loss: 966694998638592.0000\n",
      "\n",
      "Epoch 00267: val_loss improved from 968455532576768.00000 to 966694998638592.00000, saving model to Weights\\Weights-267--966694998638592.00000.hdf5\n",
      "Epoch 268/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 858156477448192.0000 - val_loss: 965596359426048.0000\n",
      "\n",
      "Epoch 00268: val_loss improved from 966694998638592.00000 to 965596359426048.00000, saving model to Weights\\Weights-268--965596359426048.00000.hdf5\n",
      "Epoch 269/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 857153199931392.0000 - val_loss: 965039691399168.0000\n",
      "\n",
      "Epoch 00269: val_loss improved from 965596359426048.00000 to 965039691399168.00000, saving model to Weights\\Weights-269--965039691399168.00000.hdf5\n",
      "Epoch 270/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 856255753093120.0000 - val_loss: 962755305668608.0000\n",
      "\n",
      "Epoch 00270: val_loss improved from 965039691399168.00000 to 962755305668608.00000, saving model to Weights\\Weights-270--962755305668608.00000.hdf5\n",
      "Epoch 271/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 855670429581312.0000 - val_loss: 961132210683904.0000\n",
      "\n",
      "Epoch 00271: val_loss improved from 962755305668608.00000 to 961132210683904.00000, saving model to Weights\\Weights-271--961132210683904.00000.hdf5\n",
      "Epoch 272/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 854112363085824.0000 - val_loss: 960749086179328.0000\n",
      "\n",
      "Epoch 00272: val_loss improved from 961132210683904.00000 to 960749086179328.00000, saving model to Weights\\Weights-272--960749086179328.00000.hdf5\n",
      "Epoch 273/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 852763273592832.0000 - val_loss: 958600126136320.0000\n",
      "\n",
      "Epoch 00273: val_loss improved from 960749086179328.00000 to 958600126136320.00000, saving model to Weights\\Weights-273--958600126136320.00000.hdf5\n",
      "Epoch 274/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 851676244213760.0000 - val_loss: 956824291377152.0000\n",
      "\n",
      "Epoch 00274: val_loss improved from 958600126136320.00000 to 956824291377152.00000, saving model to Weights\\Weights-274--956824291377152.00000.hdf5\n",
      "Epoch 275/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 850707460653056.0000 - val_loss: 955199183126528.0000\n",
      "\n",
      "Epoch 00275: val_loss improved from 956824291377152.00000 to 955199183126528.00000, saving model to Weights\\Weights-275--955199183126528.00000.hdf5\n",
      "Epoch 276/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 849624457805824.0000 - val_loss: 954302340268032.0000\n",
      "\n",
      "Epoch 00276: val_loss improved from 955199183126528.00000 to 954302340268032.00000, saving model to Weights\\Weights-276--954302340268032.00000.hdf5\n",
      "Epoch 277/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 848657150640128.0000 - val_loss: 952394032611328.0000\n",
      "\n",
      "Epoch 00277: val_loss improved from 954302340268032.00000 to 952394032611328.00000, saving model to Weights\\Weights-277--952394032611328.00000.hdf5\n",
      "Epoch 278/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 848619301240832.0000 - val_loss: 953985183776768.0000\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 952394032611328.00000\n",
      "Epoch 279/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 846213515575296.0000 - val_loss: 949179048263680.0000\n",
      "\n",
      "Epoch 00279: val_loss improved from 952394032611328.00000 to 949179048263680.00000, saving model to Weights\\Weights-279--949179048263680.00000.hdf5\n",
      "Epoch 280/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 845818848346112.0000 - val_loss: 949444732256256.0000\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 949179048263680.00000\n",
      "Epoch 281/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 844305006592000.0000 - val_loss: 946232230936576.0000\n",
      "\n",
      "Epoch 00281: val_loss improved from 949179048263680.00000 to 946232230936576.00000, saving model to Weights\\Weights-281--946232230936576.00000.hdf5\n",
      "Epoch 282/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 844048516513792.0000 - val_loss: 944764492972032.0000\n",
      "\n",
      "Epoch 00282: val_loss improved from 946232230936576.00000 to 944764492972032.00000, saving model to Weights\\Weights-282--944764492972032.00000.hdf5\n",
      "Epoch 283/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 843002490650624.0000 - val_loss: 943643439398912.0000\n",
      "\n",
      "Epoch 00283: val_loss improved from 944764492972032.00000 to 943643439398912.00000, saving model to Weights\\Weights-283--943643439398912.00000.hdf5\n",
      "Epoch 284/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 841902844805120.0000 - val_loss: 942388906295296.0000\n",
      "\n",
      "Epoch 00284: val_loss improved from 943643439398912.00000 to 942388906295296.00000, saving model to Weights\\Weights-284--942388906295296.00000.hdf5\n",
      "Epoch 285/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 841106598133760.0000 - val_loss: 943206694912000.0000\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 942388906295296.00000\n",
      "Epoch 286/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 840247000694784.0000 - val_loss: 941289394667520.0000\n",
      "\n",
      "Epoch 00286: val_loss improved from 942388906295296.00000 to 941289394667520.00000, saving model to Weights\\Weights-286--941289394667520.00000.hdf5\n",
      "Epoch 287/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 838880395788288.0000 - val_loss: 937168172220416.0000\n",
      "\n",
      "Epoch 00287: val_loss improved from 941289394667520.00000 to 937168172220416.00000, saving model to Weights\\Weights-287--937168172220416.00000.hdf5\n",
      "Epoch 288/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 837928724987904.0000 - val_loss: 936213213085696.0000\n",
      "\n",
      "Epoch 00288: val_loss improved from 937168172220416.00000 to 936213213085696.00000, saving model to Weights\\Weights-288--936213213085696.00000.hdf5\n",
      "Epoch 289/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 836397233602560.0000 - val_loss: 935398981238784.0000\n",
      "\n",
      "Epoch 00289: val_loss improved from 936213213085696.00000 to 935398981238784.00000, saving model to Weights\\Weights-289--935398981238784.00000.hdf5\n",
      "Epoch 290/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 836719020605440.0000 - val_loss: 933604624433152.0000\n",
      "\n",
      "Epoch 00290: val_loss improved from 935398981238784.00000 to 933604624433152.00000, saving model to Weights\\Weights-290--933604624433152.00000.hdf5\n",
      "Epoch 291/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 835031232675840.0000 - val_loss: 931582433034240.0000\n",
      "\n",
      "Epoch 00291: val_loss improved from 933604624433152.00000 to 931582433034240.00000, saving model to Weights\\Weights-291--931582433034240.00000.hdf5\n",
      "Epoch 292/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 834172440543232.0000 - val_loss: 930651096219648.0000\n",
      "\n",
      "Epoch 00292: val_loss improved from 931582433034240.00000 to 930651096219648.00000, saving model to Weights\\Weights-292--930651096219648.00000.hdf5\n",
      "Epoch 293/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 833084740075520.0000 - val_loss: 929620706721792.0000\n",
      "\n",
      "Epoch 00293: val_loss improved from 930651096219648.00000 to 929620706721792.00000, saving model to Weights\\Weights-293--929620706721792.00000.hdf5\n",
      "Epoch 294/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 832601891799040.0000 - val_loss: 928478446747648.0000\n",
      "\n",
      "Epoch 00294: val_loss improved from 929620706721792.00000 to 928478446747648.00000, saving model to Weights\\Weights-294--928478446747648.00000.hdf5\n",
      "Epoch 295/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 831030269313024.0000 - val_loss: 927769441599488.0000\n",
      "\n",
      "Epoch 00295: val_loss improved from 928478446747648.00000 to 927769441599488.00000, saving model to Weights\\Weights-295--927769441599488.00000.hdf5\n",
      "Epoch 296/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 830196173242368.0000 - val_loss: 926862263975936.0000\n",
      "\n",
      "Epoch 00296: val_loss improved from 927769441599488.00000 to 926862263975936.00000, saving model to Weights\\Weights-296--926862263975936.00000.hdf5\n",
      "Epoch 297/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 829369324929024.0000 - val_loss: 924687265693696.0000\n",
      "\n",
      "Epoch 00297: val_loss improved from 926862263975936.00000 to 924687265693696.00000, saving model to Weights\\Weights-297--924687265693696.00000.hdf5\n",
      "Epoch 298/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 828722798133248.0000 - val_loss: 923283885129728.0000\n",
      "\n",
      "Epoch 00298: val_loss improved from 924687265693696.00000 to 923283885129728.00000, saving model to Weights\\Weights-298--923283885129728.00000.hdf5\n",
      "Epoch 299/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 827222243934208.0000 - val_loss: 922930892505088.0000\n",
      "\n",
      "Epoch 00299: val_loss improved from 923283885129728.00000 to 922930892505088.00000, saving model to Weights\\Weights-299--922930892505088.00000.hdf5\n",
      "Epoch 300/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 826478811938816.0000 - val_loss: 920333511032832.0000\n",
      "\n",
      "Epoch 00300: val_loss improved from 922930892505088.00000 to 920333511032832.00000, saving model to Weights\\Weights-300--920333511032832.00000.hdf5\n",
      "Epoch 301/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 825204750155776.0000 - val_loss: 919006366138368.0000\n",
      "\n",
      "Epoch 00301: val_loss improved from 920333511032832.00000 to 919006366138368.00000, saving model to Weights\\Weights-301--919006366138368.00000.hdf5\n",
      "Epoch 302/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 824724452016128.0000 - val_loss: 920742673776640.0000\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 919006366138368.00000\n",
      "Epoch 303/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 824933227692032.0000 - val_loss: 916624068575232.0000\n",
      "\n",
      "Epoch 00303: val_loss improved from 919006366138368.00000 to 916624068575232.00000, saving model to Weights\\Weights-303--916624068575232.00000.hdf5\n",
      "Epoch 304/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 822785609826304.0000 - val_loss: 915520329089024.0000\n",
      "\n",
      "Epoch 00304: val_loss improved from 916624068575232.00000 to 915520329089024.00000, saving model to Weights\\Weights-304--915520329089024.00000.hdf5\n",
      "Epoch 305/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 822277729943552.0000 - val_loss: 913913004687360.0000\n",
      "\n",
      "Epoch 00305: val_loss improved from 915520329089024.00000 to 913913004687360.00000, saving model to Weights\\Weights-305--913913004687360.00000.hdf5\n",
      "Epoch 306/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 821079434067968.0000 - val_loss: 912984687771648.0000\n",
      "\n",
      "Epoch 00306: val_loss improved from 913913004687360.00000 to 912984687771648.00000, saving model to Weights\\Weights-306--912984687771648.00000.hdf5\n",
      "Epoch 307/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 821978625736704.0000 - val_loss: 911322870972416.0000\n",
      "\n",
      "Epoch 00307: val_loss improved from 912984687771648.00000 to 911322870972416.00000, saving model to Weights\\Weights-307--911322870972416.00000.hdf5\n",
      "Epoch 308/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 819326953193472.0000 - val_loss: 910523268857856.0000\n",
      "\n",
      "Epoch 00308: val_loss improved from 911322870972416.00000 to 910523268857856.00000, saving model to Weights\\Weights-308--910523268857856.00000.hdf5\n",
      "Epoch 309/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 818995234078720.0000 - val_loss: 908993186758656.0000\n",
      "\n",
      "Epoch 00309: val_loss improved from 910523268857856.00000 to 908993186758656.00000, saving model to Weights\\Weights-309--908993186758656.00000.hdf5\n",
      "Epoch 310/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 818092888293376.0000 - val_loss: 907603496402944.0000\n",
      "\n",
      "Epoch 00310: val_loss improved from 908993186758656.00000 to 907603496402944.00000, saving model to Weights\\Weights-310--907603496402944.00000.hdf5\n",
      "Epoch 311/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 816914792185856.0000 - val_loss: 906647262199808.0000\n",
      "\n",
      "Epoch 00311: val_loss improved from 907603496402944.00000 to 906647262199808.00000, saving model to Weights\\Weights-311--906647262199808.00000.hdf5\n",
      "Epoch 312/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 815727703490560.0000 - val_loss: 905371455586304.0000\n",
      "\n",
      "Epoch 00312: val_loss improved from 906647262199808.00000 to 905371455586304.00000, saving model to Weights\\Weights-312--905371455586304.00000.hdf5\n",
      "Epoch 313/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 815358336303104.0000 - val_loss: 903997468704768.0000\n",
      "\n",
      "Epoch 00313: val_loss improved from 905371455586304.00000 to 903997468704768.00000, saving model to Weights\\Weights-313--903997468704768.00000.hdf5\n",
      "Epoch 314/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 814916357324800.0000 - val_loss: 902440945713152.0000\n",
      "\n",
      "Epoch 00314: val_loss improved from 903997468704768.00000 to 902440945713152.00000, saving model to Weights\\Weights-314--902440945713152.00000.hdf5\n",
      "Epoch 315/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 814193460641792.0000 - val_loss: 900641757069312.0000\n",
      "\n",
      "Epoch 00315: val_loss improved from 902440945713152.00000 to 900641757069312.00000, saving model to Weights\\Weights-315--900641757069312.00000.hdf5\n",
      "Epoch 316/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 812526543568896.0000 - val_loss: 900788389937152.0000\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 900641757069312.00000\n",
      "Epoch 317/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 812333135822848.0000 - val_loss: 899004367896576.0000\n",
      "\n",
      "Epoch 00317: val_loss improved from 900641757069312.00000 to 899004367896576.00000, saving model to Weights\\Weights-317--899004367896576.00000.hdf5\n",
      "Epoch 318/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 811724189990912.0000 - val_loss: 898731436146688.0000\n",
      "\n",
      "Epoch 00318: val_loss improved from 899004367896576.00000 to 898731436146688.00000, saving model to Weights\\Weights-318--898731436146688.00000.hdf5\n",
      "Epoch 319/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 810414560509952.0000 - val_loss: 897405230776320.0000\n",
      "\n",
      "Epoch 00319: val_loss improved from 898731436146688.00000 to 897405230776320.00000, saving model to Weights\\Weights-319--897405230776320.00000.hdf5\n",
      "Epoch 320/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 808918838149120.0000 - val_loss: 895694625832960.0000\n",
      "\n",
      "Epoch 00320: val_loss improved from 897405230776320.00000 to 895694625832960.00000, saving model to Weights\\Weights-320--895694625832960.00000.hdf5\n",
      "Epoch 321/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 808498065571840.0000 - val_loss: 894215814905856.0000\n",
      "\n",
      "Epoch 00321: val_loss improved from 895694625832960.00000 to 894215814905856.00000, saving model to Weights\\Weights-321--894215814905856.00000.hdf5\n",
      "Epoch 322/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 808417065172992.0000 - val_loss: 893272532713472.0000\n",
      "\n",
      "Epoch 00322: val_loss improved from 894215814905856.00000 to 893272532713472.00000, saving model to Weights\\Weights-322--893272532713472.00000.hdf5\n",
      "Epoch 323/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 807445597257728.0000 - val_loss: 891616151732224.0000\n",
      "\n",
      "Epoch 00323: val_loss improved from 893272532713472.00000 to 891616151732224.00000, saving model to Weights\\Weights-323--891616151732224.00000.hdf5\n",
      "Epoch 324/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 806324879228928.0000 - val_loss: 890504157855744.0000\n",
      "\n",
      "Epoch 00324: val_loss improved from 891616151732224.00000 to 890504157855744.00000, saving model to Weights\\Weights-324--890504157855744.00000.hdf5\n",
      "Epoch 325/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 805269994995712.0000 - val_loss: 888603500609536.0000\n",
      "\n",
      "Epoch 00325: val_loss improved from 890504157855744.00000 to 888603500609536.00000, saving model to Weights\\Weights-325--888603500609536.00000.hdf5\n",
      "Epoch 326/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 804554413178880.0000 - val_loss: 887576063901696.0000\n",
      "\n",
      "Epoch 00326: val_loss improved from 888603500609536.00000 to 887576063901696.00000, saving model to Weights\\Weights-326--887576063901696.00000.hdf5\n",
      "Epoch 327/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 804328256307200.0000 - val_loss: 886530172256256.0000\n",
      "\n",
      "Epoch 00327: val_loss improved from 887576063901696.00000 to 886530172256256.00000, saving model to Weights\\Weights-327--886530172256256.00000.hdf5\n",
      "Epoch 328/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 802967489871872.0000 - val_loss: 885912368054272.0000\n",
      "\n",
      "Epoch 00328: val_loss improved from 886530172256256.00000 to 885912368054272.00000, saving model to Weights\\Weights-328--885912368054272.00000.hdf5\n",
      "Epoch 329/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 802449543659520.0000 - val_loss: 884090362396672.0000\n",
      "\n",
      "Epoch 00329: val_loss improved from 885912368054272.00000 to 884090362396672.00000, saving model to Weights\\Weights-329--884090362396672.00000.hdf5\n",
      "Epoch 330/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 801512032829440.0000 - val_loss: 884221157572608.0000\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 884090362396672.00000\n",
      "Epoch 331/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 800136770879488.0000 - val_loss: 881004428394496.0000\n",
      "\n",
      "Epoch 00331: val_loss improved from 884090362396672.00000 to 881004428394496.00000, saving model to Weights\\Weights-331--881004428394496.00000.hdf5\n",
      "Epoch 332/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 799856658481152.0000 - val_loss: 879542193356800.0000\n",
      "\n",
      "Epoch 00332: val_loss improved from 881004428394496.00000 to 879542193356800.00000, saving model to Weights\\Weights-332--879542193356800.00000.hdf5\n",
      "Epoch 333/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 798816538198016.0000 - val_loss: 879238257311744.0000\n",
      "\n",
      "Epoch 00333: val_loss improved from 879542193356800.00000 to 879238257311744.00000, saving model to Weights\\Weights-333--879238257311744.00000.hdf5\n",
      "Epoch 334/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 797991703150592.0000 - val_loss: 877039032729600.0000\n",
      "\n",
      "Epoch 00334: val_loss improved from 879238257311744.00000 to 877039032729600.00000, saving model to Weights\\Weights-334--877039032729600.00000.hdf5\n",
      "Epoch 335/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 796917088911360.0000 - val_loss: 878767891283968.0000\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 877039032729600.00000\n",
      "Epoch 336/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 795928709562368.0000 - val_loss: 875031940825088.0000\n",
      "\n",
      "Epoch 00336: val_loss improved from 877039032729600.00000 to 875031940825088.00000, saving model to Weights\\Weights-336--875031940825088.00000.hdf5\n",
      "Epoch 337/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 795249836294144.0000 - val_loss: 873721036275712.0000\n",
      "\n",
      "Epoch 00337: val_loss improved from 875031940825088.00000 to 873721036275712.00000, saving model to Weights\\Weights-337--873721036275712.00000.hdf5\n",
      "Epoch 338/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 794330444857344.0000 - val_loss: 873109741633536.0000\n",
      "\n",
      "Epoch 00338: val_loss improved from 873721036275712.00000 to 873109741633536.00000, saving model to Weights\\Weights-338--873109741633536.00000.hdf5\n",
      "Epoch 339/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 793650497847296.0000 - val_loss: 871509933424640.0000\n",
      "\n",
      "Epoch 00339: val_loss improved from 873109741633536.00000 to 871509933424640.00000, saving model to Weights\\Weights-339--871509933424640.00000.hdf5\n",
      "Epoch 340/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 793270326132736.0000 - val_loss: 871056881483776.0000\n",
      "\n",
      "Epoch 00340: val_loss improved from 871509933424640.00000 to 871056881483776.00000, saving model to Weights\\Weights-340--871056881483776.00000.hdf5\n",
      "Epoch 341/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 792575212519424.0000 - val_loss: 870455183409152.0000\n",
      "\n",
      "Epoch 00341: val_loss improved from 871056881483776.00000 to 870455183409152.00000, saving model to Weights\\Weights-341--870455183409152.00000.hdf5\n",
      "Epoch 342/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 791468990005248.0000 - val_loss: 868873830137856.0000\n",
      "\n",
      "Epoch 00342: val_loss improved from 870455183409152.00000 to 868873830137856.00000, saving model to Weights\\Weights-342--868873830137856.00000.hdf5\n",
      "Epoch 343/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 790869305196544.0000 - val_loss: 867329990721536.0000\n",
      "\n",
      "Epoch 00343: val_loss improved from 868873830137856.00000 to 867329990721536.00000, saving model to Weights\\Weights-343--867329990721536.00000.hdf5\n",
      "Epoch 344/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 789695369838592.0000 - val_loss: 866388788903936.0000\n",
      "\n",
      "Epoch 00344: val_loss improved from 867329990721536.00000 to 866388788903936.00000, saving model to Weights\\Weights-344--866388788903936.00000.hdf5\n",
      "Epoch 345/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 790323307479040.0000 - val_loss: 867630906867712.0000\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 866388788903936.00000\n",
      "Epoch 346/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 788519085670400.0000 - val_loss: 865702063898624.0000\n",
      "\n",
      "Epoch 00346: val_loss improved from 866388788903936.00000 to 865702063898624.00000, saving model to Weights\\Weights-346--865702063898624.00000.hdf5\n",
      "Epoch 347/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 787258110115840.0000 - val_loss: 862379906695168.0000\n",
      "\n",
      "Epoch 00347: val_loss improved from 865702063898624.00000 to 862379906695168.00000, saving model to Weights\\Weights-347--862379906695168.00000.hdf5\n",
      "Epoch 348/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 786685604397056.0000 - val_loss: 861219661545472.0000\n",
      "\n",
      "Epoch 00348: val_loss improved from 862379906695168.00000 to 861219661545472.00000, saving model to Weights\\Weights-348--861219661545472.00000.hdf5\n",
      "Epoch 349/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 785756951937024.0000 - val_loss: 860461666926592.0000\n",
      "\n",
      "Epoch 00349: val_loss improved from 861219661545472.00000 to 860461666926592.00000, saving model to Weights\\Weights-349--860461666926592.00000.hdf5\n",
      "Epoch 350/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 785212027961344.0000 - val_loss: 861377501593600.0000\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 860461666926592.00000\n",
      "Epoch 351/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 784018765250560.0000 - val_loss: 859312628957184.0000\n",
      "\n",
      "Epoch 00351: val_loss improved from 860461666926592.00000 to 859312628957184.00000, saving model to Weights\\Weights-351--859312628957184.00000.hdf5\n",
      "Epoch 352/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 783352709775360.0000 - val_loss: 856103013318656.0000\n",
      "\n",
      "Epoch 00352: val_loss improved from 859312628957184.00000 to 856103013318656.00000, saving model to Weights\\Weights-352--856103013318656.00000.hdf5\n",
      "Epoch 353/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 783066557579264.0000 - val_loss: 855580302376960.0000\n",
      "\n",
      "Epoch 00353: val_loss improved from 856103013318656.00000 to 855580302376960.00000, saving model to Weights\\Weights-353--855580302376960.00000.hdf5\n",
      "Epoch 354/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 781579492261888.0000 - val_loss: 853461809758208.0000\n",
      "\n",
      "Epoch 00354: val_loss improved from 855580302376960.00000 to 853461809758208.00000, saving model to Weights\\Weights-354--853461809758208.00000.hdf5\n",
      "Epoch 355/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 781659553136640.0000 - val_loss: 852956949774336.0000\n",
      "\n",
      "Epoch 00355: val_loss improved from 853461809758208.00000 to 852956949774336.00000, saving model to Weights\\Weights-355--852956949774336.00000.hdf5\n",
      "Epoch 356/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 779771176812544.0000 - val_loss: 851998568087552.0000\n",
      "\n",
      "Epoch 00356: val_loss improved from 852956949774336.00000 to 851998568087552.00000, saving model to Weights\\Weights-356--851998568087552.00000.hdf5\n",
      "Epoch 357/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 779487037882368.0000 - val_loss: 850129653334016.0000\n",
      "\n",
      "Epoch 00357: val_loss improved from 851998568087552.00000 to 850129653334016.00000, saving model to Weights\\Weights-357--850129653334016.00000.hdf5\n",
      "Epoch 358/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 778778636713984.0000 - val_loss: 852870580666368.0000\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 850129653334016.00000\n",
      "Epoch 359/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 776934485131264.0000 - val_loss: 844417816592384.0000\n",
      "\n",
      "Epoch 00359: val_loss improved from 850129653334016.00000 to 844417816592384.00000, saving model to Weights\\Weights-359--844417816592384.00000.hdf5\n",
      "Epoch 360/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 776298494427136.0000 - val_loss: 843295219515392.0000\n",
      "\n",
      "Epoch 00360: val_loss improved from 844417816592384.00000 to 843295219515392.00000, saving model to Weights\\Weights-360--843295219515392.00000.hdf5\n",
      "Epoch 361/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 775946172891136.0000 - val_loss: 844156763111424.0000\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 843295219515392.00000\n",
      "Epoch 362/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 774908401418240.0000 - val_loss: 842767139864576.0000\n",
      "\n",
      "Epoch 00362: val_loss improved from 843295219515392.00000 to 842767139864576.00000, saving model to Weights\\Weights-362--842767139864576.00000.hdf5\n",
      "Epoch 363/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 773737016197120.0000 - val_loss: 842413006389248.0000\n",
      "\n",
      "Epoch 00363: val_loss improved from 842767139864576.00000 to 842413006389248.00000, saving model to Weights\\Weights-363--842413006389248.00000.hdf5\n",
      "Epoch 364/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 772912181149696.0000 - val_loss: 839119169126400.0000\n",
      "\n",
      "Epoch 00364: val_loss improved from 842413006389248.00000 to 839119169126400.00000, saving model to Weights\\Weights-364--839119169126400.00000.hdf5\n",
      "Epoch 365/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 772971639603200.0000 - val_loss: 838346477666304.0000\n",
      "\n",
      "Epoch 00365: val_loss improved from 839119169126400.00000 to 838346477666304.00000, saving model to Weights\\Weights-365--838346477666304.00000.hdf5\n",
      "Epoch 366/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 772139154145280.0000 - val_loss: 837844369145856.0000\n",
      "\n",
      "Epoch 00366: val_loss improved from 838346477666304.00000 to 837844369145856.00000, saving model to Weights\\Weights-366--837844369145856.00000.hdf5\n",
      "Epoch 367/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 770538674847744.0000 - val_loss: 837330113921024.0000\n",
      "\n",
      "Epoch 00367: val_loss improved from 837844369145856.00000 to 837330113921024.00000, saving model to Weights\\Weights-367--837330113921024.00000.hdf5\n",
      "Epoch 368/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 769914025541632.0000 - val_loss: 836901154062336.0000\n",
      "\n",
      "Epoch 00368: val_loss improved from 837330113921024.00000 to 836901154062336.00000, saving model to Weights\\Weights-368--836901154062336.00000.hdf5\n",
      "Epoch 369/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 768912157310976.0000 - val_loss: 835763658817536.0000\n",
      "\n",
      "Epoch 00369: val_loss improved from 836901154062336.00000 to 835763658817536.00000, saving model to Weights\\Weights-369--835763658817536.00000.hdf5\n",
      "Epoch 370/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 768165168545792.0000 - val_loss: 834956808945664.0000\n",
      "\n",
      "Epoch 00370: val_loss improved from 835763658817536.00000 to 834956808945664.00000, saving model to Weights\\Weights-370--834956808945664.00000.hdf5\n",
      "Epoch 371/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 768004778360832.0000 - val_loss: 834351889645568.0000\n",
      "\n",
      "Epoch 00371: val_loss improved from 834956808945664.00000 to 834351889645568.00000, saving model to Weights\\Weights-371--834351889645568.00000.hdf5\n",
      "Epoch 372/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 766857216786432.0000 - val_loss: 832458345938944.0000\n",
      "\n",
      "Epoch 00372: val_loss improved from 834351889645568.00000 to 832458345938944.00000, saving model to Weights\\Weights-372--832458345938944.00000.hdf5\n",
      "Epoch 373/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 766151835516928.0000 - val_loss: 833436726067200.0000\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 832458345938944.00000\n",
      "Epoch 374/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 764949915762688.0000 - val_loss: 830142821695488.0000\n",
      "\n",
      "Epoch 00374: val_loss improved from 832458345938944.00000 to 830142821695488.00000, saving model to Weights\\Weights-374--830142821695488.00000.hdf5\n",
      "Epoch 375/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 764173197770752.0000 - val_loss: 829235509854208.0000\n",
      "\n",
      "Epoch 00375: val_loss improved from 830142821695488.00000 to 829235509854208.00000, saving model to Weights\\Weights-375--829235509854208.00000.hdf5\n",
      "Epoch 376/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 763547004960768.0000 - val_loss: 828421479333888.0000\n",
      "\n",
      "Epoch 00376: val_loss improved from 829235509854208.00000 to 828421479333888.00000, saving model to Weights\\Weights-376--828421479333888.00000.hdf5\n",
      "Epoch 377/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 763533247643648.0000 - val_loss: 827802534281216.0000\n",
      "\n",
      "Epoch 00377: val_loss improved from 828421479333888.00000 to 827802534281216.00000, saving model to Weights\\Weights-377--827802534281216.00000.hdf5\n",
      "Epoch 378/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 762631304511488.0000 - val_loss: 827491216261120.0000\n",
      "\n",
      "Epoch 00378: val_loss improved from 827802534281216.00000 to 827491216261120.00000, saving model to Weights\\Weights-378--827491216261120.00000.hdf5\n",
      "Epoch 379/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 760608911785984.0000 - val_loss: 825355745099776.0000\n",
      "\n",
      "Epoch 00379: val_loss improved from 827491216261120.00000 to 825355745099776.00000, saving model to Weights\\Weights-379--825355745099776.00000.hdf5\n",
      "Epoch 380/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 759633954209792.0000 - val_loss: 824288579944448.0000\n",
      "\n",
      "Epoch 00380: val_loss improved from 825355745099776.00000 to 824288579944448.00000, saving model to Weights\\Weights-380--824288579944448.00000.hdf5\n",
      "Epoch 381/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 759569395482624.0000 - val_loss: 824660765704192.0000\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 824288579944448.00000\n",
      "Epoch 382/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 758336739868672.0000 - val_loss: 821275727495168.0000\n",
      "\n",
      "Epoch 00382: val_loss improved from 824288579944448.00000 to 821275727495168.00000, saving model to Weights\\Weights-382--821275727495168.00000.hdf5\n",
      "Epoch 383/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 757855300878336.0000 - val_loss: 819995021934592.0000\n",
      "\n",
      "Epoch 00383: val_loss improved from 821275727495168.00000 to 819995021934592.00000, saving model to Weights\\Weights-383--819995021934592.00000.hdf5\n",
      "Epoch 384/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 756939600429056.0000 - val_loss: 818909133406208.0000\n",
      "\n",
      "Epoch 00384: val_loss improved from 819995021934592.00000 to 818909133406208.00000, saving model to Weights\\Weights-384--818909133406208.00000.hdf5\n",
      "Epoch 385/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 757059926622208.0000 - val_loss: 820564373536768.0000\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 818909133406208.00000\n",
      "Epoch 386/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 755292614688768.0000 - val_loss: 816082910707712.0000\n",
      "\n",
      "Epoch 00386: val_loss improved from 818909133406208.00000 to 816082910707712.00000, saving model to Weights\\Weights-386--816082910707712.00000.hdf5\n",
      "Epoch 387/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 755015790624768.0000 - val_loss: 814598865289216.0000\n",
      "\n",
      "Epoch 00387: val_loss improved from 816082910707712.00000 to 814598865289216.00000, saving model to Weights\\Weights-387--814598865289216.00000.hdf5\n",
      "Epoch 388/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 753872859561984.0000 - val_loss: 814248221474816.0000\n",
      "\n",
      "Epoch 00388: val_loss improved from 814598865289216.00000 to 814248221474816.00000, saving model to Weights\\Weights-388--814248221474816.00000.hdf5\n",
      "Epoch 389/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 752691743555584.0000 - val_loss: 812634588839936.0000\n",
      "\n",
      "Epoch 00389: val_loss improved from 814248221474816.00000 to 812634588839936.00000, saving model to Weights\\Weights-389--812634588839936.00000.hdf5\n",
      "Epoch 390/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 752403712311296.0000 - val_loss: 811573664808960.0000\n",
      "\n",
      "Epoch 00390: val_loss improved from 812634588839936.00000 to 811573664808960.00000, saving model to Weights\\Weights-390--811573664808960.00000.hdf5\n",
      "Epoch 391/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 751532169494528.0000 - val_loss: 810227997868032.0000\n",
      "\n",
      "Epoch 00391: val_loss improved from 811573664808960.00000 to 810227997868032.00000, saving model to Weights\\Weights-391--810227997868032.00000.hdf5\n",
      "Epoch 392/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 750025038626816.0000 - val_loss: 809582544814080.0000\n",
      "\n",
      "Epoch 00392: val_loss improved from 810227997868032.00000 to 809582544814080.00000, saving model to Weights\\Weights-392--809582544814080.00000.hdf5\n",
      "Epoch 393/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 749747207929856.0000 - val_loss: 808592085090304.0000\n",
      "\n",
      "Epoch 00393: val_loss improved from 809582544814080.00000 to 808592085090304.00000, saving model to Weights\\Weights-393--808592085090304.00000.hdf5\n",
      "Epoch 394/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 749018942537728.0000 - val_loss: 807272724824064.0000\n",
      "\n",
      "Epoch 00394: val_loss improved from 808592085090304.00000 to 807272724824064.00000, saving model to Weights\\Weights-394--807272724824064.00000.hdf5\n",
      "Epoch 395/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 747962716127232.0000 - val_loss: 805650837798912.0000\n",
      "\n",
      "Epoch 00395: val_loss improved from 807272724824064.00000 to 805650837798912.00000, saving model to Weights\\Weights-395--805650837798912.00000.hdf5\n",
      "Epoch 396/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 747212573245440.0000 - val_loss: 804777483042816.0000\n",
      "\n",
      "Epoch 00396: val_loss improved from 805650837798912.00000 to 804777483042816.00000, saving model to Weights\\Weights-396--804777483042816.00000.hdf5\n",
      "Epoch 397/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 746329756139520.0000 - val_loss: 804094449025024.0000\n",
      "\n",
      "Epoch 00397: val_loss improved from 804777483042816.00000 to 804094449025024.00000, saving model to Weights\\Weights-397--804094449025024.00000.hdf5\n",
      "Epoch 398/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 745916902408192.0000 - val_loss: 803612540272640.0000\n",
      "\n",
      "Epoch 00398: val_loss improved from 804094449025024.00000 to 803612540272640.00000, saving model to Weights\\Weights-398--803612540272640.00000.hdf5\n",
      "Epoch 399/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 745667190325248.0000 - val_loss: 802011926757376.0000\n",
      "\n",
      "Epoch 00399: val_loss improved from 803612540272640.00000 to 802011926757376.00000, saving model to Weights\\Weights-399--802011926757376.00000.hdf5\n",
      "Epoch 400/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 744882889031680.0000 - val_loss: 800241259380736.0000\n",
      "\n",
      "Epoch 00400: val_loss improved from 802011926757376.00000 to 800241259380736.00000, saving model to Weights\\Weights-400--800241259380736.00000.hdf5\n",
      "Epoch 401/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 744150395781120.0000 - val_loss: 799563325636608.0000\n",
      "\n",
      "Epoch 00401: val_loss improved from 800241259380736.00000 to 799563325636608.00000, saving model to Weights\\Weights-401--799563325636608.00000.hdf5\n",
      "Epoch 402/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 743197584130048.0000 - val_loss: 797618510757888.0000\n",
      "\n",
      "Epoch 00402: val_loss improved from 799563325636608.00000 to 797618510757888.00000, saving model to Weights\\Weights-402--797618510757888.00000.hdf5\n",
      "Epoch 403/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 741725886742528.0000 - val_loss: 796305391616000.0000\n",
      "\n",
      "Epoch 00403: val_loss improved from 797618510757888.00000 to 796305391616000.00000, saving model to Weights\\Weights-403--796305391616000.00000.hdf5\n",
      "Epoch 404/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 741822590615552.0000 - val_loss: 795201249476608.0000\n",
      "\n",
      "Epoch 00404: val_loss improved from 796305391616000.00000 to 795201249476608.00000, saving model to Weights\\Weights-404--795201249476608.00000.hdf5\n",
      "Epoch 405/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 740600001331200.0000 - val_loss: 794603846369280.0000\n",
      "\n",
      "Epoch 00405: val_loss improved from 795201249476608.00000 to 794603846369280.00000, saving model to Weights\\Weights-405--794603846369280.00000.hdf5\n",
      "Epoch 406/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 739205680463872.0000 - val_loss: 792469516058624.0000\n",
      "\n",
      "Epoch 00406: val_loss improved from 794603846369280.00000 to 792469516058624.00000, saving model to Weights\\Weights-406--792469516058624.00000.hdf5\n",
      "Epoch 407/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 738417688182784.0000 - val_loss: 791225116393472.0000\n",
      "\n",
      "Epoch 00407: val_loss improved from 792469516058624.00000 to 791225116393472.00000, saving model to Weights\\Weights-407--791225116393472.00000.hdf5\n",
      "Epoch 408/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 737102086012928.0000 - val_loss: 789859518119936.0000\n",
      "\n",
      "Epoch 00408: val_loss improved from 791225116393472.00000 to 789859518119936.00000, saving model to Weights\\Weights-408--789859518119936.00000.hdf5\n",
      "Epoch 409/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 736294296616960.0000 - val_loss: 788646928056320.0000\n",
      "\n",
      "Epoch 00409: val_loss improved from 789859518119936.00000 to 788646928056320.00000, saving model to Weights\\Weights-409--788646928056320.00000.hdf5\n",
      "Epoch 410/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 735943048822784.0000 - val_loss: 787125905653760.0000\n",
      "\n",
      "Epoch 00410: val_loss improved from 788646928056320.00000 to 787125905653760.00000, saving model to Weights\\Weights-410--787125905653760.00000.hdf5\n",
      "Epoch 411/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 735301353865216.0000 - val_loss: 786735466283008.0000\n",
      "\n",
      "Epoch 00411: val_loss improved from 787125905653760.00000 to 786735466283008.00000, saving model to Weights\\Weights-411--786735466283008.00000.hdf5\n",
      "Epoch 412/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 734097286627328.0000 - val_loss: 784665292046336.0000\n",
      "\n",
      "Epoch 00412: val_loss improved from 786735466283008.00000 to 784665292046336.00000, saving model to Weights\\Weights-412--784665292046336.00000.hdf5\n",
      "Epoch 413/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 733981523836928.0000 - val_loss: 782330507558912.0000\n",
      "\n",
      "Epoch 00413: val_loss improved from 784665292046336.00000 to 782330507558912.00000, saving model to Weights\\Weights-413--782330507558912.00000.hdf5\n",
      "Epoch 414/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 732663908401152.0000 - val_loss: 782515862241280.0000\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 782330507558912.00000\n",
      "Epoch 415/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 732134486573056.0000 - val_loss: 780218792935424.0000\n",
      "\n",
      "Epoch 00415: val_loss improved from 782330507558912.00000 to 780218792935424.00000, saving model to Weights\\Weights-415--780218792935424.00000.hdf5\n",
      "Epoch 416/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 730899549257728.0000 - val_loss: 779556495556608.0000\n",
      "\n",
      "Epoch 00416: val_loss improved from 780218792935424.00000 to 779556495556608.00000, saving model to Weights\\Weights-416--779556495556608.00000.hdf5\n",
      "Epoch 417/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 729671658373120.0000 - val_loss: 779083847827456.0000\n",
      "\n",
      "Epoch 00417: val_loss improved from 779556495556608.00000 to 779083847827456.00000, saving model to Weights\\Weights-417--779083847827456.00000.hdf5\n",
      "Epoch 418/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 729512543256576.0000 - val_loss: 777789250732032.0000\n",
      "\n",
      "Epoch 00418: val_loss improved from 779083847827456.00000 to 777789250732032.00000, saving model to Weights\\Weights-418--777789250732032.00000.hdf5\n",
      "Epoch 419/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 728884068745216.0000 - val_loss: 775972748001280.0000\n",
      "\n",
      "Epoch 00419: val_loss improved from 777789250732032.00000 to 775972748001280.00000, saving model to Weights\\Weights-419--775972748001280.00000.hdf5\n",
      "Epoch 420/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 727521423261696.0000 - val_loss: 774243889446912.0000\n",
      "\n",
      "Epoch 00420: val_loss improved from 775972748001280.00000 to 774243889446912.00000, saving model to Weights\\Weights-420--774243889446912.00000.hdf5\n",
      "Epoch 421/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 726562169159680.0000 - val_loss: 775059799015424.0000\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 774243889446912.00000\n",
      "Epoch 422/500\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 726162670092288.0000 - val_loss: 772940299763712.0000\n",
      "\n",
      "Epoch 00422: val_loss improved from 774243889446912.00000 to 772940299763712.00000, saving model to Weights\\Weights-422--772940299763712.00000.hdf5\n",
      "Epoch 423/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 724967327006720.0000 - val_loss: 771036219965440.0000\n",
      "\n",
      "Epoch 00423: val_loss improved from 772940299763712.00000 to 771036219965440.00000, saving model to Weights\\Weights-423--771036219965440.00000.hdf5\n",
      "Epoch 424/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 724550916505600.0000 - val_loss: 769278437490688.0000\n",
      "\n",
      "Epoch 00424: val_loss improved from 771036219965440.00000 to 769278437490688.00000, saving model to Weights\\Weights-424--769278437490688.00000.hdf5\n",
      "Epoch 425/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 723610452885504.0000 - val_loss: 768440516214784.0000\n",
      "\n",
      "Epoch 00425: val_loss improved from 769278437490688.00000 to 768440516214784.00000, saving model to Weights\\Weights-425--768440516214784.00000.hdf5\n",
      "Epoch 426/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 722726964690944.0000 - val_loss: 767400060387328.0000\n",
      "\n",
      "Epoch 00426: val_loss improved from 768440516214784.00000 to 767400060387328.00000, saving model to Weights\\Weights-426--767400060387328.00000.hdf5\n",
      "Epoch 427/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 721882064093184.0000 - val_loss: 765660061761536.0000\n",
      "\n",
      "Epoch 00427: val_loss improved from 767400060387328.00000 to 765660061761536.00000, saving model to Weights\\Weights-427--765660061761536.00000.hdf5\n",
      "Epoch 428/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 720873552084992.0000 - val_loss: 765092320772096.0000\n",
      "\n",
      "Epoch 00428: val_loss improved from 765660061761536.00000 to 765092320772096.00000, saving model to Weights\\Weights-428--765092320772096.00000.hdf5\n",
      "Epoch 429/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 720294536806400.0000 - val_loss: 763931941404672.0000\n",
      "\n",
      "Epoch 00429: val_loss improved from 765092320772096.00000 to 763931941404672.00000, saving model to Weights\\Weights-429--763931941404672.00000.hdf5\n",
      "Epoch 430/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 719486478974976.0000 - val_loss: 762733511311360.0000\n",
      "\n",
      "Epoch 00430: val_loss improved from 763931941404672.00000 to 762733511311360.00000, saving model to Weights\\Weights-430--762733511311360.00000.hdf5\n",
      "Epoch 431/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 719226968997888.0000 - val_loss: 762378169876480.0000\n",
      "\n",
      "Epoch 00431: val_loss improved from 762733511311360.00000 to 762378169876480.00000, saving model to Weights\\Weights-431--762378169876480.00000.hdf5\n",
      "Epoch 432/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 718961150787584.0000 - val_loss: 759751461830656.0000\n",
      "\n",
      "Epoch 00432: val_loss improved from 762378169876480.00000 to 759751461830656.00000, saving model to Weights\\Weights-432--759751461830656.00000.hdf5\n",
      "Epoch 433/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 718613325545472.0000 - val_loss: 759555235512320.0000\n",
      "\n",
      "Epoch 00433: val_loss improved from 759751461830656.00000 to 759555235512320.00000, saving model to Weights\\Weights-433--759555235512320.00000.hdf5\n",
      "Epoch 434/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 717278328913920.0000 - val_loss: 758425793331200.0000\n",
      "\n",
      "Epoch 00434: val_loss improved from 759555235512320.00000 to 758425793331200.00000, saving model to Weights\\Weights-434--758425793331200.00000.hdf5\n",
      "Epoch 435/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 715802269450240.0000 - val_loss: 756993958608896.0000\n",
      "\n",
      "Epoch 00435: val_loss improved from 758425793331200.00000 to 756993958608896.00000, saving model to Weights\\Weights-435--756993958608896.00000.hdf5\n",
      "Epoch 436/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 715366867140608.0000 - val_loss: 757207566123008.0000\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 756993958608896.00000\n",
      "Epoch 437/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 714262389456896.0000 - val_loss: 755372205801472.0000\n",
      "\n",
      "Epoch 00437: val_loss improved from 756993958608896.00000 to 755372205801472.00000, saving model to Weights\\Weights-437--755372205801472.00000.hdf5\n",
      "Epoch 438/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 714280173305856.0000 - val_loss: 754347721883648.0000\n",
      "\n",
      "Epoch 00438: val_loss improved from 755372205801472.00000 to 754347721883648.00000, saving model to Weights\\Weights-438--754347721883648.00000.hdf5\n",
      "Epoch 439/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 712447967100928.0000 - val_loss: 752624903127040.0000\n",
      "\n",
      "Epoch 00439: val_loss improved from 754347721883648.00000 to 752624903127040.00000, saving model to Weights\\Weights-439--752624903127040.00000.hdf5\n",
      "Epoch 440/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 712690431426560.0000 - val_loss: 751794565152768.0000\n",
      "\n",
      "Epoch 00440: val_loss improved from 752624903127040.00000 to 751794565152768.00000, saving model to Weights\\Weights-440--751794565152768.00000.hdf5\n",
      "Epoch 441/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 711866066141184.0000 - val_loss: 753253310529536.0000\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 751794565152768.00000\n",
      "Epoch 442/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 710725483888640.0000 - val_loss: 751464993521664.0000\n",
      "\n",
      "Epoch 00442: val_loss improved from 751794565152768.00000 to 751464993521664.00000, saving model to Weights\\Weights-442--751464993521664.00000.hdf5\n",
      "Epoch 443/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 710542612234240.0000 - val_loss: 748500996325376.0000\n",
      "\n",
      "Epoch 00443: val_loss improved from 751464993521664.00000 to 748500996325376.00000, saving model to Weights\\Weights-443--748500996325376.00000.hdf5\n",
      "Epoch 444/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 709330626150400.0000 - val_loss: 747941711052800.0000\n",
      "\n",
      "Epoch 00444: val_loss improved from 748500996325376.00000 to 747941711052800.00000, saving model to Weights\\Weights-444--747941711052800.00000.hdf5\n",
      "Epoch 445/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 709121514930176.0000 - val_loss: 746162722177024.0000\n",
      "\n",
      "Epoch 00445: val_loss improved from 747941711052800.00000 to 746162722177024.00000, saving model to Weights\\Weights-445--746162722177024.00000.hdf5\n",
      "Epoch 446/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 707801416466432.0000 - val_loss: 745978374127616.0000\n",
      "\n",
      "Epoch 00446: val_loss improved from 746162722177024.00000 to 745978374127616.00000, saving model to Weights\\Weights-446--745978374127616.00000.hdf5\n",
      "Epoch 447/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 706935913447424.0000 - val_loss: 743856056303616.0000\n",
      "\n",
      "Epoch 00447: val_loss improved from 745978374127616.00000 to 743856056303616.00000, saving model to Weights\\Weights-447--743856056303616.00000.hdf5\n",
      "Epoch 448/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 706934235725824.0000 - val_loss: 743299119841280.0000\n",
      "\n",
      "Epoch 00448: val_loss improved from 743856056303616.00000 to 743299119841280.00000, saving model to Weights\\Weights-448--743299119841280.00000.hdf5\n",
      "Epoch 449/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 707180726583296.0000 - val_loss: 742165248475136.0000\n",
      "\n",
      "Epoch 00449: val_loss improved from 743299119841280.00000 to 742165248475136.00000, saving model to Weights\\Weights-449--742165248475136.00000.hdf5\n",
      "Epoch 450/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 705065522298880.0000 - val_loss: 743681908801536.0000\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 742165248475136.00000\n",
      "Epoch 451/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 704695215587328.0000 - val_loss: 738747528249344.0000\n",
      "\n",
      "Epoch 00451: val_loss improved from 742165248475136.00000 to 738747528249344.00000, saving model to Weights\\Weights-451--738747528249344.00000.hdf5\n",
      "Epoch 452/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 703537990336512.0000 - val_loss: 739266548203520.0000\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 738747528249344.00000\n",
      "Epoch 453/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 702691210690560.0000 - val_loss: 738557341728768.0000\n",
      "\n",
      "Epoch 00453: val_loss improved from 738747528249344.00000 to 738557341728768.00000, saving model to Weights\\Weights-453--738557341728768.00000.hdf5\n",
      "Epoch 454/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 702852204855296.0000 - val_loss: 736171755831296.0000\n",
      "\n",
      "Epoch 00454: val_loss improved from 738557341728768.00000 to 736171755831296.00000, saving model to Weights\\Weights-454--736171755831296.00000.hdf5\n",
      "Epoch 455/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 701704509063168.0000 - val_loss: 735860706246656.0000\n",
      "\n",
      "Epoch 00455: val_loss improved from 736171755831296.00000 to 735860706246656.00000, saving model to Weights\\Weights-455--735860706246656.00000.hdf5\n",
      "Epoch 456/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 701289642065920.0000 - val_loss: 734560807550976.0000\n",
      "\n",
      "Epoch 00456: val_loss improved from 735860706246656.00000 to 734560807550976.00000, saving model to Weights\\Weights-456--734560807550976.00000.hdf5\n",
      "Epoch 457/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 700259118350336.0000 - val_loss: 734694018646016.0000\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 734560807550976.00000\n",
      "Epoch 458/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 699829554511872.0000 - val_loss: 732779671191552.0000\n",
      "\n",
      "Epoch 00458: val_loss improved from 734560807550976.00000 to 732779671191552.00000, saving model to Weights\\Weights-458--732779671191552.00000.hdf5\n",
      "Epoch 459/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 699384287199232.0000 - val_loss: 733680137928704.0000\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 732779671191552.00000\n",
      "Epoch 460/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 698169281216512.0000 - val_loss: 729247865896960.0000\n",
      "\n",
      "Epoch 00460: val_loss improved from 732779671191552.00000 to 729247865896960.00000, saving model to Weights\\Weights-460--729247865896960.00000.hdf5\n",
      "Epoch 461/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 696625374691328.0000 - val_loss: 728111511502848.0000\n",
      "\n",
      "Epoch 00461: val_loss improved from 729247865896960.00000 to 728111511502848.00000, saving model to Weights\\Weights-461--728111511502848.00000.hdf5\n",
      "Epoch 462/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 696570345422848.0000 - val_loss: 728422426869760.0000\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 728111511502848.00000\n",
      "Epoch 463/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 695996631744512.0000 - val_loss: 726996631945216.0000\n",
      "\n",
      "Epoch 00463: val_loss improved from 728111511502848.00000 to 726996631945216.00000, saving model to Weights\\Weights-463--726996631945216.00000.hdf5\n",
      "Epoch 464/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 695103211438080.0000 - val_loss: 725538826092544.0000\n",
      "\n",
      "Epoch 00464: val_loss improved from 726996631945216.00000 to 725538826092544.00000, saving model to Weights\\Weights-464--725538826092544.00000.hdf5\n",
      "Epoch 465/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 693925517983744.0000 - val_loss: 725129663348736.0000\n",
      "\n",
      "Epoch 00465: val_loss improved from 725538826092544.00000 to 725129663348736.00000, saving model to Weights\\Weights-465--725129663348736.00000.hdf5\n",
      "Epoch 466/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 693470989647872.0000 - val_loss: 724080684695552.0000\n",
      "\n",
      "Epoch 00466: val_loss improved from 725129663348736.00000 to 724080684695552.00000, saving model to Weights\\Weights-466--724080684695552.00000.hdf5\n",
      "Epoch 467/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 693306975584256.0000 - val_loss: 723259943288832.0000\n",
      "\n",
      "Epoch 00467: val_loss improved from 724080684695552.00000 to 723259943288832.00000, saving model to Weights\\Weights-467--723259943288832.00000.hdf5\n",
      "Epoch 468/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 691927217340416.0000 - val_loss: 722342766444544.0000\n",
      "\n",
      "Epoch 00468: val_loss improved from 723259943288832.00000 to 722342766444544.00000, saving model to Weights\\Weights-468--722342766444544.00000.hdf5\n",
      "Epoch 469/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 691512551669760.0000 - val_loss: 721149369516032.0000\n",
      "\n",
      "Epoch 00469: val_loss improved from 722342766444544.00000 to 721149369516032.00000, saving model to Weights\\Weights-469--721149369516032.00000.hdf5\n",
      "Epoch 470/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 691433900081152.0000 - val_loss: 719765316304896.0000\n",
      "\n",
      "Epoch 00470: val_loss improved from 721149369516032.00000 to 719765316304896.00000, saving model to Weights\\Weights-470--719765316304896.00000.hdf5\n",
      "Epoch 471/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 691108556308480.0000 - val_loss: 719718205882368.0000\n",
      "\n",
      "Epoch 00471: val_loss improved from 719765316304896.00000 to 719718205882368.00000, saving model to Weights\\Weights-471--719718205882368.00000.hdf5\n",
      "Epoch 472/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 689676453150720.0000 - val_loss: 718920147271680.0000\n",
      "\n",
      "Epoch 00472: val_loss improved from 719718205882368.00000 to 718920147271680.00000, saving model to Weights\\Weights-472--718920147271680.00000.hdf5\n",
      "Epoch 473/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 688679483867136.0000 - val_loss: 718103969267712.0000\n",
      "\n",
      "Epoch 00473: val_loss improved from 718920147271680.00000 to 718103969267712.00000, saving model to Weights\\Weights-473--718103969267712.00000.hdf5\n",
      "Epoch 474/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 688372997685248.0000 - val_loss: 718122826858496.0000\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 718103969267712.00000\n",
      "Epoch 475/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 688458628595712.0000 - val_loss: 714814695407616.0000\n",
      "\n",
      "Epoch 00475: val_loss improved from 718103969267712.00000 to 714814695407616.00000, saving model to Weights\\Weights-475--714814695407616.00000.hdf5\n",
      "Epoch 476/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 687688889925632.0000 - val_loss: 716245590605824.0000\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 714814695407616.00000\n",
      "Epoch 477/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 686104718082048.0000 - val_loss: 713040941023232.0000\n",
      "\n",
      "Epoch 00477: val_loss improved from 714814695407616.00000 to 713040941023232.00000, saving model to Weights\\Weights-477--713040941023232.00000.hdf5\n",
      "Epoch 478/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 685099091755008.0000 - val_loss: 712138997891072.0000\n",
      "\n",
      "Epoch 00478: val_loss improved from 713040941023232.00000 to 712138997891072.00000, saving model to Weights\\Weights-478--712138997891072.00000.hdf5\n",
      "Epoch 479/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 685671664582656.0000 - val_loss: 710487918510080.0000\n",
      "\n",
      "Epoch 00479: val_loss improved from 712138997891072.00000 to 710487918510080.00000, saving model to Weights\\Weights-479--710487918510080.00000.hdf5\n",
      "Epoch 480/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 684617048784896.0000 - val_loss: 711808822280192.0000\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 710487918510080.00000\n",
      "Epoch 481/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 685822995070976.0000 - val_loss: 708217491423232.0000\n",
      "\n",
      "Epoch 00481: val_loss improved from 710487918510080.00000 to 708217491423232.00000, saving model to Weights\\Weights-481--708217491423232.00000.hdf5\n",
      "Epoch 482/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 683986024136704.0000 - val_loss: 707682096906240.0000\n",
      "\n",
      "Epoch 00482: val_loss improved from 708217491423232.00000 to 707682096906240.00000, saving model to Weights\\Weights-482--707682096906240.00000.hdf5\n",
      "Epoch 483/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 682486342352896.0000 - val_loss: 707538215501824.0000\n",
      "\n",
      "Epoch 00483: val_loss improved from 707682096906240.00000 to 707538215501824.00000, saving model to Weights\\Weights-483--707538215501824.00000.hdf5\n",
      "Epoch 484/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 681744185425920.0000 - val_loss: 706922491674624.0000\n",
      "\n",
      "Epoch 00484: val_loss improved from 707538215501824.00000 to 706922491674624.00000, saving model to Weights\\Weights-484--706922491674624.00000.hdf5\n",
      "Epoch 485/500\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 680962971140096.0000 - val_loss: 704759304552448.0000\n",
      "\n",
      "Epoch 00485: val_loss improved from 706922491674624.00000 to 704759304552448.00000, saving model to Weights\\Weights-485--704759304552448.00000.hdf5\n",
      "Epoch 486/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 680672188432384.0000 - val_loss: 703412228325376.0000\n",
      "\n",
      "Epoch 00486: val_loss improved from 704759304552448.00000 to 703412228325376.00000, saving model to Weights\\Weights-486--703412228325376.00000.hdf5\n",
      "Epoch 487/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 679783734181888.0000 - val_loss: 703260495183872.0000\n",
      "\n",
      "Epoch 00487: val_loss improved from 703412228325376.00000 to 703260495183872.00000, saving model to Weights\\Weights-487--703260495183872.00000.hdf5\n",
      "Epoch 488/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 680046062731264.0000 - val_loss: 702677050720256.0000\n",
      "\n",
      "Epoch 00488: val_loss improved from 703260495183872.00000 to 702677050720256.00000, saving model to Weights\\Weights-488--702677050720256.00000.hdf5\n",
      "Epoch 489/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 679236461395968.0000 - val_loss: 702884417110016.0000\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 702677050720256.00000\n",
      "Epoch 490/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 678065143283712.0000 - val_loss: 700543257280512.0000\n",
      "\n",
      "Epoch 00490: val_loss improved from 702677050720256.00000 to 700543257280512.00000, saving model to Weights\\Weights-490--700543257280512.00000.hdf5\n",
      "Epoch 491/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 677303256350720.0000 - val_loss: 698382553186304.0000\n",
      "\n",
      "Epoch 00491: val_loss improved from 700543257280512.00000 to 698382553186304.00000, saving model to Weights\\Weights-491--698382553186304.00000.hdf5\n",
      "Epoch 492/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 677159576272896.0000 - val_loss: 700629827715072.0000\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 698382553186304.00000\n",
      "Epoch 493/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 675555204661248.0000 - val_loss: 697575501987840.0000\n",
      "\n",
      "Epoch 00493: val_loss improved from 698382553186304.00000 to 697575501987840.00000, saving model to Weights\\Weights-493--697575501987840.00000.hdf5\n",
      "Epoch 494/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 675110742654976.0000 - val_loss: 696677316952064.0000\n",
      "\n",
      "Epoch 00494: val_loss improved from 697575501987840.00000 to 696677316952064.00000, saving model to Weights\\Weights-494--696677316952064.00000.hdf5\n",
      "Epoch 495/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 674210544353280.0000 - val_loss: 695311584460800.0000\n",
      "\n",
      "Epoch 00495: val_loss improved from 696677316952064.00000 to 695311584460800.00000, saving model to Weights\\Weights-495--695311584460800.00000.hdf5\n",
      "Epoch 496/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 674390328999936.0000 - val_loss: 694807999545344.0000\n",
      "\n",
      "Epoch 00496: val_loss improved from 695311584460800.00000 to 694807999545344.00000, saving model to Weights\\Weights-496--694807999545344.00000.hdf5\n",
      "Epoch 497/500\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 672754550439936.0000 - val_loss: 694007726342144.0000\n",
      "\n",
      "Epoch 00497: val_loss improved from 694807999545344.00000 to 694007726342144.00000, saving model to Weights\\Weights-497--694007726342144.00000.hdf5\n",
      "Epoch 498/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 671728254582784.0000 - val_loss: 693153900265472.0000\n",
      "\n",
      "Epoch 00498: val_loss improved from 694007726342144.00000 to 693153900265472.00000, saving model to Weights\\Weights-498--693153900265472.00000.hdf5\n",
      "Epoch 499/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 670891071504384.0000 - val_loss: 691888965287936.0000\n",
      "\n",
      "Epoch 00499: val_loss improved from 693153900265472.00000 to 691888965287936.00000, saving model to Weights\\Weights-499--691888965287936.00000.hdf5\n",
      "Epoch 500/500\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 671339492933632.0000 - val_loss: 691431886815232.0000\n",
      "\n",
      "Epoch 00500: val_loss improved from 691888965287936.00000 to 691431886815232.00000, saving model to Weights\\Weights-500--691431886815232.00000.hdf5\n",
      "MAE: 7649248.58164761\n",
      "MSE: 691431855487071.6\n",
      "RMSE: 26295091.851656873\n",
      "VarScore: 0.7505892153592422\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=Sequential([\n",
    "    \n",
    "    Dense(17, input_dim = X_train.shape[1]), \n",
    "    ReLU(),\n",
    "    \n",
    "    Dense(17),  \n",
    "    ReLU(),\n",
    "\n",
    "    Dense(17),  \n",
    "    ReLU(),\n",
    "    \n",
    "    Dense(units=17), \n",
    "    ReLU(),\n",
    "\n",
    "    Dense(units=1, activation=\"linear\"),\n",
    "],name=\"Moscow_Housing\",)\n",
    "\n",
    "model.compile(optimizer=Adam(),loss='mse')\n",
    "checkpoint_name = 'Weights\\Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(x=X_train,y=y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size=128,\n",
    "          epochs=500,\n",
    "          callbacks=callbacks_list, \n",
    "          verbose=1)\n",
    "\n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot(figsize=(12,8))\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('VarScore:',metrics.explained_variance_score(y_test,y_pred))\n",
    "\n",
    "# Visualizing Our predictions\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(y_test,y_pred)\n",
    "# Perfect predictions\n",
    "plt.plot(y_test,y_test,'r')\n",
    "list(set(X2) - set(X))\n",
    "X2=X2[X.columns]\n",
    "y_pred2= model.predict(X2)\n",
    "X2['target']=y_pred2\n",
    "predictions=pd.DataFrame()\n",
    "predictions['id']=id_test\n",
    "predictions['price_prediction']=y_pred2\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce96951c850973756921872a34d9ef1e1b328ca83fcfea4a481c0fa9725a961"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('ass2': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
